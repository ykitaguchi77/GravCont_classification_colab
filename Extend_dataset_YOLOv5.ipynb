{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Extend_dataset_YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend dataset YOLOv5**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "#ã‚µãƒãƒ¼ãƒˆãƒ‘ãƒƒãƒã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabã‚’ãƒã‚¦ãƒ³ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e925a561-238f-4090-9755-91d8a0460595"
      },
      "source": [
        "'''\n",
        "ãƒ»dlibã‚’ç”¨ã„ã¦ç›®ã‚’åˆ‡ã‚ŠæŠœã\n",
        "ãƒ»æ¨ªå¹…ã‚’2å€ã€ç¸¦å¹…ã‚’ä¸Šã«1å€è¿½åŠ /ä¸‹ã«0.5å€è¿½åŠ ã—ãŸä¸¡çœ¼ã®ç”»åƒãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«åˆ‡ã‚Šå–ã‚‹ï¼ˆç›®ã®å…¨å¹…ã€çœ‰æ¯›ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ï¼‰\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f122be48-38e9-45a8-dde3-df03feb9c91b"
      },
      "source": [
        "#æ®‹ã‚Šæ™‚é–“ç¢ºèª\n",
        "!cat /proc/uptime | awk '{printf(\"æ®‹ã‚Šæ™‚é–“ : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ®‹ã‚Šæ™‚é–“ : 11.89"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "#ãƒ†ã‚¹ãƒˆç”»åƒ\n",
        "test_path = '/content/drive/MyDrive/Deep_learning/Face_Images/IMG_3110.JPG'\n",
        "\n",
        "# GO_extended_datasetã‚’ colabä¸Šã®ãƒ•ã‚©ãƒ«ãƒ€ã«å±•é–‹\n",
        "zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "!unzip $zip_path -d \"/content\"\n",
        "in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "#ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
        "out_path_list = ['/content/GO_extended_dataset/cont_for_yolo', '/content/GO_extended_dataset/grav_for_yolo']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2mKFMmwfTOi"
      },
      "source": [
        "#é¡”ã®ç”»åƒã‹ã‚‰ç›®ã‚’æ¤œå‡ºã—ã¦åˆ‡ã‚ŠæŠœãã‚¹ã‚¯ãƒªãƒ—ãƒˆ\n",
        "ãƒ»Haarcascade_eyeã‚’ä½¿ç”¨<br>\n",
        "ãƒ»ç›®ãŒæ¤œå‡ºã§ããªã„ã‚‚ã®ã¯skipã™ã‚‹<br>\n",
        "ãƒ»æ¨ªå¹…ã‚’1/4å€ã€ç¸¦å¹…ã‚’ä¸Šä¸‹ã«1/4å€è¿½åŠ ã—ã¦ç”»åƒã‚’åˆ‡ã‚Šå–ã‚‹ï¼ˆç›®ã®å…¨å¹…ã€çœ‰æ¯›ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ï¼‰\n",
        "\n",
        "ãƒ»åˆ‡ã‚Šå–ã£ãŸç”»åƒã‚’æ¨ªå¹…640pxã«resizeã™ã‚‹<br>\n",
        "ãƒ»ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒã‚’ä¸€æ‹¬å¤‰æ›ã—ã¦åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0shkziUPn1c"
      },
      "source": [
        "#Haarcascadeã‚’æŒ‡å®š"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfu_RX-kIlmx"
      },
      "source": [
        "# ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "# righteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_righteye_2splits.xml'\n",
        "# lefteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_lefteye_2splits.xml'\n",
        "\n",
        "\n",
        "# ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰åˆ†é¡å™¨ã®ç‰¹å¾´é‡å–å¾—\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# righteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# lefteye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ãŸã¾ã¾æ¨ªã‚’400pixelã«ç¸®å°ã™ã‚‹\n",
        "def scale_to_width(img, width):\n",
        "    scale = width / img.shape[1]\n",
        "    return cv2.resize(img, dsize=None, fx=scale, fy=scale)\n",
        "\n",
        "#å›³ã‚’è¡¨ç¤ºã™ã‚‹\n",
        "def show_image(img):\n",
        "    #img = cv2.imread(out_path)\n",
        "    dst = scale_to_width(img, 200)\n",
        "    cv2_imshow(dst)\n",
        "\n",
        "# def show_image_pillow(img):\n",
        "#     src = cv2.cvtColor(img_resized_list[0], cv2.COLOR_BGR2RGB)\n",
        "#     plt.imshow(src)\n",
        "\n",
        "def my_round(val, digit=0):\n",
        "    p = 10 ** digit\n",
        "    return int((val * p * 2 + 1) // 2 / p)\n",
        "\n",
        "def scale_to_width(img, width):\n",
        "    \"\"\"å¹…ãŒæŒ‡å®šã—ãŸå€¤ã«ãªã‚‹ã‚ˆã†ã«ã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’å›ºå®šã—ã¦ã€ãƒªã‚µã‚¤ã‚ºã™ã‚‹ã€‚\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    height = round(h * (width / w))\n",
        "    dst = cv2.resize(img, dsize=(width, height))\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "SMbQsnPTHf-K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_bilateral(in_path, class_num, size, showImage=True):\n",
        "    img_resized_list,side_list = [],[]\n",
        "\n",
        "    img = cv2.imread(in_path) \n",
        "    img2 = img.copy()\n",
        "\n",
        "    if showImage:\n",
        "        show_image(img)\n",
        "\n",
        "    # ç”»åƒã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«åŒ–\n",
        "    grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    #300pixä»¥ä¸Šã®ã‚‚ã®ã§ç›®ã«è¦‹ãˆã‚‹ã‚‚ã®ã‚’æŠ½å‡º\n",
        "    eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "    print(\"\")\n",
        "    print('image path = ',in_path)\n",
        "\n",
        "    # çœ¼æ¤œå‡ºåˆ¤å®š\n",
        "    if len(eye_list) >= 1:\n",
        "        print('ç›®ãŒ' + str(len(eye_list)) +'å€‹æ¤œå‡ºã•ã‚Œã¾ã—ãŸ')\n",
        "        pass\n",
        "    else:\n",
        "        print(\"no eye detected\")\n",
        "        pass\n",
        "\n",
        "    print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "    #ç”»åƒã®åˆ‡ã‚ŠæŠœãã¨ä¿å­˜ï¼ˆé€£ç•ªã«ã™ã‚‹ï¼‰\n",
        "    if len(eye_list)== 2: \n",
        "\n",
        "        \n",
        "        for (ex, ey, ew, eh) in eye_list:\n",
        "            print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "            \n",
        "            try:\n",
        "                cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "\n",
        "                #åˆ‡ã‚ŠæŠœãç¯„å›²ãŒå…ƒç”»åƒã‚’ã¯ã¿å‡ºã‚‹å ´åˆã¯é»’ç”»åƒã§åŸ‹ã‚ã‚‹\n",
        "                top = max(0, ey-int(eh/4))\n",
        "                bottom = min(grayscale_img.shape[0], int(ey + eh*5/4))\n",
        "                left = max(0,int(ex-int(ew/4)))\n",
        "                right = min(grayscale_img.shape[1], int(ex + ew*5/4))\n",
        "\n",
        "                #print(f\"top:{top}, bottom:{bottom}, left:{left}, right:{right}\")\n",
        "\n",
        "                img_cropped = img[top: bottom,left:right]\n",
        "                height, width = img_cropped.shape[:2]\n",
        "\n",
        "                #ã‚¯ãƒ­ãƒƒãƒ—ã—ãŸç”»åƒã‚’è¡¨ç¤º\n",
        "                if showImage:\n",
        "                    show_image(img_cropped)\n",
        "            except: \n",
        "                pass\n",
        "\n",
        "       \n",
        "        ex = min(eye_list[0][0], eye_list[1][0])\n",
        "        ey = min(eye_list[0][1], eye_list[1][1])\n",
        "        ew = max(eye_list[0][0]+eye_list[0][2], eye_list[1][0]+eye_list[1][2]) - ex\n",
        "        eh = max(eye_list[0][1]+eye_list[0][3], eye_list[1][1]+eye_list[1][3]) - ey\n",
        "\n",
        "        print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "\n",
        "        try:\n",
        "            cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "\n",
        "            #åˆ‡ã‚ŠæŠœãç¯„å›²ãŒå…ƒç”»åƒã‚’ã¯ã¿å‡ºã‚‹å ´åˆã¯é»’ç”»åƒã§åŸ‹ã‚ã‚‹\n",
        "            top = max(0, int(ey-eh/4))\n",
        "            bottom = min(grayscale_img.shape[0], int(ey+7/6*eh))\n",
        "            left = max(0,int(ex-ew/10))\n",
        "            right = min(grayscale_img.shape[1], int(ex + 11/10*ew))\n",
        "\n",
        "            print(f\"top:{top}, bottom:{bottom}, left:{left}, right:{right}\")\n",
        "\n",
        "            img_cropped = img[top: bottom,left:right]\n",
        "            height, width = img_cropped.shape[:2]\n",
        "\n",
        "            img_resized = scale_to_width(img_cropped, size) #1è¾ºã‚’æŒ‡å®šã—ãŸpixã«resize \n",
        "\n",
        "            #ã‚¯ãƒ­ãƒƒãƒ—ã—ãŸç”»åƒã‚’è¡¨ç¤º\n",
        "            if showImage:\n",
        "                show_image(img_resized)\n",
        "            print(img2.shape)\n",
        "\n",
        "            #ç¸¦ã€æ¨ªã«å¯¾ã™ã‚‹å‰²åˆ\n",
        "            X = round((right+left)/2/img2.shape[1], 6)\n",
        "            Y = round((top+bottom)/2/img2.shape[0], 6)\n",
        "            W = round((right-left)/img.shape[1], 6)\n",
        "            H = round((bottom-top)/img.shape[0], 6)\n",
        "            txt = f\"{class_num} {X} {Y} {W} {H}\"\n",
        "            return img_resized, txt\n",
        "\n",
        "        except:\n",
        "            print('crop error')\n"
      ],
      "metadata": {
        "id": "aR56JrxipDoK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, txt = crop_bilateral(test_path, class_num=0, size=640)\n",
        "print(txt)\n",
        "\n",
        "with open(\"test.txt\", mode='w') as f:\n",
        "    f.write(txt)\n",
        "\n",
        "\n",
        "# #æ¤œå‡ºã•ã‚ŒãŸç”»åƒã‚’ç¢ºèª\n",
        "# src = cv2.cvtColor(img_resized_list[0], cv2.COLOR_BGR2RGB)\n",
        "# plt.imshow(src)\n"
      ],
      "metadata": {
        "id": "sQkblQvvgl9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Aquisition of bounding boxes**\n",
        "\n",
        "ç”»åƒã¨ãƒ©ãƒ™ãƒ«ã‚’æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"
      ],
      "metadata": {
        "id": "Z8R1kiD9lU-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# orig_folder = \"/content/GO_extended_dataset/Control_photo_1886mai\" \n",
        "# dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont\"\n",
        "# class_num = 0 #classã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ãè¾¼ã‚€(0:cont, 1:grav)\n",
        "\n",
        "\n",
        "orig_folder = \"/content/GO_extended_dataset/treatable\" \n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/grav\"\n",
        "class_num = 1 #classã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ãè¾¼ã‚€(0:cont, 1:grav)\n",
        "\n",
        "path_list = glob.glob(orig_folder+\"/*\")\n",
        "path = path_list[1]\n",
        "path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8gIkLcQWmH9d",
        "outputId": "2288c940-9d4d-4fdd-b40a-fe9376e67bf7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/GO_extended_dataset/treatable/5906-20190509-75-102501_1b7f8b82f6c7c4a016f4dae73d0c91cc3e05404f4cdd6eca55b2b48e4dba2c08.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = glob.glob(orig_folder+\"/*\")\n",
        "#path_list = [path_list[0]] #ãƒ†ã‚¹ãƒˆç”¨\n",
        "dst_folder = dst_folder\n",
        "\n",
        "#å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬\n",
        "start = time.time()\n",
        "\n",
        "#ã‚‚ã—dst_folderãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦æ–°ã—ãä½œã‚Šç›´ã™\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/images\") #imageæ ¼ç´ç”¨\n",
        "os.makedirs(f\"{dst_folder}/images_cropped\") #cropped_imageæ ¼ç´ç”¨ (YOLOã§ã¯ä½¿ç”¨ã—ãªã„)\n",
        "os.makedirs(f\"{dst_folder}/labels\") #labelæ ¼ç´ç”¨\n",
        "\n",
        "\n",
        "num=0\n",
        "for path in path_list:\n",
        "    try: #ç›®ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼å›é¿\n",
        "        img, txt = crop_bilateral(path, class_num=class_num, size=640, showImage=False)  #ä¸¡çœ¼æŠœãå‡ºã—ã¦640pxã§ä¿å­˜ï¼ˆcropæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã‚‚ã®ã¯å‰Šé™¤ã•ã‚Œã‚‹ï¼‰\n",
        "        img2 = cv2.imread(path).copy()\n",
        "        img2 = scale_to_width(img2, 640)  #åˆ‡ã‚ŠæŠœãå‰ã®ç”»åƒã‚’æ¨ªã®ã‚µã‚¤ã‚ºã‚’640ã«ãªã‚‹ã‚ˆã†ã«ç¸®å°\n",
        "        cv2.imwrite(f\"{dst_folder}/images/{os.path.basename(path).split('.')[0]}.JPG\", img2) #cropã›ãšã«ç¸®å°ã—ãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä¿å­˜\n",
        "        cv2.imwrite(f\"{dst_folder}/images_cropped/{os.path.basename(path).split('.')[0]}.JPG\", img) #cropã—ãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä¿å­˜ã™ã‚‹å ´åˆ\n",
        "\n",
        "        with open(f\"{dst_folder}/labels/{os.path.basename(path).split('.')[0]}.txt\", mode='w') as f:\n",
        "            f.write(txt)\n",
        "        num+=1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\")\n",
        "print('Process done!!')\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "print (f\"image_num:{num}\")\n"
      ],
      "metadata": {
        "id": "R8k494hphZs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLO_v5 trainingç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ**\n",
        "\n",
        "datasetã‚’trainã¨valã«åˆ†ã‘ã‚‹\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels \n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YiuPobEMYWKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "def split_dataset(dataset_dir):\n",
        "    img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "    img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "    # img_train, img_testã«åå‰ãŒä¸€è‡´ã™ã‚‹txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠœãå‡ºã™\n",
        "    label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "    label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "    print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "    return img_train, img_test, label_train, label_test\n",
        "\n",
        "\n",
        "img_train_grav, img_test_grav, label_train_grav, label_test_grav = split_dataset(f\"{dataset_dir}/grav\")    \n",
        "img_train_cont, img_test_cont, label_train_cont, label_test_cont = split_dataset(f\"{dataset_dir}/cont\")    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "a5dd32b6-7548-420a-b580-6e1e8f9b8afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 1159,test: 498\n",
            "train: 1159,test: 497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨\n",
        "#ã‚‚ã—dst_folderãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦æ–°ã—ãä½œã‚Šç›´ã™\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_train_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_test_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in img_test_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_train_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")            \n",
        "for file in label_test_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n",
        "for file in label_test_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\") \n",
        "\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "b061dbdd-c2cf-4095-a235-e5ea1e606a92"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # classåã‚’å®šç¾©"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52db6d4b-b48f-4a67-ed30-aca6118c3be5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "0b78b86d-52d2-428b-d42a-3e55736c360e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ğŸš€ v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (12 CPUs, 83.5 GB RAM, 25.8/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 15 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b7baa2-c200-4893-ddc4-700ed493a0b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n.pt, cfg=, data=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=15, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ğŸš€ v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ğŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ğŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-01-27 20:28:05.693639: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\n",
            "100% 3.87M/3.87M [00:00<00:00, 4.87MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1      9471  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "Model summary: 214 layers, 1766623 parameters, 1766623 gradients, 4.2 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5n.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/labels.cache... 1159 images, 1159 backgrounds, 0 corrupt: 100% 2318/2318 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/labels.cache... 497 images, 498 backgrounds, 0 corrupt: 100% 995/995 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m1.68 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/14      2.06G    0.05282    0.01795    0.00892         23        640: 100% 145/145 [00:45<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:31<00:00,  1.03it/s]\n",
            "                   all        995        497    0.00842          1      0.355      0.144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/14      2.06G    0.03439    0.01154   0.001349         11        640: 100% 145/145 [00:15<00:00,  9.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:04<00:00,  7.70it/s]\n",
            "                   all        995        497     0.0712          1      0.335      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/14      2.06G    0.03098    0.01028    0.00113         18        640: 100% 145/145 [00:15<00:00,  9.63it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.16it/s]\n",
            "                   all        995        497      0.215      0.958      0.339      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/14      2.06G      0.027   0.009755  0.0009786         14        640: 100% 145/145 [00:14<00:00,  9.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.02it/s]\n",
            "                   all        995        497      0.346      0.998      0.484      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/14      2.06G    0.02345   0.008701  0.0007495         16        640: 100% 145/145 [00:14<00:00,  9.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.11it/s]\n",
            "                   all        995        497      0.315      0.994      0.469      0.315\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/14      2.06G    0.02193   0.008398  0.0005615         21        640: 100% 145/145 [00:14<00:00,  9.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.20it/s]\n",
            "                   all        995        497      0.458      0.988      0.478      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/14      2.06G    0.01952   0.007968  0.0004489         17        640: 100% 145/145 [00:14<00:00,  9.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.26it/s]\n",
            "                   all        995        497      0.406      0.827      0.372      0.284\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/14      2.06G    0.01788   0.007505  0.0003743         19        640: 100% 145/145 [00:14<00:00,  9.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.24it/s]\n",
            "                   all        995        497      0.466      0.895      0.442      0.318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/14      2.06G    0.01648   0.007172  0.0003326         23        640: 100% 145/145 [00:14<00:00,  9.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.22it/s]\n",
            "                   all        995        497      0.484      0.992      0.485      0.329\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/14      2.06G    0.01645   0.007203  0.0002796         16        640: 100% 145/145 [00:14<00:00,  9.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.61it/s]\n",
            "                   all        995        497      0.425      0.763      0.375      0.279\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/14      2.06G    0.01463   0.006873  0.0002385         25        640: 100% 145/145 [00:14<00:00,  9.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.50it/s]\n",
            "                   all        995        497      0.408       0.69      0.345      0.277\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/14      2.06G    0.01631   0.006794  0.0002156         26        640: 100% 145/145 [00:14<00:00,  9.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.84it/s]\n",
            "                   all        995        497      0.324      0.477      0.239      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/14      2.06G    0.01348   0.006585  0.0001754         19        640: 100% 145/145 [00:14<00:00,  9.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.59it/s]\n",
            "                   all        995        497      0.337      0.519      0.256      0.209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/14      2.06G    0.01398    0.00662  0.0001506         21        640: 100% 145/145 [00:14<00:00,  9.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  8.85it/s]\n",
            "                   all        995        497      0.265      0.354      0.179      0.147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/14      2.06G    0.01394   0.006459  0.0001381         15        640: 100% 145/145 [00:14<00:00,  9.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:03<00:00,  9.04it/s]\n",
            "                   all        995        497      0.223      0.278      0.143      0.122\n",
            "\n",
            "15 epochs completed in 0.095 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 3.8MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 3.8MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 32/32 [00:04<00:00,  7.22it/s]\n",
            "                   all        995        497      0.459      0.988      0.478      0.354\n",
            "                  cont        995        497      0.459      0.988      0.478      0.354\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNT6kpnHrTR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du5NiwCDdTcX",
        "outputId": "9d06e439-986e-4418-fef7-42f5e098fc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ğŸš€ v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/995 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/1000.JPG: 448x640 (no detections), 15.1ms\n",
            "image 2/995 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/1001-20161012-56-115135_b01830ce1019599e877728b9399534803f202413c1038fc6bfeb02fe15cf1b8b.JPG: 448x640 (no detections), 9.9ms\n",
            "image 3/995 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/1001-20161228-56-114735_b065c1be18724ae35503fa94b403e79491e5fc429879add4c06170e9af23d485.JPG: 448x640 (no detections), 9.9ms\n",
            "image 4/995 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/1001-20170222-56-104802_6f352b1a410abdcb3b575f34f25de29db69fc5eeb5cdc33bb69eb738828ea7e2.JPG: 448x640 (no detections), 9.8ms\n"
          ]
        }
      ]
    }
  ]
}