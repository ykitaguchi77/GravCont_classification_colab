{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Extend_dataset_YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend dataset YOLOv5**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b81165-56eb-4777-9a91-6ee5c2da5fec"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10b8224-aabc-44cb-fe0d-b6c9f57957de"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.76"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "#テスト画像\n",
        "test_path = '/content/drive/MyDrive/Deep_learning/Face_Images/IMG_3110.JPG'\n",
        "\n",
        "# GO_extended_datasetを colab上のフォルダに展開\n",
        "zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "!unzip $zip_path -d \"/content\"\n",
        "in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "#保存先フォルダ\n",
        "out_path_list = ['/content/GO_extended_dataset/cont_for_yolo', '/content/GO_extended_dataset/grav_for_yolo']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2mKFMmwfTOi"
      },
      "source": [
        "#顔の画像から目を検出して切り抜くスクリプト\n",
        "・Haarcascade_eyeを使用<br>\n",
        "・目が検出できないものはskipする<br>\n",
        "・横幅を1/4倍、縦幅を上下に1/4倍追加して画像を切り取る（目の全幅、眉毛が含まれるように）\n",
        "\n",
        "・切り取った画像を横幅640pxにresizeする<br>\n",
        "・フォルダ内の画像を一括変換して別フォルダに保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0shkziUPn1c"
      },
      "source": [
        "#Haarcascadeを指定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfu_RX-kIlmx"
      },
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "# righteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_righteye_2splits.xml'\n",
        "# lefteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_lefteye_2splits.xml'\n",
        "\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# righteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# lefteye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#アスペクト比を維持したまま横を400pixelに縮小する\n",
        "def scale_to_width(img, width):\n",
        "    scale = width / img.shape[1]\n",
        "    return cv2.resize(img, dsize=None, fx=scale, fy=scale)\n",
        "\n",
        "#図を表示する\n",
        "def show_image(img):\n",
        "    #img = cv2.imread(out_path)\n",
        "    dst = scale_to_width(img, 200)\n",
        "    cv2_imshow(dst)\n",
        "\n",
        "# def show_image_pillow(img):\n",
        "#     src = cv2.cvtColor(img_resized_list[0], cv2.COLOR_BGR2RGB)\n",
        "#     plt.imshow(src)\n",
        "\n",
        "def my_round(val, digit=0):\n",
        "    p = 10 ** digit\n",
        "    return int((val * p * 2 + 1) // 2 / p)\n",
        "\n",
        "def scale_to_width(img, width):\n",
        "    \"\"\"幅が指定した値になるように、アスペクト比を固定して、リサイズする。\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    height = round(h * (width / w))\n",
        "    dst = cv2.resize(img, dsize=(width, height))\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "SMbQsnPTHf-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_bilateral(in_path, class_num, size, showImage=True):\n",
        "    img_resized_list,side_list = [],[]\n",
        "\n",
        "    img = cv2.imread(in_path) \n",
        "    img2 = img.copy()\n",
        "\n",
        "    if showImage:\n",
        "        show_image(img)\n",
        "\n",
        "    # 画像グレースケール化\n",
        "    grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    #300pix以上のもので目に見えるものを抽出\n",
        "    eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "    print(\"\")\n",
        "    print('image path = ',in_path)\n",
        "\n",
        "    # 眼検出判定\n",
        "    if len(eye_list) >= 1:\n",
        "        print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "        pass\n",
        "    else:\n",
        "        print(\"no eye detected\")\n",
        "        pass\n",
        "\n",
        "    print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "    #画像の切り抜きと保存（連番にする）\n",
        "    if len(eye_list)== 2: \n",
        "\n",
        "        \n",
        "        for (ex, ey, ew, eh) in eye_list:\n",
        "            print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "            \n",
        "            try:\n",
        "                cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "\n",
        "                #切り抜き範囲が元画像をはみ出る場合は黒画像で埋める\n",
        "                top = max(0, ey-int(eh/4))\n",
        "                bottom = min(grayscale_img.shape[0], int(ey + eh*5/4))\n",
        "                left = max(0,int(ex-int(ew/4)))\n",
        "                right = min(grayscale_img.shape[1], int(ex + ew*5/4))\n",
        "\n",
        "                #print(f\"top:{top}, bottom:{bottom}, left:{left}, right:{right}\")\n",
        "\n",
        "                img_cropped = img[top: bottom,left:right]\n",
        "                height, width = img_cropped.shape[:2]\n",
        "\n",
        "                #クロップした画像を表示\n",
        "                if showImage:\n",
        "                    show_image(img_cropped)\n",
        "            except: \n",
        "                pass\n",
        "\n",
        "       \n",
        "        ex = min(eye_list[0][0], eye_list[1][0])\n",
        "        ey = min(eye_list[0][1], eye_list[1][1])\n",
        "        ew = max(eye_list[0][0]+eye_list[0][2], eye_list[1][0]+eye_list[1][2]) - ex\n",
        "        eh = max(eye_list[0][1]+eye_list[0][3], eye_list[1][1]+eye_list[1][3]) - ey\n",
        "\n",
        "        print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "\n",
        "        try:\n",
        "            cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "\n",
        "            #切り抜き範囲が元画像をはみ出る場合は黒画像で埋める\n",
        "            top = max(0, int(ey-eh/4))\n",
        "            bottom = min(grayscale_img.shape[0], int(ey+7/6*eh))\n",
        "            left = max(0,int(ex-ew/10))\n",
        "            right = min(grayscale_img.shape[1], int(ex + 11/10*ew))\n",
        "\n",
        "            print(f\"top:{top}, bottom:{bottom}, left:{left}, right:{right}\")\n",
        "\n",
        "            img_cropped = img[top: bottom,left:right]\n",
        "            height, width = img_cropped.shape[:2]\n",
        "\n",
        "            img_resized = scale_to_width(img_cropped, size) #1辺を指定したpixにresize \n",
        "\n",
        "            #クロップした画像を表示\n",
        "            if showImage:\n",
        "                show_image(img_resized)\n",
        "            print(img2.shape)\n",
        "\n",
        "            #縦、横に対する割合\n",
        "            X = round((right+left)/2/img2.shape[1], 6)\n",
        "            Y = round((top+bottom)/2/img2.shape[0], 6)\n",
        "            W = round((right-left)/img.shape[1], 6)\n",
        "            H = round((bottom-top)/img.shape[0], 6)\n",
        "            txt = f\"{class_num} {X} {Y} {W} {H}\"\n",
        "            return img_resized, txt\n",
        "\n",
        "        except:\n",
        "            print('crop error')\n"
      ],
      "metadata": {
        "id": "aR56JrxipDoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, txt = crop_bilateral(test_path, class_num=0, size=640)\n",
        "print(txt)\n",
        "\n",
        "with open(\"test.txt\", mode='w') as f:\n",
        "    f.write(txt)\n",
        "\n",
        "\n",
        "# #検出された画像を確認\n",
        "# src = cv2.cvtColor(img_resized_list[0], cv2.COLOR_BGR2RGB)\n",
        "# plt.imshow(src)\n"
      ],
      "metadata": {
        "id": "sQkblQvvgl9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Aquisition of bounding boxes**\n",
        "\n",
        "画像とラベルを指定フォルダに保存"
      ],
      "metadata": {
        "id": "Z8R1kiD9lU-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# orig_folder = \"/content/GO_extended_dataset/Control_photo_1886mai\" \n",
        "# dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont\"\n",
        "# class_num = 0 #classをテキストに書き込む(0:cont, 1:grav)\n",
        "\n",
        "\n",
        "orig_folder = \"/content/GO_extended_dataset/treatable\" \n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/grav\"\n",
        "class_num = 1 #classをテキストに書き込む(0:cont, 1:grav)\n",
        "\n",
        "path_list = glob.glob(orig_folder+\"/*\")\n",
        "path = path_list[1]\n",
        "path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8gIkLcQWmH9d",
        "outputId": "2288c940-9d4d-4fdd-b40a-fe9376e67bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/GO_extended_dataset/treatable/5906-20190509-75-102501_1b7f8b82f6c7c4a016f4dae73d0c91cc3e05404f4cdd6eca55b2b48e4dba2c08.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = glob.glob(orig_folder+\"/*\")\n",
        "#path_list = [path_list[0]] #テスト用\n",
        "dst_folder = dst_folder\n",
        "\n",
        "#処理時間の計測\n",
        "start = time.time()\n",
        "\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/images\") #image格納用\n",
        "os.makedirs(f\"{dst_folder}/images_cropped\") #cropped_image格納用 (YOLOでは使用しない)\n",
        "os.makedirs(f\"{dst_folder}/labels\") #label格納用\n",
        "\n",
        "\n",
        "num=0\n",
        "for path in path_list:\n",
        "    try: #目が検出されなかった場合のエラー回避\n",
        "        img, txt = crop_bilateral(path, class_num=class_num, size=640, showImage=False)  #両眼抜き出して640pxで保存（crop時にエラーが出るものは削除される）\n",
        "        img2 = cv2.imread(path).copy()\n",
        "        img2 = scale_to_width(img2, 640)  #切り抜く前の画像を横のサイズを640になるように縮小\n",
        "        cv2.imwrite(f\"{dst_folder}/images/{os.path.basename(path).split('.')[0]}.JPG\", img2) #cropせずに縮小したイメージを保存\n",
        "        cv2.imwrite(f\"{dst_folder}/images_cropped/{os.path.basename(path).split('.')[0]}.JPG\", img) #cropしたイメージを保存する場合\n",
        "\n",
        "        with open(f\"{dst_folder}/labels/{os.path.basename(path).split('.')[0]}.txt\", mode='w') as f:\n",
        "            f.write(txt)\n",
        "        num+=1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\")\n",
        "print('Process done!!')\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "print (f\"image_num:{num}\")\n"
      ],
      "metadata": {
        "id": "R8k494hphZs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLO_v5 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels \n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YiuPobEMYWKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "def split_dataset(dataset_dir):\n",
        "    img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "    img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "    # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "    label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "    label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "    print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "    return img_train, img_test, label_train, label_test\n",
        "\n",
        "\n",
        "img_train_grav, img_test_grav, label_train_grav, label_test_grav = split_dataset(f\"{dataset_dir}/grav\")    \n",
        "img_train_cont, img_test_cont, label_train_cont, label_test_cont = split_dataset(f\"{dataset_dir}/cont\")    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "a5dd32b6-7548-420a-b580-6e1e8f9b8afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 1159,test: 498\n",
            "train: 1159,test: 497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_train_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_test_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in img_test_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_train_grav:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")            \n",
        "for file in label_test_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n",
        "for file in label_test_cont:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\") \n",
        "\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "b061dbdd-c2cf-4095-a235-e5ea1e606a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52db6d4b-b48f-4a67-ed30-aca6118c3be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "2160743c-4be1-48e0-8187-a7b8bdb738ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 15 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "6541bd69-e3a6-42de-fea1-b957591086d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2318 995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[0]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "b04b0e97-e10b-49fc-aac5-add281059ab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/5913-20200221-24-103529_808035888f8a9b2d3f0268d69e4700a10310e721d1dbbe9c0de89d0625ea061d.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/5913-20200221-24-103529_808035888f8a9b2d3f0268d69e4700a10310e721d1dbbe9c0de89d0625ea061d.JPG: 448x640 1 grav, 38.0ms\n",
            "Speed: 0.6ms pre-process, 38.0ms inference, 1.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "#stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "#imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#             transforms.Resize(size=(480,640)),\n",
        "#             transforms.ToTensor(),\n",
        "#             # transforms.Normalize(\n",
        "#             #     mean=[0.5, 0.5, 0.5],\n",
        "#             #     std=[0.5, 0.5, 0.5]\n",
        "#             #    )\n",
        "#             ])\n",
        "\n",
        "img_pil = Image.open(img)  #PILで開く(streamlitの仕様)\n",
        "img_numpy = np.array(img_pil, dtype=np.uint8)\n",
        "img_cv2 = cv2.cvtColor(img_numpy, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#img_cv2 = cv2.imread(img)\n",
        "img_cv2 = cv2.resize(img_cv2, dsize=(640, 640)) #480*600pxにリサイズ\n",
        "img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "#img_tensor = transform(img_np)\n",
        "img_tensor /= 255\n",
        "print(img_tensor.shape)\n",
        "\n",
        "print(img_tensor)\n",
        "img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "print(f\"pred: {pred}\")"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY",
        "outputId": "245ff6a5-eaf6-4953-a23e-ad56bd80c5aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 640, 640])\n",
            "tensor([[[0.14118, 0.11765, 0.16078,  ..., 0.35294, 0.25490, 0.14902],\n",
            "         [0.13725, 0.11373, 0.17255,  ..., 0.32549, 0.27059, 0.20000],\n",
            "         [0.12941, 0.12549, 0.18039,  ..., 0.27451, 0.30196, 0.27843],\n",
            "         ...,\n",
            "         [0.05882, 0.05882, 0.06667,  ..., 0.13725, 0.15294, 0.14510],\n",
            "         [0.06275, 0.05490, 0.05882,  ..., 0.13725, 0.14902, 0.12941],\n",
            "         [0.06667, 0.05490, 0.05882,  ..., 0.14118, 0.14510, 0.12157]],\n",
            "\n",
            "        [[0.10196, 0.08235, 0.12549,  ..., 0.32941, 0.23137, 0.12549],\n",
            "         [0.09804, 0.07843, 0.13725,  ..., 0.30196, 0.24706, 0.17647],\n",
            "         [0.09020, 0.09020, 0.14902,  ..., 0.25098, 0.27843, 0.25490],\n",
            "         ...,\n",
            "         [0.05490, 0.05098, 0.05882,  ..., 0.12941, 0.14510, 0.13725],\n",
            "         [0.05882, 0.04706, 0.05098,  ..., 0.12941, 0.14118, 0.12157],\n",
            "         [0.06275, 0.04706, 0.05098,  ..., 0.13333, 0.13725, 0.11373]],\n",
            "\n",
            "        [[0.09412, 0.06275, 0.10588,  ..., 0.32941, 0.23137, 0.12549],\n",
            "         [0.09020, 0.05882, 0.11765,  ..., 0.30196, 0.24706, 0.17647],\n",
            "         [0.08235, 0.07059, 0.12549,  ..., 0.25098, 0.27843, 0.25490],\n",
            "         ...,\n",
            "         [0.03922, 0.06275, 0.07059,  ..., 0.13333, 0.14902, 0.14118],\n",
            "         [0.04314, 0.05882, 0.06275,  ..., 0.13333, 0.14510, 0.12549],\n",
            "         [0.04706, 0.05882, 0.06275,  ..., 0.13725, 0.14118, 0.11765]]])\n",
            "pred: [tensor([[1.92689e+01, 1.40332e+02, 6.01923e+02, 4.73685e+02, 3.85235e-01, 1.00000e+00]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}