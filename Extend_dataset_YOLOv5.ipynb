{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Extend_dataset_YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend dataset YOLOv5**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "09eb9ae8-a61f-4efa-c11b-f01f65ce2a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb  9 15:59:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b90f7a-bad8-44b0-9f31-e48911729719"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cd2009-9470-41b2-d6d5-05976d3e8312"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.80"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "#テスト画像\n",
        "test_path = '/content/drive/MyDrive/Deep_learning/Face_Images/IMG_3110.JPG'\n",
        "\n",
        "# GO_extended_datasetを colab上のフォルダに展開\n",
        "zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "!unzip $zip_path -d \"/content\"\n",
        "in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "#保存先フォルダ\n",
        "out_path_list = ['/content/GO_extended_dataset/cont_for_yolo', '/content/GO_extended_dataset/grav_for_yolo']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2mKFMmwfTOi"
      },
      "source": [
        "#顔の画像から目を検出して切り抜くスクリプト\n",
        "・Haarcascade_eyeを使用<br>\n",
        "・目が検出できないものはskipする<br>\n",
        "・横幅を1/4倍、縦幅を上下に1/4倍追加して画像を切り取る（目の全幅、眉毛が含まれるように）\n",
        "\n",
        "・切り取った画像を横幅640pxにresizeする<br>\n",
        "・フォルダ内の画像を一括変換して別フォルダに保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0shkziUPn1c"
      },
      "source": [
        "#Haarcascadeを指定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfu_RX-kIlmx"
      },
      "source": [
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "# righteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_righteye_2splits.xml'\n",
        "# lefteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_lefteye_2splits.xml'\n",
        "\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# righteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# lefteye_cascade = cv2.CascadeClassifier(eye_cascade_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#アスペクト比を維持したまま横を400pixelに縮小する\n",
        "def scale_to_width(img, width):\n",
        "    scale = width / img.shape[1]\n",
        "    return cv2.resize(img, dsize=None, fx=scale, fy=scale)\n",
        "\n",
        "#図を表示する\n",
        "def show_image(img):\n",
        "    #img = cv2.imread(out_path)\n",
        "    dst = scale_to_width(img, 200)\n",
        "    cv2_imshow(dst)\n",
        "\n",
        "# def show_image_pillow(img):\n",
        "#     src = cv2.cvtColor(img_resized_list[0], cv2.COLOR_BGR2RGB)\n",
        "#     plt.imshow(src)\n",
        "\n",
        "def my_round(val, digit=0):\n",
        "    p = 10 ** digit\n",
        "    return int((val * p * 2 + 1) // 2 / p)\n",
        "\n",
        "def scale_to_width(img, width):\n",
        "    \"\"\"幅が指定した値になるように、アスペクト比を固定して、リサイズする。\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    height = round(h * (width / w))\n",
        "    dst = cv2.resize(img, dsize=(width, height))\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "SMbQsnPTHf-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_bilateral(in_path, class_num, size, showImage=True):\n",
        "    img_resized_list,side_list = [],[]\n",
        "\n",
        "    img = cv2.imread(in_path) \n",
        "    img2 = img.copy()\n",
        "\n",
        "    if showImage:\n",
        "        show_image(img)\n",
        "\n",
        "    # 画像グレースケール化\n",
        "    grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    #300pix以上のもので目に見えるものを抽出\n",
        "    eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(300, 300))\n",
        "    print(\"\")\n",
        "    print('image path = ',in_path)\n",
        "\n",
        "    # 眼検出判定\n",
        "    if len(eye_list) >= 1:\n",
        "        print('目が' + str(len(eye_list)) +'個検出されました')\n",
        "        pass\n",
        "    else:\n",
        "        print(\"no eye detected\")\n",
        "        pass\n",
        "\n",
        "    print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "    #画像の切り抜きと保存（連番にする）\n",
        "    if len(eye_list)== 2: \n",
        "\n",
        "        \n",
        "        for (ex, ey, ew, eh) in eye_list:\n",
        "            print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "            \n",
        "            try:\n",
        "                cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "\n",
        "                #切り抜き範囲が元画像をはみ出る場合は黒画像で埋める\n",
        "                top = max(0, ey-int(eh/4))\n",
        "                bottom = min(grayscale_img.shape[0], int(ey + eh*5/4))\n",
        "                left = max(0,int(ex-int(ew/4)))\n",
        "                right = min(grayscale_img.shape[1], int(ex + ew*5/4))\n",
        "\n",
        "                #print(f\"top:{top}, bottom:{bottom}, left:{left}, right:{right}\")\n",
        "\n",
        "                img_cropped = img[top: bottom,left:right]\n",
        "                height, width = img_cropped.shape[:2]\n",
        "\n",
        "                #クロップした画像を表示\n",
        "                if showImage:\n",
        "                    show_image(img_cropped)\n",
        "            except: \n",
        "                pass\n",
        "\n",
        "       \n",
        "        ex = min(eye_list[0][0], eye_list[1][0])\n",
        "        ey = min(eye_list[0][1], eye_list[1][1])\n",
        "        ew = max(eye_list[0][0]+eye_list[0][2], eye_list[1][0]+eye_list[1][2]) - ex\n",
        "        eh = max(eye_list[0][1]+eye_list[0][3], eye_list[1][1]+eye_list[1][3]) - ey\n",
        "\n",
        "        print(\"[ex,ey] = %d,%d [ew,eh] = %d,%d\" %(ex, ey, ew, eh))\n",
        "\n",
        "        try:\n",
        "            cv2.rectangle(img2, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "\n",
        "            #切り抜き範囲が元画像をはみ出る場合は黒画像で埋める\n",
        "            top = max(0, int(ey-eh/4))\n",
        "            bottom = min(grayscale_img.shape[0], int(ey+7/6*eh))\n",
        "            left = max(0,int(ex-ew/10))\n",
        "            right = min(grayscale_img.shape[1], int(ex + 11/10*ew))\n",
        "\n",
        "            print(f\"top:{top}, bottom:{bottom}, left:{left}, right:{right}\")\n",
        "\n",
        "            img_cropped = img[top: bottom,left:right]\n",
        "            height, width = img_cropped.shape[:2]\n",
        "\n",
        "            img_resized = scale_to_width(img_cropped, size) #1辺を指定したpixにresize \n",
        "\n",
        "            #クロップした画像を表示\n",
        "            if showImage:\n",
        "                show_image(img_resized)\n",
        "            print(img2.shape)\n",
        "\n",
        "            #縦、横に対する割合\n",
        "            X = round((right+left)/2/img2.shape[1], 6)\n",
        "            Y = round((top+bottom)/2/img2.shape[0], 6)\n",
        "            W = round((right-left)/img.shape[1], 6)\n",
        "            H = round((bottom-top)/img.shape[0], 6)\n",
        "            txt = f\"{class_num} {X} {Y} {W} {H}\"\n",
        "            return img_resized, txt\n",
        "\n",
        "        except:\n",
        "            print('crop error')\n"
      ],
      "metadata": {
        "id": "aR56JrxipDoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, txt = crop_bilateral(test_path, class_num=0, size=640)\n",
        "print(txt)\n",
        "\n",
        "with open(\"test.txt\", mode='w') as f:\n",
        "    f.write(txt)\n",
        "\n",
        "\n",
        "# #検出された画像を確認\n",
        "# src = cv2.cvtColor(img_resized_list[0], cv2.COLOR_BGR2RGB)\n",
        "# plt.imshow(src)\n"
      ],
      "metadata": {
        "id": "sQkblQvvgl9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Aquisition of bounding boxes**\n",
        "\n",
        "画像とラベルを指定フォルダに保存"
      ],
      "metadata": {
        "id": "Z8R1kiD9lU-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# orig_folder = \"/content/GO_extended_dataset/Control_photo_1886mai\" \n",
        "# dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont\"\n",
        "# class_num = 0 #classをテキストに書き込む(0:cont, 1:grav)\n",
        "\n",
        "\n",
        "orig_folder = \"/content/GO_extended_dataset/treatable\" \n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/grav\"\n",
        "class_num = 1 #classをテキストに書き込む(0:cont, 1:grav)\n",
        "\n",
        "path_list = glob.glob(orig_folder+\"/*\")\n",
        "path = path_list[1]\n",
        "path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8gIkLcQWmH9d",
        "outputId": "2288c940-9d4d-4fdd-b40a-fe9376e67bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/GO_extended_dataset/treatable/5906-20190509-75-102501_1b7f8b82f6c7c4a016f4dae73d0c91cc3e05404f4cdd6eca55b2b48e4dba2c08.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = glob.glob(orig_folder+\"/*\")\n",
        "#path_list = [path_list[0]] #テスト用\n",
        "dst_folder = dst_folder\n",
        "\n",
        "#処理時間の計測\n",
        "start = time.time()\n",
        "\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/images\") #image格納用\n",
        "os.makedirs(f\"{dst_folder}/images_cropped\") #cropped_image格納用 (YOLOでは使用しない)\n",
        "os.makedirs(f\"{dst_folder}/labels\") #label格納用\n",
        "\n",
        "\n",
        "num=0\n",
        "for path in path_list:\n",
        "    try: #目が検出されなかった場合のエラー回避\n",
        "        img, txt = crop_bilateral(path, class_num=class_num, size=640, showImage=False)  #両眼抜き出して640pxで保存（crop時にエラーが出るものは削除される）\n",
        "        img2 = cv2.imread(path).copy()\n",
        "        img2 = scale_to_width(img2, 640)  #切り抜く前の画像を横のサイズを640になるように縮小\n",
        "        cv2.imwrite(f\"{dst_folder}/images/{os.path.basename(path).split('.')[0]}.JPG\", img2) #cropせずに縮小したイメージを保存\n",
        "        cv2.imwrite(f\"{dst_folder}/images_cropped/{os.path.basename(path).split('.')[0]}.JPG\", img) #cropしたイメージを保存する場合\n",
        "\n",
        "        with open(f\"{dst_folder}/labels/{os.path.basename(path).split('.')[0]}.txt\", mode='w') as f:\n",
        "            f.write(txt)\n",
        "        num+=1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\")\n",
        "print('Process done!!')\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "print (f\"image_num:{num}\")\n"
      ],
      "metadata": {
        "id": "R8k494hphZs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLO_v5 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels \n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YiuPobEMYWKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "    \n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "a5a7abde-c6c0-443b-e15d-c904b1b0f2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 1657\n",
            "cont: 1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GroupKfolds\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "num_folds = 5 #number of folds\n",
        "\n",
        "img_train, img_val, label_train, label_val =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(cont_image_list, groups=cont_id_idx):\n",
        "    for idx in train_idxs:\n",
        "        img_train[i].append(cont_image_list[idx])\n",
        "        label_train[i].append(cont_label_list[idx])\n",
        "    for idx in val_idxs:\n",
        "        img_val[i].append(cont_image_list[idx])\n",
        "        label_val[i].append(cont_label_list[idx])    \n",
        "    i+=1\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(grav_image_list, groups=grav_id_idx):\n",
        "    for idx in train_idxs:\n",
        "        img_train[i].append(grav_image_list[idx])\n",
        "        label_train[i].append(grav_label_list[idx])\n",
        "    for idx in val_idxs:\n",
        "        img_val[i].append(grav_image_list[idx])\n",
        "        label_val[i].append(grav_label_list[idx])    \n",
        "    i+=1\n",
        "\n",
        "print(f\"img_train: {len(img_train[0])}\")    \n",
        "print(f\"img_val: {len(img_val[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ypkajhq4Yjp",
        "outputId": "9d94aa18-858f-4a27-a212-45a27857f714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_train: 2649\n",
            "img_val: 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")            \n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\") \n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "5d8631eb-b096-471d-96c5-0f3913a7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e0bdb6-f54e-47f9-e9f4-413245975212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "476d809b-269b-4cd6-a0fe-82f4da46dd53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1261e6c4-dd82-4034-ed52-e063d58ca44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n.pt, cfg=, data=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-01-31 09:30:02.140933: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\n",
            "100% 3.87M/3.87M [00:00<00:00, 157MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1      9471  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "Model summary: 214 layers, 1766623 parameters, 1766623 gradients, 4.2 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5n.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/labels... 2649 images, 0 backgrounds, 0 corrupt: 100% 2649/2649 [00:05<00:00, 508.53it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/labels... 664 images, 0 backgrounds, 0 corrupt: 100% 664/664 [00:02<00:00, 308.58it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/labels.cache\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m1.70 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/99      1.91G    0.05312    0.02456     0.0208         30        640: 100% 166/166 [00:22<00:00,  7.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:04<00:00,  4.27it/s]\n",
            "                   all        664        664      0.379      0.639       0.39      0.143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/99      1.91G    0.03314    0.01597    0.01893         22        640: 100% 166/166 [00:18<00:00,  9.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.53it/s]\n",
            "                   all        664        664      0.468      0.908      0.564      0.279\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/99      1.91G    0.02925    0.01351    0.01724         22        640: 100% 166/166 [00:17<00:00,  9.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.50it/s]\n",
            "                   all        664        664      0.806      0.866      0.898      0.527\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/99      1.91G    0.02538    0.01192    0.01449         24        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.75it/s]\n",
            "                   all        664        664      0.775      0.794      0.831      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/99      1.91G    0.02223     0.0112    0.01398         27        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.91it/s]\n",
            "                   all        664        664      0.678      0.843      0.867      0.418\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/99      1.91G    0.02158    0.01106     0.0139         27        640: 100% 166/166 [00:17<00:00,  9.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.68it/s]\n",
            "                   all        664        664      0.896      0.901      0.942      0.702\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/99      1.91G    0.02025     0.0103    0.01212         27        640: 100% 166/166 [00:17<00:00,  9.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.48it/s]\n",
            "                   all        664        664      0.826      0.873      0.941      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/99      1.91G    0.01846    0.01032    0.01212         28        640: 100% 166/166 [00:17<00:00,  9.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.56it/s]\n",
            "                   all        664        664      0.907      0.922      0.958      0.679\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/99      1.91G      0.018   0.009884     0.0122         28        640: 100% 166/166 [00:17<00:00,  9.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.81it/s]\n",
            "                   all        664        664      0.601      0.705      0.684      0.459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/99      1.91G    0.01836     0.0099    0.01262         27        640: 100% 166/166 [00:17<00:00,  9.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.85it/s]\n",
            "                   all        664        664      0.852      0.902      0.958      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/99      1.91G    0.01752   0.009799    0.01161         27        640: 100% 166/166 [00:17<00:00,  9.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.884        0.9      0.955      0.709\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/99      1.91G    0.01703   0.009689    0.01201         28        640: 100% 166/166 [00:17<00:00,  9.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.67it/s]\n",
            "                   all        664        664      0.828      0.862      0.932      0.742\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/99      1.91G    0.01723   0.009729    0.01189         20        640: 100% 166/166 [00:17<00:00,  9.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.71it/s]\n",
            "                   all        664        664      0.915      0.924      0.954      0.761\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/99      1.91G    0.01747   0.009538    0.01121         25        640: 100% 166/166 [00:17<00:00,  9.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.853      0.894      0.953      0.752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/99      1.91G    0.01648   0.009433    0.01169         25        640: 100% 166/166 [00:17<00:00,  9.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.831      0.901      0.947      0.743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/99      1.91G    0.01665   0.009478    0.01133         29        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.78it/s]\n",
            "                   all        664        664      0.893      0.919      0.964      0.787\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/99      1.91G    0.01621   0.009281    0.01061         29        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.59it/s]\n",
            "                   all        664        664      0.905      0.926      0.971      0.808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/99      1.91G    0.01611   0.009289    0.01055         23        640: 100% 166/166 [00:17<00:00,  9.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.59it/s]\n",
            "                   all        664        664      0.896      0.927      0.972      0.773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/99      1.91G    0.01644   0.009125    0.01103         29        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.80it/s]\n",
            "                   all        664        664      0.884      0.938      0.965      0.791\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/99      1.91G    0.01543   0.008854    0.01033         26        640: 100% 166/166 [00:17<00:00,  9.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664      0.906      0.914      0.969      0.745\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/99      1.91G    0.01583   0.008897    0.01057         26        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.895      0.942      0.974      0.732\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/99      1.91G    0.01415   0.008979    0.01013         21        640: 100% 166/166 [00:17<00:00,  9.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.59it/s]\n",
            "                   all        664        664      0.933      0.935      0.973      0.777\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/99      1.91G    0.01448   0.009125    0.01003         28        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664      0.905       0.96      0.976      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/99      1.91G    0.01447   0.008803   0.009932         22        640: 100% 166/166 [00:17<00:00,  9.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.58it/s]\n",
            "                   all        664        664      0.895      0.914      0.963      0.796\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/99      1.91G    0.01437   0.008833   0.009798         24        640: 100% 166/166 [00:17<00:00,  9.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.58it/s]\n",
            "                   all        664        664       0.91       0.95      0.977      0.802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/99      1.91G    0.01452   0.008891   0.009614         26        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.65it/s]\n",
            "                   all        664        664       0.91      0.913      0.977      0.806\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/99      1.91G     0.0141   0.008641   0.009001         19        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.62it/s]\n",
            "                   all        664        664      0.871      0.973      0.983      0.789\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/99      1.91G    0.01422   0.008863    0.01011         27        640: 100% 166/166 [00:17<00:00,  9.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.53it/s]\n",
            "                   all        664        664      0.822      0.915      0.965      0.757\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/99      1.91G    0.01415   0.008829     0.0103         27        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.78it/s]\n",
            "                   all        664        664      0.929       0.95      0.979      0.808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/99      1.91G    0.01344   0.008641   0.009028         23        640: 100% 166/166 [00:17<00:00,  9.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.44it/s]\n",
            "                   all        664        664      0.928      0.944      0.979      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/99      1.91G    0.01443   0.008515   0.009309         26        640: 100% 166/166 [00:17<00:00,  9.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.69it/s]\n",
            "                   all        664        664      0.911       0.92       0.97      0.793\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/99      1.91G    0.01411   0.008619   0.009714         19        640: 100% 166/166 [00:17<00:00,  9.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.71it/s]\n",
            "                   all        664        664      0.902       0.95      0.976      0.825\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/99      1.91G    0.01389    0.00862   0.009341         25        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.78it/s]\n",
            "                   all        664        664      0.895      0.908      0.977      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/99      1.91G    0.01388   0.008408   0.009004         29        640: 100% 166/166 [00:17<00:00,  9.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.79it/s]\n",
            "                   all        664        664      0.831      0.899      0.947      0.785\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/99      1.91G    0.01315   0.008404   0.008661         28        640: 100% 166/166 [00:17<00:00,  9.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.76it/s]\n",
            "                   all        664        664       0.93      0.926      0.979       0.82\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/99      1.91G    0.01362   0.008414   0.009474         23        640: 100% 166/166 [00:17<00:00,  9.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664      0.934      0.925      0.977      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/99      1.91G     0.0137   0.008387   0.008817         26        640: 100% 166/166 [00:17<00:00,  9.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.90it/s]\n",
            "                   all        664        664       0.92      0.917      0.967       0.79\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/99      1.91G    0.01354   0.008462   0.009194         29        640: 100% 166/166 [00:17<00:00,  9.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.918      0.921      0.979      0.804\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/99      1.91G    0.01314   0.008265    0.00925         25        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664      0.913      0.941      0.969      0.809\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/99      1.91G    0.01341   0.008396   0.009239         31        640: 100% 166/166 [00:17<00:00,  9.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.66it/s]\n",
            "                   all        664        664      0.922      0.928      0.977      0.799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/99      1.91G    0.01282   0.008312   0.008357         31        640: 100% 166/166 [00:17<00:00,  9.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.43it/s]\n",
            "                   all        664        664      0.904      0.953      0.975      0.815\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/99      1.91G    0.01312   0.008268   0.008458         27        640: 100% 166/166 [00:17<00:00,  9.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.93it/s]\n",
            "                   all        664        664      0.935      0.936      0.979      0.817\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/99      1.91G    0.01315   0.008422   0.007996         27        640: 100% 166/166 [00:17<00:00,  9.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.78it/s]\n",
            "                   all        664        664      0.917      0.923      0.976      0.827\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/99      1.91G    0.01298     0.0084   0.008252         25        640: 100% 166/166 [00:17<00:00,  9.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.58it/s]\n",
            "                   all        664        664      0.891       0.91       0.97      0.814\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/99      1.91G    0.01305   0.008144   0.008747         26        640: 100% 166/166 [00:17<00:00,  9.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.66it/s]\n",
            "                   all        664        664      0.877       0.96      0.976      0.844\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/99      1.91G     0.0129   0.008253   0.008063         27        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.67it/s]\n",
            "                   all        664        664      0.884      0.956      0.973      0.822\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/99      1.91G    0.01342   0.008242   0.009251         21        640: 100% 166/166 [00:17<00:00,  9.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.63it/s]\n",
            "                   all        664        664      0.933      0.931      0.973      0.801\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/99      1.91G    0.01289   0.008237   0.008811         22        640: 100% 166/166 [00:17<00:00,  9.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.59it/s]\n",
            "                   all        664        664      0.931      0.941      0.979      0.822\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/99      1.91G    0.01321   0.008168   0.008809         25        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.67it/s]\n",
            "                   all        664        664      0.946      0.914      0.978       0.81\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/99      1.91G    0.01207   0.008087    0.00777         23        640: 100% 166/166 [00:17<00:00,  9.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.75it/s]\n",
            "                   all        664        664      0.871      0.923      0.968      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      50/99      1.91G    0.01229   0.008075   0.008802         25        640: 100% 166/166 [00:17<00:00,  9.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.72it/s]\n",
            "                   all        664        664      0.942      0.949      0.984      0.838\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      51/99      1.91G    0.01339   0.008216   0.008693         30        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.67it/s]\n",
            "                   all        664        664      0.925       0.95      0.977      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      52/99      1.91G    0.01241   0.008089   0.008131         26        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.72it/s]\n",
            "                   all        664        664      0.944      0.958      0.982      0.815\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      53/99      1.91G    0.01341   0.008014   0.008492         28        640: 100% 166/166 [00:17<00:00,  9.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664      0.946      0.949      0.981       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      54/99      1.91G     0.0125   0.008117   0.008122         22        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.926      0.943       0.98      0.841\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      55/99      1.91G    0.01299   0.008027    0.00762         27        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.68it/s]\n",
            "                   all        664        664      0.927      0.943      0.977      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      56/99      1.91G     0.0123   0.008051   0.007768         26        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.59it/s]\n",
            "                   all        664        664      0.939      0.944      0.979      0.841\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      57/99      1.91G    0.01238    0.00816   0.008149         21        640: 100% 166/166 [00:17<00:00,  9.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.67it/s]\n",
            "                   all        664        664      0.938      0.946      0.982      0.831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      58/99      1.91G    0.01221   0.007963   0.007693         26        640: 100% 166/166 [00:17<00:00,  9.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.66it/s]\n",
            "                   all        664        664      0.935      0.947      0.982      0.837\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      59/99      1.91G    0.01225   0.007868   0.008116         24        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.74it/s]\n",
            "                   all        664        664       0.91      0.941      0.976       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      60/99      1.91G     0.0115   0.007913    0.00695         27        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.71it/s]\n",
            "                   all        664        664      0.941      0.939      0.976       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      61/99      1.91G    0.01244   0.007955   0.007397         21        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.916      0.928      0.972      0.832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      62/99      1.91G    0.01228    0.00781     0.0073         25        640: 100% 166/166 [00:17<00:00,  9.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.58it/s]\n",
            "                   all        664        664      0.937      0.928      0.973      0.834\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      63/99      1.91G    0.01209   0.008045   0.008584         25        640: 100% 166/166 [00:17<00:00,  9.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.74it/s]\n",
            "                   all        664        664      0.919      0.935      0.973      0.832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      64/99      1.91G    0.01217    0.00814   0.007944         33        640: 100% 166/166 [00:17<00:00,  9.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.77it/s]\n",
            "                   all        664        664      0.935      0.941      0.976      0.829\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      65/99      1.91G    0.01186   0.007735   0.007893         23        640: 100% 166/166 [00:17<00:00,  9.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.876      0.922      0.967      0.832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      66/99      1.91G     0.0121   0.007864   0.007351         25        640: 100% 166/166 [00:17<00:00,  9.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.69it/s]\n",
            "                   all        664        664      0.934      0.955       0.98      0.847\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      67/99      1.91G    0.01205    0.00788   0.007576         28        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.34it/s]\n",
            "                   all        664        664      0.924      0.953      0.978      0.836\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      68/99      1.91G    0.01184   0.007792   0.007579         27        640: 100% 166/166 [00:17<00:00,  9.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.73it/s]\n",
            "                   all        664        664      0.951      0.953      0.986      0.835\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      69/99      1.91G    0.01173   0.007783   0.007868         27        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.66it/s]\n",
            "                   all        664        664      0.889      0.928      0.974       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      70/99      1.91G    0.01136   0.007786   0.007072         29        640: 100% 166/166 [00:17<00:00,  9.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.65it/s]\n",
            "                   all        664        664      0.937      0.935      0.977      0.837\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      71/99      1.91G    0.01174   0.007784   0.007161         25        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.76it/s]\n",
            "                   all        664        664       0.94      0.955      0.982       0.84\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      72/99      1.91G    0.01187   0.007659   0.007676         24        640: 100% 166/166 [00:17<00:00,  9.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.75it/s]\n",
            "                   all        664        664      0.923      0.953      0.975      0.825\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      73/99      1.91G    0.01155   0.007871   0.007212         23        640: 100% 166/166 [00:17<00:00,  9.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.61it/s]\n",
            "                   all        664        664      0.946      0.952      0.981      0.825\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      74/99      1.91G    0.01168    0.00789   0.007564         25        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.63it/s]\n",
            "                   all        664        664      0.928      0.933      0.972      0.831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      75/99      1.91G    0.01172   0.007759   0.007225         27        640: 100% 166/166 [00:17<00:00,  9.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.75it/s]\n",
            "                   all        664        664      0.953      0.937      0.976      0.838\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      76/99      1.91G    0.01161   0.007758   0.006937         27        640: 100% 166/166 [00:17<00:00,  9.63it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.61it/s]\n",
            "                   all        664        664      0.942      0.937      0.976       0.84\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      77/99      1.91G    0.01081   0.007746   0.006949         31        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.60it/s]\n",
            "                   all        664        664      0.933       0.94      0.973      0.825\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      78/99      1.91G    0.01115   0.007726   0.007014         23        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.69it/s]\n",
            "                   all        664        664      0.939      0.948      0.981       0.85\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      79/99      1.91G    0.01079   0.007587   0.006314         25        640: 100% 166/166 [00:17<00:00,  9.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.73it/s]\n",
            "                   all        664        664      0.945      0.951      0.983      0.842\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      80/99      1.91G    0.01087   0.007717   0.006109         27        640: 100% 166/166 [00:17<00:00,  9.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.78it/s]\n",
            "                   all        664        664      0.936       0.94      0.982       0.84\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      81/99      1.91G    0.01132   0.007653   0.007145         24        640: 100% 166/166 [00:17<00:00,  9.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.68it/s]\n",
            "                   all        664        664      0.943      0.959      0.984      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      82/99      1.91G    0.01159   0.007749    0.00737         26        640: 100% 166/166 [00:17<00:00,  9.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.69it/s]\n",
            "                   all        664        664       0.93      0.958      0.983      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      83/99      1.91G    0.01159   0.007506   0.006489         25        640: 100% 166/166 [00:17<00:00,  9.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.70it/s]\n",
            "                   all        664        664      0.936      0.941      0.978      0.849\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      84/99      1.91G    0.01073   0.007517   0.006596         25        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.76it/s]\n",
            "                   all        664        664      0.936      0.945      0.979      0.845\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      85/99      1.91G     0.0105   0.007646   0.006193         26        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.73it/s]\n",
            "                   all        664        664      0.941      0.946      0.979      0.847\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      86/99      1.91G    0.01082   0.007753   0.006447         26        640: 100% 166/166 [00:17<00:00,  9.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.63it/s]\n",
            "                   all        664        664      0.937      0.961      0.981       0.85\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      87/99      1.91G    0.01111   0.007711   0.006343         25        640: 100% 166/166 [00:17<00:00,  9.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.73it/s]\n",
            "                   all        664        664      0.948      0.956      0.985      0.851\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      88/99      1.91G    0.01105   0.007484   0.006723         24        640: 100% 166/166 [00:17<00:00,  9.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.50it/s]\n",
            "                   all        664        664      0.943      0.927      0.975      0.847\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      89/99      1.91G    0.01085   0.007432    0.00721         24        640: 100% 166/166 [00:17<00:00,  9.63it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.75it/s]\n",
            "                   all        664        664      0.919      0.948      0.975      0.844\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      90/99      1.91G    0.01063   0.007593    0.00643         28        640: 100% 166/166 [00:17<00:00,  9.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.74it/s]\n",
            "                   all        664        664      0.938      0.956      0.983      0.849\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      91/99      1.91G    0.01028   0.007495   0.005539         24        640: 100% 166/166 [00:17<00:00,  9.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664      0.939      0.951      0.981      0.848\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      92/99      1.91G    0.01065   0.007452   0.006502         26        640: 100% 166/166 [00:17<00:00,  9.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.68it/s]\n",
            "                   all        664        664      0.927      0.936      0.975      0.841\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      93/99      1.91G    0.01083   0.007373   0.006362         25        640: 100% 166/166 [00:17<00:00,  9.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.64it/s]\n",
            "                   all        664        664       0.95      0.941      0.982      0.851\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      94/99      1.91G     0.0102   0.007557   0.005407         24        640: 100% 166/166 [00:17<00:00,  9.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.69it/s]\n",
            "                   all        664        664      0.942      0.952       0.98      0.853\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      95/99      1.91G    0.01025    0.00741   0.005452         30        640: 100% 166/166 [00:17<00:00,  9.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.71it/s]\n",
            "                   all        664        664      0.934      0.956      0.979      0.851\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      96/99      1.91G    0.01048   0.007359   0.005772         23        640: 100% 166/166 [00:17<00:00,  9.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.71it/s]\n",
            "                   all        664        664      0.952       0.94      0.979      0.843\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      97/99      1.91G    0.01063   0.007442    0.00605         22        640: 100% 166/166 [00:17<00:00,  9.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.76it/s]\n",
            "                   all        664        664      0.945      0.949       0.98       0.85\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      98/99      1.91G     0.0104   0.007532   0.005701         28        640: 100% 166/166 [00:17<00:00,  9.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.55it/s]\n",
            "                   all        664        664      0.946      0.949      0.981      0.849\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      99/99      1.91G    0.01063   0.007413   0.006232         29        640: 100% 166/166 [00:17<00:00,  9.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:03<00:00,  5.62it/s]\n",
            "                   all        664        664       0.94      0.962      0.982      0.844\n",
            "\n",
            "100 epochs completed in 0.596 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 3.8MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 3.8MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 21/21 [00:04<00:00,  4.92it/s]\n",
            "                   all        664        664      0.941      0.951       0.98      0.854\n",
            "                  cont        664        332      0.938      0.959       0.98      0.847\n",
            "                  grav        664        332      0.943      0.943      0.981      0.861\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951a6753-9a5b-4e71-e026-7416d32bcc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "430d02b5-7a51-42f0-f012-cd33f4d3178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2649 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "66105c9d-2766-4fff-ef56-107bb88cda87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG: 448x640 1 grav, 18.4ms\n",
            "Speed: 0.7ms pre-process, 18.4ms inference, 38.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8851aa3-9005-45b4-a358-1afb274dd14d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}