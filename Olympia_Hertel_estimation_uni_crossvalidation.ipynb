{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled72.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_estimation_uni_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJiUlScYrIgg"
      },
      "source": [
        "#**Olympia_Hertel_estimation_RepVGG-A2**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ1FDbAxrAsn",
        "outputId": "2eba104c-375d-4c66-fc27-c6a9db7b0566"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "import shutil\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 1 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "'''\n",
        "黒の空白を挿入することにより500px*500pxの画像を生成\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "Olympia_dataset----dataset_500px_divided----train (72%)\n",
        "                                            |\n",
        "                                            |--val (18%)\n",
        "                                            |\n",
        "                                            |--test (10%)\n",
        "\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 506 kB/s \n",
            "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (4.1.1)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n",
            "Random Seed:  1234\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XD7y5oqsMwg"
      },
      "source": [
        "#**Set Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Er_Gm6sRDv"
      },
      "source": [
        "path = '/content/drive/MyDrive/Deep_learning/Olympia_dataset'\n",
        "os.chdir(path)\n",
        "\n",
        "# grav or cont, age, and sex\n",
        "#NUM_CLASSES = 3\n",
        "\n",
        "# contains train, val\n",
        "#DATASET_PATH = r\"./dataset_500px\"\n",
        "DATASET_PATH = r\"./dataset_500px_uni\"\n",
        "#TRAIN_FOLDER_NAME = \"train\"\n",
        "#VAL_FOLDER_NAME = \"val\"\n",
        "#EFFICIENT_NET_NAME = \"RepVGG-A2-train\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"./Hertel_unilateral.csv\"\n",
        "#OPTIMIZER_PATH = \"./optimizer_multi.pth\"\n",
        "#SEX_DICT_PATH = \"gender_json\"\n",
        "#AGE_DICT_PATH = \"age_json\"\n",
        "LOG_PATH = \"./log_multi.txt\"\n",
        "ROC_PATH = \"./roc_multi.png\"\n",
        "#CHECKPOINT_COUNT = 10\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# transforms param　　左右分けているのでflipはしない\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])]) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-Foldに分割**"
      ],
      "metadata": {
        "id": "aLBpMuFwhz1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(DATASET_PATH)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"patiend num: {}\".format(len(id)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZXXWIO9k9Cq",
        "outputId": "1540e28b-e06c-4767-c429-87ed4232604c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patiend num: 2032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split ＋　Group K-foldを用いてデータセット分け\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9で分割\n",
        "X = np.ones(len(id))\n",
        "y = np.ones(len(id))\n",
        "groups = patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "id": "fzjZhrg3lrdU",
        "outputId": "daea383c-3e06-4b76-ff72-6615ebdcb2b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1462\n",
            "val_dataset: 366\n",
            "test_dataset: 204\n",
            "\n",
            "extracted_id (example): 491_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "num_folds = 5 #number of folds\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "#まず全体の1割をテストセットとしてよけておく\n",
        "remain_set, test_set = train_test_split(path_list, test_size=0.1, shuffle=True, random_state=random_seed) \n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_set):\n",
        "    for idx in train_idxs:\n",
        "        train_set[i].append(remain_set[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_set[i].append(remain_set[idx])\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_set[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_set[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_set)))\n",
        "print(\"\")\n",
        "print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm3s5AMRh6U1",
        "outputId": "7d09374a-b6c1-43dc-cbe7-01e311342249"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1462\n",
            "val_dataset: 366\n",
            "test_dataset: 204\n",
            "\n",
            "extracted_id (example): 512_R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GURyJLkrtsx"
      },
      "source": [
        "#**Create Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "     \n",
        "    def __init__(self, img_list, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for image_path in img_list:\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([image_path, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "     \n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.item_paths[index][0]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image).float()\n",
        "        hertel = self.item_paths[index][1]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Create_Datasets(train_set[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms) \n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))\n",
        "print('test_dataset_size: ' +str(len(test_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXqKg2jfswY3",
        "outputId": "ebbcc664-3eed-46c6-eb08-330073571774"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1462\n",
            "val_dataset_size: 366\n",
            "test_dataset_size: 204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dmPkFnjiUXJ"
      },
      "source": [
        "print(train_dataset[1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMch8ogOX1X6"
      },
      "source": [
        "#**Test with early-stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IIK64KHX1nA"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size, optimizer, patience, n_epochs, device):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "       \n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:  \n",
        "            #target = target.squeeze(1)         \n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "        \n",
        "        print(print_msg)\n",
        "        \n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXAxIikRdQEu"
      },
      "source": [
        "#**define RepVGG-A2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdZk-1LhdQTK"
      },
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.fc = nn.Linear(in_features=1408, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYO37TYHeFwG"
      },
      "source": [
        "#**ConvNetの調整**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6p9djzEeF7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f7f533-9ba5-4886-bf82-909de302f6b1"
      },
      "source": [
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-lPDAqyEEx4"
      },
      "source": [
        "model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4o3lkBGIOxj"
      },
      "source": [
        "#**Draw Learning Curves**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEkl9xMiIno_"
      },
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBy1BeytJGel"
      },
      "source": [
        "#**Evaluation using testset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "val_dataset = Create_Datasets(val_set[0], CSV_PATH, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in val_loader:  \n",
        "      target = target.view(len(target), 1)         \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())      \n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n"
      ],
      "metadata": {
        "id": "9uZfA263UUhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aMQX9LzJJBC"
      },
      "source": [
        "#evaluation using test dataset\n",
        "\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "\n",
        "model.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:  \n",
        "      target = target.view(len(target), 1)         \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model(image_tensor)\n",
        "\n",
        "\n",
        "      outputs.append(output[0].item())      \n",
        "      targets.append(target[0].item())\n",
        "      #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveError: '+str(statistics.mean(corrected_error))) #平均誤差\n",
        "print('Corrected_StdError: '+str(statistics.stdev(corrected_error))) #誤差標準偏差\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError))) #平均絶対誤差\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError))) #絶対誤差標準偏差\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError))) #平均絶対誤差(四捨五入)\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError))) #絶対誤差標準偏差(四捨五入)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lamJcFxkjkxA"
      },
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSxjXrglZc4k"
      },
      "source": [
        "#Bland-Altman-Plot \n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R6aSRMkWEZO"
      },
      "source": [
        "#線形近似式算出\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# 説明変数xに \"x1\"のデータを使用\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# 目的変数yに \"x2\"のデータを使用\n",
        "y = target.values\n",
        "\n",
        "# 予測モデルを作成（単回帰）\n",
        "clf.fit(x, y)\n",
        "\n",
        "# パラメータ（回帰係数、切片）を抽出\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# パラメータの表示\n",
        "print(\"回帰係数:\", a)\n",
        "print(\"切片:\", b)\n",
        "print(\"決定係数:\", clf.score(x, y))\n",
        "\n",
        "#平均値により補正した値\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#回帰直線により補正した値\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#残差\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#残差\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlWXLynoKxy",
        "outputId": "ef9eadf8-bb26-47ff-fed9-f4927b9d0fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      estimate  target  Corrected_estimate_1  Corrected_estimate_2  \\\n",
            "0    17.221966    22.0             19.631564             20.248560   \n",
            "1    14.117248    14.0             16.526845             16.232585   \n",
            "2    13.923048    15.0             16.332646             15.981386   \n",
            "3    13.549201    15.0             15.958799             15.497812   \n",
            "4    15.702682    19.0             18.112280             18.283356   \n",
            "..         ...     ...                   ...                   ...   \n",
            "199  12.128220    15.0             14.537817             13.659763   \n",
            "200  15.005243    18.0             17.414841             17.381214   \n",
            "201  12.970710    15.0             15.380308             14.749530   \n",
            "202  16.962021    20.0             19.371619             19.912320   \n",
            "203  17.364050    20.0             19.773648             20.432347   \n",
            "\n",
            "     Residual_error_1  Residual_error_2  \n",
            "0           -2.368436         -1.751440  \n",
            "1            2.526845          2.232585  \n",
            "2            1.332646          0.981386  \n",
            "3            0.958799          0.497812  \n",
            "4           -0.887720         -0.716644  \n",
            "..                ...               ...  \n",
            "199         -0.462183         -1.340237  \n",
            "200         -0.585159         -0.618786  \n",
            "201          0.380308         -0.250470  \n",
            "202         -0.628381         -0.087680  \n",
            "203         -0.226352          0.432347  \n",
            "\n",
            "[204 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSiUi44-jGIZ"
      },
      "source": [
        "#平均近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "\n",
        "print('')\n",
        "print('-1<Error<1: '+ str(sum((i < 1 and i > -1 for i in df['Residual_error_2']))))\n",
        "print('-2<Error<2: '+ str(sum((i < 2 and i > -2 for i in df['Residual_error_2']))))\n",
        "print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1     \n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1     \n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x96i5oZDM0dm"
      },
      "source": [
        "#Bland-Altman-Plot using corrected value (平均値により補正)\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "\n",
        "corrected_estimate = df.loc[:,'Corrected_estimate_1']\n",
        "target = df.loc[:,'target']\n",
        "\n",
        "bland_altman_plot(corrected_estimate, target)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBFhobtCbv6t"
      },
      "source": [
        "#線形近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_2'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_2', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_2']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_2'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_2'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "print('')\n",
        "print('-1<Error<1: '+ str(sum((i < 1 and i > -1 for i in df['Residual_error_2']))))\n",
        "print('-2<Error<2: '+ str(sum((i < 2 and i > -2 for i in df['Residual_error_2']))))\n",
        "print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,3]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,3]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,3]< 18:\n",
        "        FN += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,3]< 18:\n",
        "        TN += 1     \n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,3]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,3]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,3]<= 16:\n",
        "        minogashi += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,3]<= 16:\n",
        "        oknegative += 1     \n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPJMCKTFqFnQ"
      },
      "source": [
        "#Bland-Altman-Plot using corrected value (線形近似により補正)\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "\n",
        "corrected_estimate = df.loc[:,'Corrected_estimate_2']\n",
        "target = df.loc[:,'target']\n",
        "\n",
        "bland_altman_plot(corrected_estimate, target)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Automated 5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "h1pCIJY5NeiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['target', 'fold0', 'fold1', 'fold2', 'fold3', 'fold4', 'fold0_corrected', 'fold1_corrected', 'fold2_corrected', 'fold3_corrected', 'fold4_corrected',]\n",
        "df_result = pd.DataFrame(index=[], columns=cols)\n",
        "\n",
        "cols = ['AveError', 'AveStdError', 'AveAbsError', 'StdAbsError', 'Corrected_AveAbsError', 'Corrected_StdAbsError', 'Round_Corrected_AveAbsError', 'Round_Corrected_StdAbsError']\n",
        "indices = ['fold0', 'fold1', 'fold2', 'fold3', 'fold4']\n",
        "df_summary = pd.DataFrame(index=indices, columns=cols)"
      ],
      "metadata": {
        "id": "akk3glokkBIM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold_start = 0 #スタートするfold\n",
        "dst_result_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_estimation_result.csv\"\n",
        "dst_summary_path = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/Hertel_estimation_summary.csv\"\n",
        "\n",
        "#四捨五入のモジュール\n",
        "def my_round(x, d=0):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "for fold in range(fold_start,5,1):    \n",
        "    ###############\n",
        "    ##Define model ##\n",
        "    ###############\n",
        "\n",
        "    model_ft = create_RepVGG_A2(deploy=False)\n",
        "    model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "    model_ft = mod_RepVGG()\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "    #損失関数を定義\n",
        "    loss_func = nn.MSELoss()\n",
        "    #Optimizer\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "\n",
        "    #################\n",
        "    ## select dataset ##\n",
        "    ################\n",
        "    train_dataset = Create_Datasets(train_set[fold], CSV_PATH, train_data_transforms)\n",
        "    val_dataset = Create_Datasets(val_set[fold], CSV_PATH, val_data_transforms)\n",
        "    test_dataset = Create_Datasets(test_set, CSV_PATH, val_data_transforms) \n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "    ###############\n",
        "    ## train model ###\n",
        "    ###############\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n",
        "\n",
        "    ####################\n",
        "    ## Draw learning curve ##\n",
        "    ####################\n",
        "\n",
        "    # visualize the loss as the network trained\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "    plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "    # find position of lowest validation loss\n",
        "    minposs = valid_loss.index(min(valid_loss))+1 \n",
        "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.ylim(0, 10.0) # consistent scale\n",
        "    plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig('loss_plot(Hertel)_fold'+str(fold)+'.png', bbox_inches='tight', dpi=350)\n",
        "\n",
        "\n",
        "    ###########################\n",
        "    ## eval using validation dataset ##\n",
        "    ###########################\n",
        "\n",
        "    #evaluation using validation dataset (1枚ずつevalする)\n",
        "    val_dataset = Create_Datasets(val_set[fold], CSV_PATH, val_data_transforms)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = 1)\n",
        "\n",
        "    model.eval() # prep model for evaluation\n",
        "\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in val_loader:  \n",
        "          target = target.view(len(target), 1)         \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(image_tensor)\n",
        "\n",
        "          outputs.append(output[0].item())      \n",
        "          targets.append(target[0].item())\n",
        "          errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "    AbsError = [abs(i) for i in errors]\n",
        "\n",
        "    print('AveError: '+str(statistics.mean(errors)))\n",
        "    print('StdError: '+str(statistics.stdev(errors)))\n",
        "    print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "    print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "    print('')\n",
        "\n",
        "\n",
        "    #平均からの差分を補正\n",
        "    corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "    corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "    corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "    round_output = [my_round(i) for i in outputs]\n",
        "    round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "    print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "    print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "    print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "    print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n",
        "\n",
        "    #結果をdataframeに書き込む\n",
        "    df_result['target'] = targets\n",
        "    df_result['fold'+str(fold)] = outputs\n",
        "    df_result['fold'+str(fold)+'_corrected'] = corrected_output\n",
        "    df_summary.iloc[fold, 'AveError'] = statistics.mean(errors)\n",
        "    df_summary.iloc[fold, 'StdError'] = statistics.stdev(errors)\n",
        "    df_summary.iloc[fold, 'AveAbsError'] = statistics.mean(AbsError)\n",
        "    df_summary.iloc[fold, 'StdAbsError'] = statistics.stdev(AbsError)\n",
        "    df_summary.iloc[fold, 'Corrected_AveAbsError'] = statistics.mean(corrected_AbsError)\n",
        "    df_summary.iloc[fold, 'Corrected_StdAbsError'] = statistics.stdev(corrected_AbsError)\n",
        "    df_summary.iloc[fold, 'Round_Corrected_AveAbsError'] = statistics.mean(round_corrected_AbsError)\n",
        "    df_summary.iloc[fold, 'Round_Corrected_StdAbsError'] = statistics.stdev(round_corrected_AbsError)\n"
      ],
      "metadata": {
        "id": "5O9JFMDTjHRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}