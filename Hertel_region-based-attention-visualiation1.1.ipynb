{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNKUhVE1OjnkzmSwfKJIjF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Hertel_region-based-attention-visualiation1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hertel estimation_判断根拠の可視化**"
      ],
      "metadata": {
        "id": "mcxWkOZpPIRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF9sfqWkO7r-",
        "outputId": "52f5357e-e087-4b34-e878-eed873e091e2",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 25 14:36:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    14W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Random Seed:  1234\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Import modules\n",
        "from __future__ import print_function, division\n",
        "!pip install --q torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install --q pingouin\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "!nvidia-smi\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example_path_list\n",
        "glob.glob(f\"/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_periocular/*\")[0:5]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO46-frN-wWY",
        "outputId": "6e37dd3a-5ead-4476-b542-cd5a1f9ead2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_periocular/472_R.JPG',\n",
              " '/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_periocular/465_R.JPG',\n",
              " '/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_periocular/469_R.JPG',\n",
              " '/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_periocular/468_L.JPG',\n",
              " '/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_periocular/466_L.JPG']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title set parameters\n",
        "area = 'periocular' #@param [\"half\", \"periocular\", \"eye\"] {allow-input: true}\n",
        "patient = 468#@param {type:\"number\"} {allow-input: true}\n",
        "side = 'R' #@param [\"R\", \"L\"] {allow-input: true}\n",
        "\n",
        "DATASET_PATH = f\"/content/drive/MyDrive/発表/2022眼科AI学会/dataset_250px_uni_{area}/{patient}_{side}.JPG\"\n",
        "model_path =f\"/content/drive/MyDrive/発表/2022眼科AI学会/models_Hertel_estimation/5-fold-crossvalidation/{area}_fold0_RepVGGA2.pth\"\n",
        "csv_path = f\"/content/drive/MyDrive/発表/2022眼科AI学会/Hertel_unilateral.csv\"\n",
        "\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "train_data_transforms = nn.Sequential(\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ConvertImageDtype(torch.float32),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])\n",
        "                ).to(device)\n",
        "val_data_transforms = nn.Sequential(\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ConvertImageDtype(torch.float32),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])\n",
        "                ).to(device)\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p"
      ],
      "metadata": {
        "id": "1HLRqTyNP8UR",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create datasets\n",
        "################\n",
        "# Create datasets #\n",
        "################\n",
        "\n",
        "class Create_Datasets(Dataset):\n",
        "     \n",
        "    def __init__(self, image_path_list_list, idxs, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.path_list_list = path_list_list\n",
        "        self.idxs = idxs\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for idx in idxs:\n",
        "            path_0, path_1, path_2 = path_list_list[0][idx], path_list_list[1][idx], path_list_list[2][idx]\n",
        "            base_name = os.path.splitext(os.path.basename(path_0))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([path_0, path_1, path_2, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "     \n",
        "    def __getitem__(self, index):\n",
        "        # [tensor[path0, path1, path2], hertel_value]\n",
        "        def tensor_img(image_path):\n",
        "            pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "            tensor_image = transforms.functional.to_tensor(pilr_image)\n",
        "            tensor_image = self.transform(tensor_image)\n",
        "            # tensor_image = self.transform(pilr_image).float()\n",
        "            # tensor_image = self.transform(read_image(path=image_path))\n",
        "            return tensor_image\n",
        "        tensor_image_0 = tensor_img(self.item_paths[index][0]) \n",
        "        tensor_image_1 = tensor_img(self.item_paths[index][1])      \n",
        "        tensor_image_2 = tensor_img(self.item_paths[index][2])      \n",
        "        tensor_image = torch.stack([tensor_image_0, tensor_image_1, tensor_image_2])\n",
        "        #tensor_image = tensor_image_0\n",
        "        hertel = self.item_paths[index][3]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "\n",
        "######################\n",
        "# Test with early stopping #\n",
        "######################\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#1つずつ解析するバージョン\n",
        "def train_model(model, loss_func, batch_size, optimizer, patience, n_epochs, device, area_num, alpha=0):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # define scaler for fastening\n",
        "        scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor[:,area_num])  #16,3,3,224,224 --> 16,3,224,224 (バッチサイズの次の次元でスライスすることによりtensorを取り出す)\n",
        "            # calculate the loss\n",
        "            with torch.cuda.amp.autocast(): \n",
        "                loss = loss_func(output, target)\n",
        "\n",
        "                ################\n",
        "                ##l2_normalization##\n",
        "                ################\n",
        "                l2 = torch.tensor(0., requires_grad=True)\n",
        "                for w in model.parameters():\n",
        "                    l2 = l2 + torch.norm(w)**2\n",
        "                loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            scaler.scale(loss).backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            scaler.step(optimizer) \n",
        "            scaler.update() \n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "       \n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:  \n",
        "            #target = target.squeeze(1)         \n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor[:,area_num])\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "        \n",
        "        print(print_msg)\n",
        "\n",
        "        \n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "        \n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "9mZcq_RjR2eJ",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define RepVGG-A2\n",
        "##################\n",
        "# Define RepVGG-A2 #\n",
        "##################\n",
        "\n",
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc = nn.Linear(in_features=1408, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x) #dropoutを1層追加\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wiW7sCy0Rj_n",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ここはskip\n",
        "# # Interference several images\n",
        "# %matplotlib inline\n",
        "\n",
        "# ###########################\n",
        "# # ここを入力する！\n",
        "# image_number = 0\n",
        "# ###########################\n",
        "\n",
        "\n",
        "# #Datasetの読み込み\n",
        "# image_path_list = glob.glob(f\"{DATASET_PATH}/*\")[image_number:image_number+1]\n",
        "# csv_path = csv_path\n",
        "\n",
        "# #ラベルの読み込み\n",
        "# with codecs.open(csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "#         df = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "# targets, outputs = [], []\n",
        "# for path in image_path_list:\n",
        "#     print(path)\n",
        "#     base_name = os.path.splitext(os.path.basename(path))[0] #フォルダより画像番号を抜き出す\n",
        "#     hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "#     targets.append(hertel) \n",
        "\n",
        "\n",
        "# #評価のための画像下処理\n",
        "# def image_transform(image_path):    \n",
        "#     image=Image.open(image_path)\n",
        "#     transform = transforms.Compose([\n",
        "#             transforms.Resize(256),\n",
        "#             transforms.CenterCrop(224),\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "#     image_tensor = transform(image)\n",
        "#     #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "#     image_tensor.unsqueeze_(0)\n",
        "#     #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "#     image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "#     return(image_tensor)\n",
        "\n",
        "# #Interference\n",
        "# model_ft = create_RepVGG_A2(deploy=False) \n",
        "# model_ft = mod_RepVGG()\n",
        "# model_ft = model_ft.to(device)\n",
        "# model_ft.load_state_dict(torch.load(model_path))\n",
        "# model_ft.eval() # prep model for evaluation\n",
        "# print(f\"model_path: {model_path}\")\n",
        " \n",
        "# for img in image_path_list:\n",
        "#       Image.register_open\n",
        "#       with torch.inference_mode():\n",
        "#           image_tensor = image_transform(img)  \n",
        "#           # forward pass: compute predicted outputs by passing inputs to the model\n",
        "#           outputs.append(model_ft(image_tensor).item()) \n",
        "\n",
        "# for path, target, output in zip(image_path_list, targets, outputs):\n",
        "#     imgPIL = Image.open(path)  # 画像読み込み\n",
        "#     plt.imshow(imgPIL)\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.show()\n",
        "#     print(f\"target: {target}, output: {output}\")"
      ],
      "metadata": {
        "id": "l9oqE3aRQN9x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "image = Image.open(DATASET_PATH)\n",
        "image = image.resize((224, 224))\n",
        "\n",
        "#ラベルの読み込み\n",
        "with codecs.open(csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "targets = []\n",
        "base_name = os.path.splitext(os.path.basename(DATASET_PATH))[0] #フォルダより画像番号を抜き出す\n",
        "hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "targets.append(hertel) \n",
        "\n",
        "def randmask(image, left_upper, right_lower, min_size, max_size):\n",
        "    coordinates_x = np.sort(np.random.randint(left_upper[0], right_lower[0]+1, size=2)).reshape(2,1)\n",
        "    coordinates_y = np.sort(np.random.randint(left_upper[1], right_lower[1]+1, size=2)).reshape(2,1)\n",
        "    coordinates = np.hstack((coordinates_x, coordinates_y))\n",
        "    #print(coordinates)\n",
        "\n",
        "    while np.all(coordinates[1,:]-coordinates[0,:] >= min_size) == False or np.all(coordinates[1,:]-coordinates[0,:] <= max_size) == False: #幅が範囲外の場合は設定し直す\n",
        "        coordinates_x = np.sort(np.random.randint(min_size, max_size+1, (2,1)))\n",
        "        coordinates_y = np.sort(np.random.randint(min_size, max_size+1, (2,1)))\n",
        "        coordinates = np.hstack((coordinates_x, coordinates_y))\n",
        "        print(coordinates)\n",
        "    x1 = coordinates[0,0]\n",
        "    y1 = coordinates[0,1]\n",
        "    x2 = coordinates[1,0]\n",
        "    y2 = coordinates[1,1]\n",
        "    print(x1, y1, x2, y2)\n",
        "\n",
        "    # image_1 = image.copy()\n",
        "    # rect_d = ImageDraw.Draw(image_1)\n",
        "    # rect_d.rectangle(\n",
        "    #     [(x1, y1), (x2, y2)], fill=(0, 0, 0)\n",
        "    # )\n",
        "\n",
        "    black = Image.new('RGB', (224, 224), 0)\n",
        "    mask = Image.new(\"L\", (224,224), 0)\n",
        "    draw = ImageDraw.Draw(mask)\n",
        "    draw.rectangle([(x1, y1), (x2, y2)], fill=255)\n",
        "    image_1 = Image.composite(black, image, mask)\n",
        "    image_2 = Image.composite(image, black, mask)\n",
        "    return image_1, image_2, [x1, y1, x2, y2] #PIL\n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image):    \n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "    return(image_tensor)\n",
        "\n",
        "\n",
        "#Load model\n",
        "model_ft = create_RepVGG_A2(deploy=False) \n",
        "model_ft = mod_RepVGG()\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "model_ft.eval() # prep model for evaluation\n",
        "print(f\"model_path: {model_path}\")\n",
        "\n",
        "#消去なし画像のアウトプットを確認\n",
        "orig_output = []\n",
        "with torch.inference_mode():\n",
        "    image_tensor = image_transform(image)  \n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    orig_output.append(model_ft(image_tensor).item()) \n",
        "\n",
        "imgPIL = Image.open(DATASET_PATH)  # 画像読み込み\n",
        "plt.imshow(imgPIL)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "print(f\"target: {targets}, output: {orig_output}\")"
      ],
      "metadata": {
        "id": "RvEqXWYNY_7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#パターン１\n",
        "\n",
        "attention = np.zeros((224, 224), dtype=int)\n",
        "time_sta = time.time()\n",
        "\n",
        "renew = 0\n",
        "for epoch in range(500):\n",
        "    masked_img = [0]*2\n",
        "    outputs = []\n",
        "    masked_img[0], masked_img[1], c = randmask(image, left_upper=0, right_lower=224, min_size=50, max_size=224) #c:画像内の座標\n",
        "\n",
        "    for img in masked_img:\n",
        "          with torch.inference_mode():\n",
        "              image_tensor = image_transform(img)  \n",
        "              # forward pass: compute predicted outputs by passing inputs to the model\n",
        "              outputs.append(model_ft(image_tensor).item()) \n",
        "\n",
        "    for i, imgPIL in enumerate(masked_img):\n",
        "        plt.imshow(imgPIL)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        print(f\"epoch {epoch}\")\n",
        "        print(f\"target: {targets[0]}, output: {outputs[i]}\")\n",
        "\n",
        "        time_end = time.time()\n",
        "        tim = time_end- time_sta\n",
        "        print(f\"time: {round(tim,1)} sec\")\n",
        "\n",
        "    print(\"\")\n",
        "    if abs(outputs[0]-orig_output[0]) > abs(outputs[1]-orig_output[0]): #切り抜き画像の精度 > 消去画像の精度の場合\n",
        "        #square_measure = 50176-(c[2]-c[0])*(c[3]-c[1]) #矩形領域を除いた面積をcoefとする（矩形が小さいほどcoefが大きくなる）\n",
        "        coef = 1\n",
        "        attention[c[1]:c[3]+1, c[0]:c[2]+1] += coef*2 #mask内の座標にcoefポイント加算\n",
        "        attention -= coef #全体から1ポイント減算\n",
        "        renew += 1\n",
        "        print(f\"attention renewed!! ({renew})\")\n",
        "        print(\"\")\n",
        "\n",
        "print(f\"Attention search done!! renew: {renew}\")\n",
        "attention_backup = attention"
      ],
      "metadata": {
        "id": "2N_8Bk9jojYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パターン2\n",
        "\n",
        "attention = np.zeros((224, 224), dtype=int)\n",
        "time_sta = time.time()\n",
        "\n",
        "def judge_invert_images(image, left_upper, right_lower, min_size, max_size):\n",
        "    masked_img = [0]*2\n",
        "    outputs = []\n",
        "    masked_img[0], masked_img[1], c = randmask(image, left_upper, right_lower, min_size, max_size) #c:画像内の座標\n",
        "\n",
        "    for img in masked_img:\n",
        "          with torch.inference_mode():\n",
        "              image_tensor = image_transform(img)  \n",
        "              # forward pass: compute predicted outputs by passing inputs to the model\n",
        "              outputs.append(model_ft(image_tensor).item()) \n",
        "\n",
        "    for i, imgPIL in enumerate(masked_img):\n",
        "        plt.imshow(imgPIL)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        print(f\"epoch {epoch}\")\n",
        "        print(f\"target: {targets[0]}, output: {outputs[i]}\")\n",
        "\n",
        "        time_end = time.time()\n",
        "        tim = time_end- time_sta\n",
        "        print(f\"time: {round(tim,1)} sec\")\n",
        "\n",
        "    print(\"\")\n",
        "    if abs(outputs[0]-orig_output[0]) > abs(outputs[1]-orig_output[0]): #切り抜き画像の精度 > 消去画像の精度の場合\n",
        "        judge = True\n",
        "    else:\n",
        "        judge = False\n",
        "    return judge, c\n",
        "\n",
        "\n",
        "renew = 0\n",
        "for epoch in range(300):\n",
        "    print(f\"epoch {epoch}\")\n",
        "    masked_img = [0]*2\n",
        "    outputs = []\n",
        "\n",
        "    judge, c = judge_invert_images(image, left_upper=[0,0], right_lower=[224,224], min_size=50, max_size=224)\n",
        "\n",
        "    if judge == True:\n",
        "        print(\"Area of attention found!!\")\n",
        "        k,l,m = 1,1,1\n",
        "        while True:\n",
        "            print(f\"scan_{k}_{l}\")\n",
        "            judge, c_temp = judge_invert_images(image, left_upper=[c[0],c[1]], right_lower=[c[2],c[3]], min_size=0, max_size=224)\n",
        "            if judge == True:\n",
        "                c = c_temp\n",
        "                k+= 1 #セット数を進める\n",
        "                l = 1 #スキャン数をリセット\n",
        "            else:\n",
        "                l += 1 #スキャン数を進める\n",
        "                if l==10: #10回スキャンして見つからなければ終了\n",
        "                    break\n",
        "\n",
        "        #square_measure = 50176-(c[2]-c[0])*(c[3]-c[1]) #矩形領域を除いた面積をcoefとする（矩形が小さいほどcoefが大きくなる）\n",
        "        coef = 1\n",
        "        attention[c[1]:c[3]+1, c[0]:c[2]+1] += coef*2 #mask内の座標にcoefポイント加算\n",
        "        attention -= coef #全体から1ポイント減算\n",
        "        renew += 1\n",
        "        print(f\"attention renewed!! ({renew})\")\n",
        "        print(\"\")\n",
        "\n",
        "    if renew == 50:\n",
        "        print(f\"Attention search done!! {epoch} epoch, {renew} renew\")\n",
        "        break\n",
        "\n",
        "attention_backup = attention"
      ],
      "metadata": {
        "id": "GEB84IoY-ZN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#最大と最小が0と255になるように補正\n",
        "attention = attention_backup.copy()\n",
        "min = np.amin(attention)\n",
        "attention = 255*(attention -np.amin(attention))/(np.amax(attention) - np.amin(attention))\n",
        "attention = attention.astype(int)\n",
        "attention\n",
        "\n",
        "#グラフを1行2列に並べたうちの1番目\n",
        "plt.subplots_adjust(wspace=0,hspace=0)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.imshow(np.array(attention), alpha=0.5, cmap='jet')\n",
        "\n",
        "#元の画像を並べて表示\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "\n",
        "plt.savefig(\"colormap.png\", dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wiujkU-oGRPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#マイナスを削除した上で、最大と最小が0と255になるように補正\n",
        "attention = attention_backup.copy()\n",
        "min = np.amin(attention)\n",
        "attention[attention < 0] = 0\n",
        "attention = 255*(attention -np.amin(attention))/(np.amax(attention) - np.amin(attention))\n",
        "attention = attention.astype(int)\n",
        "\n",
        "#グラフを1行2列に並べたうちの1番目\n",
        "plt.subplots_adjust(wspace=0,hspace=0)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.imshow(np.array(attention), alpha=0.5, cmap='jet')\n",
        "\n",
        "#元の画像を並べて表示\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "\n",
        "plt.savefig(\"colormap.png\", dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqljG_u2CJoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0,224)\n",
        "y = np.arange(0,224)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "data = attention\n",
        "\n",
        "plt.contourf(X, Y, data, cmap=\"jet\") #等高線をプロット\n",
        "plt.show() #画像の表示"
      ],
      "metadata": {
        "id": "BR9dXKNbD3v4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}