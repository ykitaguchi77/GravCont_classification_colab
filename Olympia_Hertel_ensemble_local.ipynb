{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled72.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Olympia_Hertel_ensemble_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJiUlScYrIgg"
      },
      "source": [
        "#**Olympia_Hertel_estimation_RepVGG-A2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import modules"
      ],
      "metadata": {
        "id": "EZvxdGnfLm8o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ1FDbAxrAsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d10e28-9867-4490-cfb1-f90c281df28f"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install --q torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import statistics\n",
        "import math\n",
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "import shutil\n",
        "import codecs\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install --q pingouin\n",
        "import pingouin as pg\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "!nvidia-smi\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "# #google driveをcolabolatoryにマウント\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 15 10:19:27 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 526.67       Driver Version: 526.67       CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Quadro RTX 5000    WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   45C    P0    33W /  N/A |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Random Seed:  1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XD7y5oqsMwg"
      },
      "source": [
        "## **Set Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Er_Gm6sRDv"
      },
      "source": [
        "path = r'C:\\Users\\ykita\\OneDrive\\デスクトップ\\Hertel_dataset'\n",
        "os.chdir(path)\n",
        "\n",
        "# grav or cont, age, and sex\n",
        "#NUM_CLASSES = 3\n",
        "\n",
        "# contains train, val\n",
        "#DATASET_PATH = r\"./dataset_500px\"\n",
        "AREA = [\"half\", \"periocular\", \"eye\"]\n",
        "DATASET_PATH_0 = f\"./dataset_250px_uni_{AREA[0]}\"\n",
        "DATASET_PATH_1 = f\"./dataset_250px_uni_{AREA[1]}\"\n",
        "DATASET_PATH_2 = f\"./dataset_250px_uni_{AREA[2]}\"\n",
        "PARENT_PATHS = [DATASET_PATH_0, DATASET_PATH_1, DATASET_PATH_2]\n",
        "#TRAIN_FOLDER_NAME = \"train\"\n",
        "#VAL_FOLDER_NAME = \"val\"\n",
        "#EFFICIENT_NET_NAME = \"RepVGG-A2-train\"\n",
        "MODEL_PATH = \"./RepVGG-A2-train.pth\"\n",
        "CSV_PATH = \"./Hertel_unilateral.csv\"\n",
        "#OPTIMIZER_PATH = \"./optimizer_multi.pth\"\n",
        "#SEX_DICT_PATH = \"gender_json\"\n",
        "#AGE_DICT_PATH = \"age_json\"\n",
        "LOG_PATH = \"./log_multi.txt\"\n",
        "ROC_PATH = \"./roc_multi.png\"\n",
        "#CHECKPOINT_COUNT = 10\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# transforms param　　左右分けているのでflipはしない\n",
        "PX = 224\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "ROTATION_DEGREES = 3\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "\n",
        "VAL_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "# train_data_transforms = transforms.Compose([\n",
        "#                 transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "#                 #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "#                 transforms.RandomHorizontalFlip(),\n",
        "#                 transforms.ToTensor(),\n",
        "#                 transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])])\n",
        "# val_data_transforms = transforms.Compose([\n",
        "#                 transforms.Resize(PX),\n",
        "#                 transforms.ToTensor(),\n",
        "#                 transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])]) \n",
        "\n",
        "# https://buildersbox.corp-sansan.com/entry/2020/11/05/110000\n",
        "train_data_transforms = nn.Sequential(\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                #transforms.RandomRotation(ROTATION_DEGREES),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ConvertImageDtype(torch.float32),\n",
        "                transforms.Normalize(TRAIN_NORMALIZE_PARAM[0], TRAIN_NORMALIZE_PARAM[1])\n",
        "                ).to(device)\n",
        "val_data_transforms = nn.Sequential(\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ConvertImageDtype(torch.float32),\n",
        "                transforms.Normalize(VAL_NORMALIZE_PARAM[0], VAL_NORMALIZE_PARAM[1])\n",
        "                ).to(device)\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p"
      ],
      "execution_count": 614,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5-Foldに分割**"
      ],
      "metadata": {
        "id": "aLBpMuFwhz1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    #id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    id_list = [os.path.basename(name).split(\".\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "def extract_patient_number(path_list):\n",
        "    patient_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(patient_list)\n",
        "\n",
        "\n",
        "path_list = make_path_list(PARENT_PATHS[1])\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "id = extract_ids(path_list)\n",
        "patient = extract_patient_number(id)\n",
        "\n",
        "print(\"id_num: {}\".format(len(id)))\n",
        "print(\"patient_num: {}\".format(len(patient)))\n",
        "\n",
        "print(len(path_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZXXWIO9k9Cq",
        "outputId": "76f378f1-6251-49ad-e78a-ae7c0003f2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_num: 1959\n",
            "patient_num: 1959\n",
            "1959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_list_0 = extract_ids(make_path_list(DATASET_PATH_0))\n",
        "id_list_1 = extract_ids(make_path_list(DATASET_PATH_1))\n",
        "id_list_2 = extract_ids(make_path_list(DATASET_PATH_2))\n",
        "common_id = set(id_list_0) & set(id_list_1) & set(id_list_2)\n",
        "common_patient = list([id.split(\"_\")[0] for id in common_id])\n",
        "\n",
        "path_list_0 = [f\"{DATASET_PATH_0}/{id}.JPG\" for id in common_id]    \n",
        "path_list_1 = [f\"{DATASET_PATH_1}/{id}.JPG\" for id in common_id]    \n",
        "path_list_2 = [f\"{DATASET_PATH_2}/{id}.JPG\" for id in common_id]    \n",
        "path_list_list = [path_list_0, path_list_1, path_list_2]\n",
        "\n",
        "print(f\"common_ids: {len(common_id)}\")\n",
        "print(f\"common_patients: {len(common_patient)}\")\n",
        "print(f\"{path_list_0[20]}\")\n",
        "print(f\"{path_list_1[20]}\")\n",
        "print(f\"{path_list_2[20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK9H3EZ2B3Dl",
        "outputId": "a1cd740f-f20d-40e5-f864-dfe1d06739f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "common_ids: 1959\n",
            "common_patients: 1959\n",
            "./dataset_250px_uni_half/802_L.JPG\n",
            "./dataset_250px_uni_periocular/802_L.JPG\n",
            "./dataset_250px_uni_eye/802_L.JPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Shuffle Split ＋　Group K-foldを用いてデータセット分け(idxを抜き出し)\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#fold数だけ空のリストを作成\n",
        "num_folds = 5\n",
        "train_set, val_set =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "train_idx, val_idx =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "test_idx = []\n",
        "test_set, remain_set = [], []\n",
        "\n",
        "#remain:test = 1:9で分割\n",
        "X = np.ones(len(common_id))\n",
        "y = np.ones(len(common_id))\n",
        "groups = common_patient\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=random_seed)\n",
        "for remain_idxs, test_idxs in gss.split(X, y, groups):\n",
        "    pass\n",
        "\n",
        "test_idx = [idx for idx in test_idxs]\n",
        "# test_set = [path_list[idxs] for idxs in test_idxs]\n",
        "\n",
        "remain_patients = [patient[idxs] for idxs in remain_idxs]\n",
        "# remain_set = [path_list[idxs] for idxs in remain_idxs]\n",
        "\n",
        "X = np.ones(len(remain_idxs))\n",
        "y = np.ones(len(remain_idxs))\n",
        "gkf = GroupKFold(n_splits=num_folds)\n",
        "i=0\n",
        "for train_idxs, val_idxs in gkf.split(X, y, groups=remain_patients):\n",
        "    for idx in train_idxs:\n",
        "        # train_set[i].append(remain_set[idx])\n",
        "        train_idx[i].append(idx)\n",
        "    for idx in val_idxs:\n",
        "        # val_set[i].append(remain_set[idx])\n",
        "        val_idx[i].append(idx)\n",
        "    i+=1\n",
        "\n",
        "print(\"train_dataset: {}\".format(len(train_idx[0])))\n",
        "print(\"val_dataset: {}\".format(len(val_idx[0])))\n",
        "print(\"test_dataset: {}\".format(len(test_idx)))\n",
        "# print(\"\")\n",
        "# print(\"extracted_id (example): {}\".format(extract_ids(test_set)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzjZhrg3lrdU",
        "outputId": "abac6c8a-1f94-46d1-f6fa-a8a1c5392369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset: 1410\n",
            "val_dataset: 353\n",
            "test_dataset: 196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GURyJLkrtsx"
      },
      "source": [
        "## **Create Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Datasets(Dataset):\n",
        "     \n",
        "    def __init__(self, image_path_list_list, idxs, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.path_list_list = path_list_list\n",
        "        self.idxs = idxs\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        df = self.df\n",
        "\n",
        "        k=0\n",
        "        for idx in idxs:\n",
        "            path_0, path_1, path_2 = path_list_list[0][idx], path_list_list[1][idx], path_list_list[2][idx]\n",
        "            base_name = os.path.splitext(os.path.basename(path_0))[0] #フォルダより画像番号を抜き出す\n",
        "            hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "            self.item_paths.append([path_0, path_1, path_2, hertel]) #[path, hertel]の組み合わせをリストに追加する\n",
        "            item_paths = self.item_paths\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "     \n",
        "    def __getitem__(self, index):\n",
        "        # [tensor[path0, path1, path2], hertel_value]\n",
        "        def tensor_img(image_path):\n",
        "            pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "            tensor_image = transforms.functional.to_tensor(pilr_image)\n",
        "            tensor_image = self.transform(tensor_image)\n",
        "            # tensor_image = self.transform(pilr_image).float()\n",
        "            # tensor_image = self.transform(read_image(path=image_path))\n",
        "            return tensor_image\n",
        "        tensor_image_0 = tensor_img(self.item_paths[index][0]) \n",
        "        tensor_image_1 = tensor_img(self.item_paths[index][1])      \n",
        "        tensor_image_2 = tensor_img(self.item_paths[index][2])      \n",
        "        tensor_image = torch.stack([tensor_image_0, tensor_image_1, tensor_image_2])\n",
        "        #tensor_image = tensor_image_0\n",
        "        hertel = self.item_paths[index][3]\n",
        "        target= torch.tensor([hertel]).float()\n",
        "        return  tensor_image, target\n",
        "\n",
        "train_dataset = Create_Datasets(path_list_list, train_idx[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(path_list_list, val_idx[0], CSV_PATH, val_data_transforms)\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, num_workers=0, pin_memory=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, num_workers=0, pin_memory=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))\n",
        "print('test_dataset_size: ' +str(len(test_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXqKg2jfswY3",
        "outputId": "27a83f29-d66a-4dc9-f6c5-241e2e35b2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1410\n",
            "val_dataset_size: 353\n",
            "test_dataset_size: 196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMch8ogOX1X6"
      },
      "source": [
        "## **Test with early-stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IIK64KHX1nA"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#1つずつ解析するバージョン\n",
        "def train_model(model, loss_func, batch_size, optimizer, patience, n_epochs, device, area_num, alpha=0):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # define scaler for fastening\n",
        "        scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor[:,area_num])  #16,3,3,224,224 --> 16,3,224,224 (バッチサイズの次の次元でスライスすることによりtensorを取り出す)\n",
        "            # calculate the loss\n",
        "            with torch.cuda.amp.autocast(): \n",
        "                loss = loss_func(output, target)\n",
        "\n",
        "                ################\n",
        "                ##l2_normalization##\n",
        "                ################\n",
        "                l2 = torch.tensor(0., requires_grad=True)\n",
        "                for w in model.parameters():\n",
        "                    l2 = l2 + torch.norm(w)**2\n",
        "                loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            scaler.scale(loss).backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            scaler.step(optimizer) \n",
        "            scaler.update() \n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "       \n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:  \n",
        "            #target = target.squeeze(1)         \n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor[:,area_num])\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "        \n",
        "        print(print_msg)\n",
        "\n",
        "        \n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "        \n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXAxIikRdQEu"
      },
      "source": [
        "## **define RepVGG-A2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdZk-1LhdQTK"
      },
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc = nn.Linear(in_features=1408, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x) #dropoutを1層追加\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYO37TYHeFwG"
      },
      "source": [
        "## **ConvNetの調整**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6p9djzEeF7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdad065-5e08-4e74-9e60-d6cbeb0260c8"
      },
      "source": [
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "# !pip install ranger_adabelief\n",
        "# from ranger_adabelief import RangerAdaBelief\n",
        "# optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft =  optim.AdaBound(\n",
        "#     model_ft.parameters(),\n",
        "#     lr= 1e-3,\n",
        "#     betas= (0.9, 0.999),\n",
        "#     final_lr = 0.1,\n",
        "#     gamma=1e-3,\n",
        "#     eps= 1e-8,\n",
        "#     weight_decay=5e-4,\n",
        "#     amsbound=False,\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**モデルのトレーニング**"
      ],
      "metadata": {
        "id": "bGofj_nxPfx1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-lPDAqyEEx4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ed2c10e-e4a2-4784-fbc5-1639854d4b94"
      },
      "source": [
        "\"\"\"\n",
        "area_num\n",
        "1: half \n",
        "2: periocular\n",
        "3: eye\n",
        "データセットからそれぞれの画像を読みこんでトレーニング\n",
        "\"\"\"\n",
        "train_dataset = Create_Datasets(path_list_list, train_idx[0], CSV_PATH, train_data_transforms)\n",
        "val_dataset = Create_Datasets(path_list_list, val_idx[0], CSV_PATH, val_data_transforms)\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, num_workers=0, pin_memory=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, num_workers=0, pin_memory=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "print('train_dataset_size: ' +str(len(train_dataset)))\n",
        "print('val_dataset_size: ' +str(len(val_dataset)))\n",
        "print('test_dataset_size: ' +str(len(test_dataset)))\n",
        "\n",
        "\n",
        "for area_num in [0]:\n",
        "    model_ft = create_RepVGG_A2(deploy=False)\n",
        "    model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "    model_ft = mod_RepVGG()\n",
        "    model_ft = model_ft.to(device)\n",
        "    loss_func = nn.MSELoss()\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device, area_num=area_num)\n",
        "\n",
        "    #save the model\n",
        "    PATH = f\"./models_Hertel_estimation/{AREA[area_num]}_test_RepVGGA2.pth\"\n",
        "    torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_size: 1410\n",
            "val_dataset_size: 353\n",
            "test_dataset_size: 196\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "Epoch: [  1/100] \n",
            "train_loss: 23.68999 \n",
            "valid_loss: 12.41374 \n",
            "Validation loss decreased (inf --> 12.413740).  Saving model ...\n",
            "\n",
            "Epoch: [  2/100] \n",
            "train_loss: 4.13531 \n",
            "valid_loss: 3.94686 \n",
            "Validation loss decreased (12.413740 --> 3.946857).  Saving model ...\n",
            "\n",
            "Epoch: [  3/100] \n",
            "train_loss: 3.54612 \n",
            "valid_loss: 6.82275 \n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "Epoch: [  4/100] \n",
            "train_loss: 3.25421 \n",
            "valid_loss: 9.64995 \n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "Epoch: [  5/100] \n",
            "train_loss: 3.01224 \n",
            "valid_loss: 4.63676 \n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "Epoch: [  6/100] \n",
            "train_loss: 2.78526 \n",
            "valid_loss: 6.79342 \n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "Epoch: [  7/100] \n",
            "train_loss: 2.55843 \n",
            "valid_loss: 3.40311 \n",
            "Validation loss decreased (3.946857 --> 3.403112).  Saving model ...\n",
            "\n",
            "Epoch: [  8/100] \n",
            "train_loss: 2.53047 \n",
            "valid_loss: 3.16447 \n",
            "Validation loss decreased (3.403112 --> 3.164472).  Saving model ...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [174]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     26\u001b[0m optimizer_ft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model_ft\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.0002\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m model, train_loss, valid_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marea_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marea_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#save the model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models_Hertel_estimation/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAREA[area_num]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_RepVGGA2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Input \u001b[1;32mIn [103]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_func, batch_size, optimizer, patience, n_epochs, device, area_num, alpha)\u001b[0m\n\u001b[0;32m    116\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# prep model for evaluation\u001b[39;00m\n\u001b[0;32m    118\u001b[0m running_corrects, val_acc\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_tensor, target \u001b[38;5;129;01min\u001b[39;00m val_loader:  \n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#target = target.squeeze(1)         \u001b[39;00m\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(target), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m     image_tensor \u001b[38;5;241m=\u001b[39m image_tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36mCreate_Datasets.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# tensor_image = self.transform(pilr_image).float()\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# tensor_image = self.transform(read_image(path=image_path))\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor_image\n\u001b[1;32m---> 32\u001b[0m tensor_image_0 \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     33\u001b[0m tensor_image_1 \u001b[38;5;241m=\u001b[39m tensor_img(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_paths[index][\u001b[38;5;241m1\u001b[39m])      \n\u001b[0;32m     34\u001b[0m tensor_image_2 \u001b[38;5;241m=\u001b[39m tensor_img(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_paths[index][\u001b[38;5;241m2\u001b[39m])      \n",
            "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36mCreate_Datasets.__getitem__.<locals>.tensor_img\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_img\u001b[39m(image_path):\n\u001b[1;32m---> 26\u001b[0m     pilr_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     tensor_image \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mto_tensor(pilr_image)\n\u001b[0;32m     28\u001b[0m     tensor_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(tensor_image)\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\ImageFile.py:235\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\JpegImagePlugin.py:402\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes):\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ended\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "PATH = f\"./models_Hertel_estimation/{AREA[area_num]}_test_RepVGGA2.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "hp5LBdAGNJGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#backup models\n",
        "for area_num in [0,1,2]:\n",
        "    orig_path = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2.pth\"\n",
        "    dst_path = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2_backup.pth\"\n",
        "    shutil.copy(orig_path, dst_path)"
      ],
      "metadata": {
        "id": "NXLBk6vDZqqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEkl9xMiIno_"
      },
      "source": [
        "#Draw learning curve\n",
        "\"\"\"\n",
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss', color=\"#377eb8\")\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss', color=\"#ff7f00\")\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 10.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "BAdFoY4h22gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define model\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "#Training\n",
        "\n",
        "for fold in [0,1,2,3,4]:\n",
        "    # Define dataset and dataloader\n",
        "    train_dataset = Create_Datasets(path_list_list, train_idx[fold], CSV_PATH, train_data_transforms)\n",
        "    val_dataset = Create_Datasets(path_list_list, val_idx[fold], CSV_PATH, val_data_transforms)\n",
        "    test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, num_workers=0, pin_memory=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, num_workers=0, pin_memory=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    #train models\n",
        "    AREA = [\"half\", \"periocular\", \"eye\"]\n",
        "    for area_num in [0,1,2]:\n",
        "        print(f\"area: {AREA[area_num], fold: {fold}}\")\n",
        "        model_ft = create_RepVGG_A2(deploy=False)\n",
        "        model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "        model_ft = mod_RepVGG()\n",
        "        model_ft = model_ft.to(device)\n",
        "        loss_func = nn.MSELoss()\n",
        "        optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "        model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device, area_num=area_num)\n",
        "\n",
        "        #save the model\n",
        "        PATH = f\"./models_Hertel_estimation/5-fold-crossvalidation/{AREA[area_num]}_fold{str(fold)}_RepVGGA2.pth\"\n",
        "        torch.save(model_ft.state_dict(), PATH)\n",
        "\n",
        "        PATH = f\"./models_Hertel_estimation/5-fold-crossvalidation/{AREA[area_num]}_fold{str(fold)}_backup_RepVGGA2.pth\"\n",
        "        torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "EDC5obyi26YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference on single model\n",
        "\n",
        "model_path = f\"./models_Hertel_estimation/half_test_RepVGGA2.pth\"\n",
        "\n",
        "#model_path = f\"./models_Hertel_estimation/5-fold-crossvalidation/half_fold0_RepVGGA2.pth\"\n",
        "#model_path = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2.pth\"\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False) \n",
        "model_ft = mod_RepVGG()\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "model_ft.eval() # prep model for evaluation\n",
        "print(f\"model_path: {model_path}\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = []\n",
        "    for image_tensor, target in test_loader:  \n",
        "          target = target.view(len(target), 1)         \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft(image_tensor[:,area_num]) #dim0はbach_size、dim1がarea_num\n",
        "          outputs.append(output[0].item())      \n",
        "df[f'{area}_fold{str(fold)}'] = outputs\n",
        "print(f\"targets: {targets}\")\n",
        "print(f\"outputs: {[my_round(i) for i in outputs]}\")"
      ],
      "metadata": {
        "id": "S_AUuV--W4NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load backup\n",
        "\n",
        "AREA = [\"half\", \"periocular\", \"eye\"]\n",
        "\n",
        "for area_num in [0,1,2]:\n",
        "  for fold in [0,1,2,3,4]:\n",
        "      backup_PATH = f\"./models_Hertel_estimation/5-fold-crossvalidation/{AREA[area_num]}_fold{str(fold)}_backup_RepVGGA2.pth\"\n",
        "      main_PATH = f\"./models_Hertel_estimation/5-fold-crossvalidation/{AREA[area_num]}_fold{str(fold)}_RepVGGA2.pth\"\n",
        "      shutil.copy(backup_PATH, main_PATH)"
      ],
      "metadata": {
        "id": "VEbHbassgvrf"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = Decimal(str(x)).quantize(Decimal(str(1/10**d)), rounding=ROUND_HALF_UP)\n",
        "    p = float(p)\n",
        "    return p\n",
        "\n",
        "# カラムがないindexだけ設定されている\n",
        "# DataFrameを作成\n",
        "df = pd.DataFrame(index=[], columns=[])\n",
        "\n",
        "#Define dataset\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "#Interference\n",
        "with torch.inference_mode():\n",
        "    targets = []\n",
        "    for image_tensor, target in test_loader:  \n",
        "            target = target.view(len(target), 1)         \n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device) \n",
        "            targets.append(target[0].item())\n",
        "    df['targets'] = targets\n",
        "\n",
        "FOLD = [0,1,2,3,4]\n",
        "AREA = [\"half\", \"periocular\", \"eye\"]\n",
        "for area_num in [0,1,2]:\n",
        "    for fold in FOLD:\n",
        "        model_path = f\"./models_Hertel_estimation/5-fold-crossvalidation/{AREA[area_num]}_fold{str(fold)}_RepVGGA2.pth\"\n",
        "        model_ft = create_RepVGG_A2(deploy=False) \n",
        "        model_ft = mod_RepVGG()\n",
        "        model_ft = model_ft.to(device)\n",
        "        model_ft.load_state_dict(torch.load(model_path))\n",
        "        model_ft.eval() # prep model for evaluation\n",
        "        print(f\"model_path: {model_path}\")\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            outputs = []\n",
        "            for image_tensor, target in test_loader:  \n",
        "                  target = target.view(len(target), 1)         \n",
        "                  image_tensor = image_tensor.to(device)\n",
        "                  target = target.to(device)\n",
        "                  # forward pass: compute predicted outputs by passing inputs to the model\n",
        "                  output = model_ft(image_tensor[:,area_num]) #dim0はbach_size、dim1がarea_num\n",
        "                  outputs.append(output[0].item())      \n",
        "        df[f'{AREA[area_num]}_fold{str(fold)}'] = outputs\n",
        "        print(f\"targets: {targets}\")\n",
        "        print(f\"outputs: {my_round(i) for i in outputs}\")\n",
        "\n",
        "df.to_csv( f\"./models_Hertel_estimation/5-fold-crossvalidation/result.csv\", header=True, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "6e_mo_HU99r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8WbUvsaK3hKa",
        "outputId": "85875467-23d8-4e5d-9104-9a11b2ea7f2b"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     targets  half_fold0  half_fold1  half_fold2  half_fold3  half_fold4  \\\n",
              "0       14.0   13.378623   14.685628   13.294028   13.973372   14.123014   \n",
              "1       17.0   16.527866   17.420464   18.572323   18.026186   17.614462   \n",
              "2       16.0   14.413683   15.394675   13.035702   15.077651   14.736663   \n",
              "3       18.0   17.310123   20.631779   17.513378   18.866766   18.291342   \n",
              "4       19.0   18.193491   18.763926   18.263947   18.002565   18.582289   \n",
              "5       21.0   18.632507   20.075674   20.377279   19.139229   20.997280   \n",
              "6       14.0   15.866582   14.374516   13.856710   14.705297   17.762205   \n",
              "7       20.5   19.918442   20.954706   21.114191   19.959185   21.328409   \n",
              "8       13.0   14.061344   13.705263   13.044377   14.158328   13.660732   \n",
              "9       17.0   15.136623   17.785122   16.808434   16.992243   15.816254   \n",
              "10      15.0   14.522603   15.791290   15.759887   16.061544   15.657650   \n",
              "11      13.0   14.548234   14.040708   11.965037   13.018007   14.617029   \n",
              "12      17.0   14.629607   15.688865   16.740273   15.920527   15.881575   \n",
              "13      11.0   11.677833   10.838790    9.319304   12.407351   11.048605   \n",
              "14      20.0   17.516705   20.553877   18.362318   20.199848   19.681187   \n",
              "15      16.0   17.708431   16.385679   14.859704   18.968016   17.258400   \n",
              "16      15.0   15.047476   14.518665   13.082020   14.970243   14.868305   \n",
              "17      17.0   15.469520   17.082392   15.009986   16.953379   14.858942   \n",
              "18      14.0   15.383628   15.075351   14.994046   14.018382   14.425631   \n",
              "19      24.0   21.722124   21.975380   23.327999   23.090868   24.661572   \n",
              "20      18.0   16.031721   16.109421   15.816394   15.655579   17.090631   \n",
              "21      18.0   19.102634   17.056438   17.621603   17.788414   18.857197   \n",
              "22      19.0   17.255619   18.926838   18.987755   20.596418   18.632954   \n",
              "23      13.0   12.973821   15.818367   10.536770   13.935122   11.171301   \n",
              "24      20.0   17.271559   19.924372   18.244999   18.052849   17.481569   \n",
              "25      13.0   14.486809   13.468585   11.514981   12.488697   13.688466   \n",
              "26      19.0   17.736250   19.385244   17.831570   18.329245   18.747551   \n",
              "27      18.0   18.750006   17.519140   18.658243   18.325762   18.970346   \n",
              "28      11.0   12.000104   11.165142   10.432534   12.097527   10.197731   \n",
              "29      18.0   16.010939   18.640636   16.737968   18.007343   17.600397   \n",
              "30      12.0   13.460879   12.299352   12.730235   12.924224   12.861664   \n",
              "31      17.0   17.096197   17.247944   17.533447   18.622046   18.955292   \n",
              "32      16.0   15.659000   15.800076   14.273402   14.578097   14.725208   \n",
              "33      16.0   15.311764   17.195402   15.020043   16.367628   16.041491   \n",
              "34      17.0   18.674849   17.429712   18.653074   19.794067   18.402611   \n",
              "35      14.0   15.063380   16.471708   13.893177   16.282831   14.372046   \n",
              "36      10.0   15.929156   11.749814   14.190647   14.377904   11.620865   \n",
              "37      22.0   19.954533   23.259424   21.624434   20.013008   22.180674   \n",
              "38      19.0   17.015457   18.171274   17.846905   19.292269   16.229349   \n",
              "39       9.0    9.107154    9.802299    7.524721   10.926050    8.608464   \n",
              "40      17.0   14.499024   17.556055   15.800036   15.371421   16.679575   \n",
              "41      18.0   17.650383   19.351572   17.880341   17.638901   20.056206   \n",
              "42      24.0   22.163698   24.172239   22.358864   20.693138   19.157946   \n",
              "43      15.0   15.295153   13.523074   13.151304   15.168266   15.430214   \n",
              "44      13.0   14.998239   14.551076   12.997133   13.976496   15.119221   \n",
              "45      20.0   19.632641   20.715481   20.028692   20.496227   20.264244   \n",
              "46      19.0   17.370024   18.671225   16.876211   17.012009   17.729921   \n",
              "47      21.0   19.545668   22.429083   22.122698   20.139862   22.100269   \n",
              "48      15.0   15.270207   16.172794   16.527843   16.083225   17.036819   \n",
              "49      12.0   13.529584   13.393477   10.963497   12.996196   12.773609   \n",
              "50      18.0   18.156004   19.980135   19.242186   18.312149   19.839521   \n",
              "51      17.0   16.600533   17.262775   15.227246   14.840694   16.934450   \n",
              "52      19.0   17.717543   19.299875   18.949003   17.242058   18.199465   \n",
              "53      20.0   17.804230   19.396788   16.465361   18.109066   19.420012   \n",
              "54      17.0   16.788042   17.143147   16.955376   16.729162   16.026024   \n",
              "55      21.0   19.318007   20.997274   19.663721   20.871473   21.455597   \n",
              "56      18.0   17.947807   18.202932   16.327032   16.791550   18.988745   \n",
              "57      16.0   14.775107   16.087587   14.202390   14.481210   14.054086   \n",
              "58      20.0   19.745102   20.208601   19.859838   18.580120   19.657810   \n",
              "59      18.0   15.801980   15.065242   15.927911   16.535433   17.149719   \n",
              "60      19.0   19.627504   20.154867   19.572161   20.170290   18.956127   \n",
              "61      17.0   15.810636   17.553711   15.826330   18.098150   16.370750   \n",
              "62      12.0   12.446628   14.410253   11.627754   13.825035   13.439529   \n",
              "63      19.0   19.801182   18.163044   17.045841   17.733816   18.545794   \n",
              "64      15.0   15.081377   15.295362   15.540390   15.066266   13.271460   \n",
              "65      19.0   18.222950   18.541761   18.913450   18.605051   18.408512   \n",
              "66      19.0   16.552792   17.515215   17.166183   19.798037   19.634186   \n",
              "67      20.0   19.876671   20.514748   19.687992   19.829268   19.764217   \n",
              "68      15.0   16.880955   16.031755   15.664324   16.272444   15.212618   \n",
              "69       9.0    8.981208   10.203212    8.664715   10.068004    9.997063   \n",
              "70      16.0   15.718888   16.137630   17.399063   16.677647   15.042620   \n",
              "71      16.0   17.429861   17.173769   14.670510   14.019113   15.954159   \n",
              "72      13.0   14.434525   14.826165   16.572517   14.204306   12.455984   \n",
              "73      20.0   17.742685   20.352535   20.306265   18.540133   20.291185   \n",
              "74      15.0   15.639606   15.701774   14.921044   14.789254   14.881083   \n",
              "75      20.0   18.624105   20.478455   18.657927   16.123985   18.745687   \n",
              "76      20.0   17.051630   19.490273   18.934885   18.423679   19.744778   \n",
              "77      16.0   15.350200   17.084431   14.866444   13.656182   15.348877   \n",
              "78      17.0   16.616489   16.891802   15.924697   16.594709   17.226166   \n",
              "79      14.0   16.531332   14.787730   15.255507   15.952616   16.019878   \n",
              "80      15.0   14.996792   16.581591   15.352725   15.377995   16.114653   \n",
              "81      15.0   15.905897   15.523425   15.102746   14.564447   15.405523   \n",
              "82      12.0   13.083528   12.930374   10.683826   13.097983   11.153977   \n",
              "83      20.0   18.558292   20.788132   20.482178   20.995062   20.450191   \n",
              "84      13.0   14.751754   13.908798   13.734552   14.342483   13.000732   \n",
              "85      14.0   15.031129   15.114771   13.772435   15.060906   14.349575   \n",
              "86      19.0   18.196518   18.886425   18.497211   17.204891   18.881599   \n",
              "87      13.0   14.259568   15.745381   13.767362   15.342570   15.208616   \n",
              "88      16.0   16.351345   16.333231   17.286863   17.012785   17.121038   \n",
              "89      16.0   14.850819   16.068043   15.173154   16.000347   15.291598   \n",
              "90      20.0   19.143242   20.630028   20.109547   17.628563   20.020170   \n",
              "91      19.0   18.617836   19.047892   18.044704   18.261036   17.299269   \n",
              "92      16.0   14.807504   17.258949   15.959424   15.707125   16.713694   \n",
              "93      20.0   16.382084   19.782688   15.386102   19.156275   17.966675   \n",
              "94      15.0   14.908904   15.846946   13.861213   14.999058   14.759367   \n",
              "95      14.0   13.823689   15.209287   13.986428   13.858667   13.026795   \n",
              "96      19.0   20.060217   19.210367   18.404139   17.225912   20.680990   \n",
              "97      13.0   13.181026   13.512937   12.350112   13.174659   13.695223   \n",
              "98      16.0   15.028329   17.992580   16.939325   16.271935   15.652617   \n",
              "99      15.0   15.086997   15.615657   14.360969   14.364262   17.624975   \n",
              "100     21.0   18.638048   21.176109   18.807835   17.325092   20.200136   \n",
              "101     20.0   18.864462   19.417206   18.168188   19.343380   18.921673   \n",
              "102     18.0   17.098911   18.633390   18.321386   18.948175   17.692709   \n",
              "103     17.0   16.447002   17.879698   16.542421   18.590281   18.157232   \n",
              "104     17.0   15.360436   16.078901   15.590281   15.930055   15.799952   \n",
              "105     21.0   21.297897   21.585949   20.642149   19.966974   20.772736   \n",
              "106     14.0   13.352776   14.594517   13.410445   15.585615   13.877559   \n",
              "107     15.0   14.025788   15.484532   13.311967   14.626474   12.615044   \n",
              "108     20.0   19.878788   20.901239   19.074677   19.885807   20.103422   \n",
              "109     17.0   15.984420   18.255655   14.924376   16.602781   16.294741   \n",
              "110     19.0   18.780916   20.032724   19.728315   19.836271   18.448744   \n",
              "111     17.0   15.898147   16.779900   16.388227   16.195053   15.727386   \n",
              "112     17.0   15.396921   16.101025   16.518602   15.145787   18.067350   \n",
              "113     21.0   20.415398   20.575958   17.751497   18.864243   21.180555   \n",
              "114     14.0   14.348152   15.434422   13.682901   13.657386   14.457260   \n",
              "115     15.0   17.765886   16.267172   15.677871   16.346952   15.458024   \n",
              "116     12.0   15.194238   13.312358   11.530807   13.821135   12.009678   \n",
              "117     18.0   16.752169   18.511625   17.439846   17.924599   16.363886   \n",
              "118     15.0   14.799852   15.311362   13.884873   13.584968   14.753664   \n",
              "119     17.0   16.475554   16.917189   17.140219   16.556530   17.689756   \n",
              "120     15.0   15.098559   15.376578   14.634631   14.272134   16.056553   \n",
              "121     18.0   18.291595   18.744356   18.194330   18.585718   18.458769   \n",
              "122     20.0   20.241253   20.047888   19.125957   18.768213   18.106382   \n",
              "123     14.0   13.412836   13.860312   14.856524   13.390033   13.885411   \n",
              "124     15.0   15.202406   14.459836   15.373020   14.631548   15.403196   \n",
              "125     15.0   15.882224   15.612880   14.610225   15.140519   15.736253   \n",
              "126     16.0   14.869064   15.536246   14.549211   13.131105   16.442209   \n",
              "127     15.0   14.124471   16.087503   13.799226   15.121427   14.210649   \n",
              "128     15.0   15.249518   15.971235   14.473754   15.020272   16.120718   \n",
              "129     12.0   12.753238   12.835464   10.760091   12.127373   13.712824   \n",
              "130     15.0   15.445794   15.192214   13.986918   13.264548   14.465387   \n",
              "131     18.0   19.059650   17.987600   18.044706   16.933945   17.625566   \n",
              "132     12.0   12.285661   12.982884    9.747102   12.254848   11.421160   \n",
              "133     18.0   17.694237   19.089067   17.192537   16.906252   18.042591   \n",
              "134     15.0   14.440679   14.767140   13.867667   15.243022   15.077799   \n",
              "135     20.0   18.877007   21.089241   21.078741   20.634472   21.063847   \n",
              "136     14.0   13.531987   15.184554   13.745651   13.876119   13.902111   \n",
              "137     18.0   18.148500   21.264231   19.884722   19.032616   18.862614   \n",
              "138     17.0   17.245813   17.335249   16.987471   16.680485   16.994375   \n",
              "139     23.0   22.981985   23.232721   22.106230   21.159370   23.601875   \n",
              "140     18.0   18.420778   17.791380   18.692806   18.077353   17.572319   \n",
              "141     17.0   15.622041   17.187464   15.563935   16.846245   14.765182   \n",
              "142     17.0   16.741795   17.009197   14.956962   17.064259   16.840876   \n",
              "143     17.0   15.908775   17.138792   16.153131   15.315886   16.197733   \n",
              "144     23.0   21.565781   21.541309   22.878180   20.558395   22.468512   \n",
              "145     20.0   18.193138   19.952080   19.559202   19.450968   19.928667   \n",
              "146     16.0   15.771754   17.990484   15.276317   17.273472   16.205570   \n",
              "147     23.0   21.753462   23.090391   22.704580   21.541676   24.161610   \n",
              "148     14.0   14.747776   15.262019   13.843303   17.112844   14.330640   \n",
              "149     17.0   18.115503   17.656206   17.728838   16.610216   15.988777   \n",
              "150     14.0   14.342243   13.649552   15.713218   14.653938   14.625059   \n",
              "151     15.0   14.598826   15.131998   15.590769   13.863337   14.178958   \n",
              "152     18.0   15.722518   18.015635   15.965040   17.059731   16.794769   \n",
              "153     17.0   16.184433   16.903803   14.983273   15.870845   16.622766   \n",
              "154     19.0   18.545658   19.382166   17.910528   17.776884   19.394764   \n",
              "155     18.0   18.502022   18.708866   16.648998   17.161612   18.250038   \n",
              "156     12.0   13.285331   13.485398   11.954938   12.597207   12.949023   \n",
              "157     15.0   14.198696   14.919567   11.242394   13.576180   14.835239   \n",
              "158     20.0   19.245588   18.932291   16.931675   18.611197   19.819712   \n",
              "159     16.0   14.737025   16.687754   15.102037   15.399813   15.920429   \n",
              "160     12.0   12.756974   11.964728   12.773540   12.936831   14.169472   \n",
              "161     16.0   15.713203   15.782406   16.828037   15.399594   15.364683   \n",
              "162     10.0   11.717183   12.896450    9.001308   11.456105   10.730436   \n",
              "163     17.0   16.722887   17.005983   16.347675   13.626843   16.679037   \n",
              "164     25.0   23.579699   21.654158   23.531755   22.609715   25.512274   \n",
              "165     18.0   19.941765   19.453693   16.728209   17.508717   18.619324   \n",
              "166     17.0   17.853767   17.711685   17.491310   16.810427   17.227379   \n",
              "167     18.0   17.772358   17.396465   17.650469   17.710220   18.568296   \n",
              "168     16.0   15.123564   16.714666   15.984319   15.498320   15.894736   \n",
              "169     19.0   18.146376   19.726698   18.476334   18.572832   19.165113   \n",
              "170     16.0   15.679877   15.947832   14.104043   14.999710   15.038914   \n",
              "171     21.0   20.682327   21.040085   21.996138   21.234104   22.955542   \n",
              "172     14.0   14.081783   16.316137   14.236874   13.721917   14.920563   \n",
              "173     16.0   15.283908   14.361036   15.049790   16.290926   15.813463   \n",
              "174     15.0   15.169580   13.831416   12.465230   13.404502   13.580338   \n",
              "175     17.0   16.601206   17.014296   16.862167   17.094461   17.411974   \n",
              "176     17.0   16.173536   17.417570   16.055334   16.362955   17.636421   \n",
              "177     21.0   20.799650   23.424494   21.556377   21.416651   22.642544   \n",
              "178     12.0   13.542101   13.806045   11.338939   14.050797   14.727674   \n",
              "179     19.0   17.730577   18.939526   18.355873   18.757765   19.612286   \n",
              "180     18.0   17.296686   19.056641   19.009691   18.744556   18.000357   \n",
              "181     19.0   18.988354   19.460320   19.138590   19.560942   19.348114   \n",
              "182     15.0   17.626106   18.111706   16.890810   17.524708   17.863947   \n",
              "183     20.0   17.405796   18.618668   17.579794   18.059498   16.602160   \n",
              "184     17.0   14.868840   15.925675   13.585452   15.440026   16.448004   \n",
              "185     19.0   20.629482   21.309982   22.547499   21.798948   22.236031   \n",
              "186     21.0   18.606709   21.710974   19.252287   19.713871   23.118177   \n",
              "187     15.0   14.855022   14.477091   13.823819   14.630529   13.599232   \n",
              "188     17.0   14.243763   15.850630   14.578341   14.833133   14.790435   \n",
              "189     15.0   14.652842   15.439577   14.483457   16.622023   14.280610   \n",
              "190     21.0   17.878487   18.302361   16.927153   16.978041   18.880093   \n",
              "191     19.0   16.332293   18.511236   16.109682   16.466679   17.397280   \n",
              "192     17.0   18.421608   19.502392   19.346493   18.655588   20.087976   \n",
              "193     16.0   15.132609   16.224701   15.771392   16.424994   15.888462   \n",
              "194     20.0   19.492592   22.159861   20.409103   19.957970   20.678410   \n",
              "195     16.0   15.496405   16.312719   13.045691   15.777010   15.306217   \n",
              "\n",
              "     periocular_fold0  periocular_fold1  periocular_fold2  periocular_fold3  \\\n",
              "0           14.628881         14.507998         14.967015         13.757207   \n",
              "1           16.501270         16.867992         17.573963         17.416254   \n",
              "2           15.614464         16.265606         14.597241         16.318535   \n",
              "3           17.554770         19.608227         17.574945         17.915594   \n",
              "4           18.368830         18.018055         20.731621         18.140236   \n",
              "5           20.834209         19.681776         20.667870         18.895624   \n",
              "6           14.463510         14.217272         15.651177         14.593225   \n",
              "7           22.539968         20.494875         20.870911         20.251326   \n",
              "8           14.519747         13.075567         12.439370         15.904149   \n",
              "9           16.999086         16.612087         16.711815         17.889015   \n",
              "10          14.248920         15.313585         15.004320         15.345590   \n",
              "11          12.243546         13.441017         11.864726         12.637973   \n",
              "12          16.183681         15.412989         16.605211         16.222584   \n",
              "13          13.044492         11.104789          9.554597         11.319505   \n",
              "14          20.988750         20.506330         20.287525         19.255453   \n",
              "15          16.944719         16.111366         15.687928         16.758253   \n",
              "16          14.516607         15.281561         14.178105         15.758600   \n",
              "17          16.724972         16.515635         15.868102         16.486046   \n",
              "18          13.622437         13.988466         13.342361         13.869771   \n",
              "19          24.632839         21.624823         26.178135         26.117241   \n",
              "20          17.300663         15.543173         18.012842         18.382524   \n",
              "21          18.164101         18.337790         16.614420         18.743999   \n",
              "22          19.612595         18.230295         19.568745         18.702543   \n",
              "23          13.138489         14.073352         12.837631         13.603251   \n",
              "24          19.386703         18.547815         18.664896         18.857344   \n",
              "25          13.913793         10.742638         11.333972         13.030467   \n",
              "26          19.045212         18.828514         19.000971         18.597195   \n",
              "27          18.714369         17.978840         18.249096         16.872839   \n",
              "28          11.144415         11.647454         10.341166         11.788428   \n",
              "29          16.610077         17.198915         16.898172         17.616724   \n",
              "30          12.026237         12.527395         13.294838         12.608129   \n",
              "31          17.720116         18.117304         17.109341         17.165464   \n",
              "32          14.966218         16.003679         15.573401         15.689538   \n",
              "33          16.786741         15.626900         14.794608         16.103279   \n",
              "34          18.106737         17.721672         15.507354         20.388166   \n",
              "35          14.242184         15.473936         14.582948         14.620574   \n",
              "36          16.791941         10.310551         13.376885         11.281787   \n",
              "37          20.115894         22.307640         21.779461         20.058226   \n",
              "38          18.455141         18.710060         19.098074         18.763845   \n",
              "39           8.628554          8.366113          9.780136          9.999585   \n",
              "40          16.511866         16.411034         17.106390         16.519764   \n",
              "41          17.829126         17.749088         17.225000         17.677038   \n",
              "42          24.360102         23.935865         25.646303         24.353662   \n",
              "43          14.536941         15.247784         13.697691         15.836574   \n",
              "44          13.723742         12.739313         13.166018         12.730762   \n",
              "45          20.532089         19.959583         19.151796         20.360538   \n",
              "46          18.511530         18.333344         19.476656         19.378344   \n",
              "47          18.350037         20.610500         22.435776         22.209272   \n",
              "48          15.305883         15.397141         15.044633         15.689833   \n",
              "49          13.597820         13.187181         12.225644         11.552445   \n",
              "50          19.312429         17.413691         16.555969         18.821276   \n",
              "51          14.634020         16.045580         16.711262         16.084055   \n",
              "52          18.000790         18.883318         18.719837         19.084002   \n",
              "53          19.533716         20.121250         16.654278         18.881573   \n",
              "54          17.203480         17.416317         16.558975         17.568600   \n",
              "55          21.098988         22.095234         21.978210         21.728188   \n",
              "56          17.828665         18.486780         17.163084         17.825018   \n",
              "57          14.952366         14.356369         13.215174         15.356174   \n",
              "58          20.849588         20.947172         19.593424         20.565172   \n",
              "59          18.568817         15.058388         19.007841         17.639315   \n",
              "60          19.014236         18.218279         20.199995         20.947311   \n",
              "61          17.288031         17.221249         16.823380         17.132397   \n",
              "62          11.711887         11.583888         11.604568         12.158920   \n",
              "63          20.058847         18.654970         18.284294         18.650532   \n",
              "64          14.965826         14.932250         15.921086         15.549075   \n",
              "65          19.236889         19.314754         20.088148         18.985424   \n",
              "66          17.013847         17.087328         17.908186         19.691286   \n",
              "67          20.698696         20.213478         19.137295         19.770252   \n",
              "68          14.150229         15.252031         14.023492         16.222433   \n",
              "69           9.371231          8.508332          9.183559          9.491282   \n",
              "70          16.476988         16.433208         16.740553         16.651012   \n",
              "71          15.556892         16.039589         18.462616         16.250582   \n",
              "72          12.669462         13.514441         15.623628         12.887915   \n",
              "73          21.281702         18.846563         19.671055         20.078466   \n",
              "74          14.293040         15.803938         15.697816         15.171117   \n",
              "75          20.532948         19.187893         17.729475         20.145765   \n",
              "76          18.788286         19.020283         20.144550         20.997332   \n",
              "77          15.714656         15.494652         13.317635         14.997259   \n",
              "78          18.735447         15.646786         14.803500         17.198198   \n",
              "79          16.901943         13.890097         14.856192         14.477530   \n",
              "80          14.762013         14.857765         14.443054         15.017770   \n",
              "81          15.423151         14.630257         14.869218         16.035784   \n",
              "82          12.932888         14.319544         12.859988         11.596682   \n",
              "83          19.307186         19.159975         21.121853         21.137178   \n",
              "84          16.054569         13.658901         13.343998         13.660720   \n",
              "85          15.037434         13.762219         13.383395         14.028338   \n",
              "86          19.088055         18.571501         19.181599         19.331451   \n",
              "87          14.349669         15.368238         15.072402         14.509226   \n",
              "88          15.839384         15.530210         15.563415         16.138378   \n",
              "89          15.810497         15.943932         15.801341         16.602005   \n",
              "90          20.493275         18.158710         21.165142         20.488102   \n",
              "91          19.084530         18.789234         18.050550         18.005165   \n",
              "92          15.676069         15.428617         15.740199         16.588556   \n",
              "93          19.761395         19.550489         17.338873         21.197283   \n",
              "94          15.704677         14.686828         15.323499         15.339183   \n",
              "95          14.078362         13.715359         12.626691         14.807911   \n",
              "96          19.513662         18.110865         20.363853         17.470575   \n",
              "97          13.149683         13.163710         13.181358         12.802911   \n",
              "98          16.493561         15.355536         15.432858         17.649641   \n",
              "99          15.236846         15.405605         14.379397         15.444663   \n",
              "100         20.443295         20.925205         20.987070         18.583937   \n",
              "101         20.606117         19.179138         19.219522         19.412638   \n",
              "102         18.486105         17.939966         17.923208         17.134146   \n",
              "103         17.086639         16.629871         17.023520         17.988283   \n",
              "104         16.262114         14.508701         15.887969         17.249607   \n",
              "105         20.571589         21.691191         23.027718         22.271929   \n",
              "106         15.040571         14.198187         13.693387         14.259379   \n",
              "107         14.965826         14.932250         15.921086         15.549075   \n",
              "108         19.657938         20.434942         21.051067         20.817110   \n",
              "109         17.894037         16.809717         18.089190         18.573507   \n",
              "110         18.661188         19.892614         18.039982         18.805685   \n",
              "111         17.617300         17.375399         16.898617         16.807962   \n",
              "112         14.733345         17.480612         15.910026         16.836212   \n",
              "113         20.319645         19.803225         16.190231         21.687798   \n",
              "114         14.919415         15.733963         13.988755         14.334916   \n",
              "115         17.299557         15.068815         16.353140         16.310656   \n",
              "116         14.329511         13.002660         12.132273         12.887012   \n",
              "117         19.031120         17.739796         18.060125         18.148172   \n",
              "118         15.764261         14.384544         13.940983         14.219676   \n",
              "119         16.671846         17.413422         17.718996         16.108139   \n",
              "120         16.051395         14.597668         14.442822         15.375036   \n",
              "121         18.781454         18.536146         18.058222         19.258188   \n",
              "122         19.471380         16.831236         18.785355         22.478306   \n",
              "123         13.772001         13.456437         15.024978         14.645157   \n",
              "124         12.901976         13.784710         13.944419         15.591275   \n",
              "125         16.173225         14.506193         14.763242         14.773370   \n",
              "126         15.145329         15.654291         15.889692         14.365667   \n",
              "127         14.264853         15.098917         14.927148         14.581848   \n",
              "128         15.480841         14.984310         14.353437         15.948468   \n",
              "129         11.667554         11.085661         10.026088         11.567861   \n",
              "130         13.554104         14.145494         12.829890         13.284381   \n",
              "131         17.995028         17.922344         18.686195         17.383821   \n",
              "132         12.836797         11.608201         12.469773         12.479422   \n",
              "133         17.358809         18.473795         19.054863         18.505905   \n",
              "134         14.327374         13.528830         14.093220         15.135263   \n",
              "135         20.852699         21.062891         19.971523         20.810244   \n",
              "136         14.301983         14.152697         15.046363         14.899579   \n",
              "137         19.788206         22.172640         21.414703         18.543257   \n",
              "138         17.735214         17.258974         16.933256         17.917749   \n",
              "139         23.798874         23.401407         22.487732         21.829874   \n",
              "140         18.695293         18.947659         21.891418         16.626087   \n",
              "141         17.672035         16.536583         15.204309         17.085436   \n",
              "142         17.920731         16.818827         17.851246         16.888235   \n",
              "143         16.281738         17.172140         14.160371         16.425190   \n",
              "144         22.660545         19.723120         24.719385         24.156837   \n",
              "145         20.616032         21.104029         20.246727         20.441389   \n",
              "146         18.607220         16.402576         16.470734         16.072285   \n",
              "147         22.267651         23.226456         24.333838         23.796125   \n",
              "148         13.261964         13.529705         15.098651         15.034329   \n",
              "149         17.013485         17.241848         18.800375         17.830242   \n",
              "150         13.371374         14.265774         14.608607         14.222448   \n",
              "151         12.852365         12.520601         16.158298         13.204428   \n",
              "152         18.884508         16.890114         16.730564         17.353325   \n",
              "153         17.275204         16.146082         15.229789         16.782652   \n",
              "154         19.486778         18.617241         19.114006         17.588797   \n",
              "155         18.566711         16.109606         17.245033         18.036596   \n",
              "156         11.700385         13.286694         10.854680         13.055973   \n",
              "157         13.961906         14.392944         13.370080         15.026933   \n",
              "158         19.462013         19.287527         18.394209         20.953188   \n",
              "159         15.805330         15.858692         14.890807         16.850431   \n",
              "160         12.555597         11.420494         12.787109         11.825041   \n",
              "161         16.791475         14.627212         15.500779         14.602944   \n",
              "162         10.442030         13.060412         10.474332         11.758207   \n",
              "163         16.878830         15.923225         15.096434         13.964755   \n",
              "164         24.814274         19.400486         25.490408         25.403139   \n",
              "165         19.364697         17.110109         18.714136         18.465761   \n",
              "166         16.796055         17.772554         18.832869         17.723480   \n",
              "167         17.802296         16.039776         17.801203         16.655733   \n",
              "168         16.684517         15.717626         14.470608         16.271568   \n",
              "169         18.368469         18.775406         17.700766         20.356167   \n",
              "170         14.553293         15.235319         15.476308         15.347179   \n",
              "171         21.242752         21.090660         21.708891         22.004087   \n",
              "172         14.955633         15.031363         15.108020         14.847644   \n",
              "173         14.513535         14.549343         15.037898         15.637522   \n",
              "174         13.842150         12.816131         12.959144         13.540159   \n",
              "175         17.766369         18.516867         16.800285         17.217720   \n",
              "176         16.143549         15.132627         16.348934         15.554867   \n",
              "177         22.017376         22.021044         23.042858         22.518988   \n",
              "178         11.843203         12.967569         12.134704         13.193330   \n",
              "179         17.932442         17.478149         17.080858         18.046850   \n",
              "180         18.300846         18.260881         18.304726         18.502983   \n",
              "181         19.540974         19.265949         20.617126         20.384748   \n",
              "182         17.392155         16.650349         16.613667         17.136173   \n",
              "183         18.458616         17.565907         17.233433         18.920244   \n",
              "184         15.997107         14.989971         17.009195         15.642363   \n",
              "185         20.572357         20.097321         22.055424         20.518080   \n",
              "186         20.311005         20.287062         20.461077         20.580431   \n",
              "187         13.656416         14.540446         14.656211         15.798012   \n",
              "188         15.031107         15.618595         14.221186         16.396439   \n",
              "189         16.630512         14.993143         17.060226         16.214722   \n",
              "190         18.344734         17.721203         18.155409         18.658598   \n",
              "191         18.765438         17.932436         17.759705         19.999836   \n",
              "192         17.838074         18.120354         18.588303         19.190153   \n",
              "193         16.716595         16.471413         16.551378         16.688139   \n",
              "194         21.200632         20.790155         18.605188         20.717855   \n",
              "195         15.312069         14.831355         16.326694         15.811578   \n",
              "\n",
              "     periocular_fold4  eye_fold0  eye_fold1  eye_fold2  eye_fold3  eye_fold4  \n",
              "0           13.487124  14.274810  14.551019  15.181063  12.912588  13.956070  \n",
              "1           15.861028  16.626665  15.791635  16.090891  18.066759  16.905663  \n",
              "2           14.994992  15.825631  17.309702  13.362928  16.382545  16.692049  \n",
              "3           18.106993  18.756432  20.563534  17.468340  19.188147  18.581377  \n",
              "4           19.150448  19.392269  19.751122  18.823761  18.247440  18.200075  \n",
              "5           19.149971  20.219944  22.163109  19.191067  18.353054  19.565882  \n",
              "6           17.372561  15.431116  15.361065  13.709132  13.666772  16.253014  \n",
              "7           20.327513  21.877436  20.002913  19.861214  20.815548  20.882113  \n",
              "8           13.471133  14.589243  13.971314  13.590286  15.549702  13.000081  \n",
              "9           16.337070  16.643187  17.092567  17.830677  18.145947  15.407084  \n",
              "10          13.985805  14.303629  16.516327  16.150364  15.115948  14.146938  \n",
              "11          13.604434  14.119826  11.922945  14.219202  12.905776  11.435546  \n",
              "12          16.156956  15.513434  14.452009  16.760256  15.432339  14.227433  \n",
              "13          11.720358  13.219360  10.565756  10.930639   9.071243  10.346580  \n",
              "14          19.822994  18.937723  18.897736  18.827013  17.983629  19.447701  \n",
              "15          17.070749  15.329899  15.394428  14.881329  15.775295  16.750942  \n",
              "16          12.927438  16.073662  15.139214  15.010571  15.496675  13.613491  \n",
              "17          16.016472  17.086077  16.276300  15.420680  17.056505  15.683866  \n",
              "18          13.816732  13.724952  13.537360  13.630291  13.367039  13.935292  \n",
              "19          22.968039  22.487616  21.479668  25.478451  24.955587  24.256048  \n",
              "20          16.377371  16.651539  16.770557  17.794308  16.979347  17.049082  \n",
              "21          18.175346  17.354548  17.525187  17.234940  17.425377  17.813021  \n",
              "22          17.536196  18.143642  18.333529  19.019339  19.092112  17.908457  \n",
              "23          13.373225  13.282551  15.528111  11.782574  13.547136  14.660616  \n",
              "24          16.567917  18.338924  19.612276  20.245289  20.037903  18.387369  \n",
              "25          12.147717  12.672179  10.751394  12.341632  12.393042  10.098878  \n",
              "26          17.756920  20.073200  19.055943  18.644924  19.172749  17.413557  \n",
              "27          18.102043  17.598238  17.289528  17.775970  18.095301  17.829952  \n",
              "28          11.335707  12.730444  11.762501  10.346120   9.327543  11.315918  \n",
              "29          19.009506  17.518831  17.157833  16.383596  18.974602  17.499184  \n",
              "30          12.472093  13.797016  13.349607  14.710016  11.561083  13.174564  \n",
              "31          17.720928  16.522379  16.882860  17.292444  17.232567  16.440983  \n",
              "32          15.614478  16.513536  16.279016  12.826980  15.106087  16.898932  \n",
              "33          16.376207  15.401998  14.607372  15.667814  14.196516  15.944733  \n",
              "34          17.932936  18.054737  18.160099  18.281828  19.237141  16.333475  \n",
              "35          13.957411  15.585864  16.118095  13.482611  13.998526  15.377835  \n",
              "36          11.905852  15.985344  12.500313   9.043743  10.957983  12.759283  \n",
              "37          20.591259  20.536217  21.320547  20.877834  21.055281  20.584452  \n",
              "38          16.633511  19.098383  18.197874  18.524269  17.162685  18.106840  \n",
              "39           9.937102  10.454482   9.878193   9.276745   8.533656  10.181276  \n",
              "40          16.193165  15.639833  16.264223  14.518070  15.666966  16.671841  \n",
              "41          17.929848  16.982676  17.167124  17.922649  16.915779  18.005463  \n",
              "42          18.593773  23.661455  24.748644  23.649305  25.363045  18.875423  \n",
              "43          15.261174  15.732479  15.682404  13.994151  13.474389  14.631439  \n",
              "44          12.998941  13.278191  12.952986  12.486665  12.314035  12.922033  \n",
              "45          20.506062  18.524500  18.686649  19.057964  19.539009  19.724142  \n",
              "46          17.823067  17.391676  18.299767  17.184555  18.324238  17.569887  \n",
              "47          22.162458  20.680803  20.623741  20.505507  20.886307  21.455515  \n",
              "48          15.715986  15.078802  15.174941  15.545979  16.138443  15.832549  \n",
              "49          12.682956  12.915602  11.882187  12.200031  11.783596  12.549768  \n",
              "50          19.416414  18.463596  18.192904  17.650389  17.874939  18.244631  \n",
              "51          16.018074  15.481498  15.909639  16.872824  15.876369  15.420093  \n",
              "52          18.092539  19.691950  18.799528  17.701302  17.638187  19.664036  \n",
              "53          19.347694  18.988909  19.176096  16.655128  19.042583  18.601881  \n",
              "54          16.056961  15.758860  15.528811  17.307396  15.720948  15.627150  \n",
              "55          20.789938  21.219679  21.197365  21.057579  19.929359  20.432877  \n",
              "56          18.419476  16.984722  17.102310  16.774094  16.653841  18.727053  \n",
              "57          15.137986  14.326322  14.993634  14.245468  14.420192  15.192637  \n",
              "58          20.073389  20.714273  20.447802  19.551035  18.888151  19.743498  \n",
              "59          16.937702  15.334141  15.009532  17.754236  17.370213  16.150246  \n",
              "60          20.355782  18.980156  18.810745  18.015308  20.764074  19.665220  \n",
              "61          16.048351  16.837603  17.209518  16.826164  16.741520  16.474344  \n",
              "62          12.975930  13.113430  13.503294  10.657827  11.066300  13.136365  \n",
              "63          17.163870  19.994102  19.747293  18.003857  17.976900  18.831484  \n",
              "64          13.340388  14.961090  13.729846  15.408571  15.403586  15.565741  \n",
              "65          18.472523  21.296585  19.221098  18.761751  18.817520  19.542006  \n",
              "66          18.592529  15.882755  17.471411  18.161449  19.120003  17.991695  \n",
              "67          19.113983  19.896721  18.435385  17.971863  18.276350  19.331652  \n",
              "68          16.235882  16.650421  15.114342  15.298063  15.893363  15.627771  \n",
              "69          10.014640  10.351007  10.202777   9.856271   8.680709   9.017848  \n",
              "70          15.296760  16.859655  17.002056  17.182262  15.775515  16.639914  \n",
              "71          14.743722  15.373385  18.208101  16.761637  13.922328  16.186548  \n",
              "72          13.757453  14.307617  14.090839  15.569736  14.256449  13.803965  \n",
              "73          20.559238  19.931515  18.848400  19.345093  20.227673  21.121088  \n",
              "74          14.600053  15.098475  14.859530  16.653254  12.956518  15.458138  \n",
              "75          17.543434  19.284327  20.020079  20.713921  20.723600  17.147121  \n",
              "76          18.765572  16.942690  18.239008  19.967186  19.310268  19.043873  \n",
              "77          15.005263  16.564766  15.038461  15.302752  14.521314  14.385362  \n",
              "78          16.404284  15.011387  14.078819  16.439119  17.136992  15.658569  \n",
              "79          15.395231  15.622538  14.618567  14.059141  13.168842  14.822374  \n",
              "80          15.307230  14.881834  17.105925  14.055600  15.160906  16.149475  \n",
              "81          14.358626  16.949654  14.629050  15.377235  15.001814  16.227140  \n",
              "82          12.017550  13.443398  13.681669  10.106813  12.300837  12.854052  \n",
              "83          20.061499  20.798206  22.153612  20.244562  22.240379  22.191500  \n",
              "84          13.940194  17.450058  15.473287  12.487250  13.989946  14.919375  \n",
              "85          13.796208  17.247681  15.165790  14.843243  14.411689  14.362759  \n",
              "86          18.821510  19.830971  18.885170  17.916826  18.039738  20.542759  \n",
              "87          14.101257  14.654187  15.459135  13.004227  13.757391  14.135849  \n",
              "88          16.471975  16.844343  15.939867  16.424520  13.699211  15.406459  \n",
              "89          16.350616  16.105104  16.122105  14.975595  14.693605  15.715784  \n",
              "90          17.815222  20.069004  19.374189  19.722439  18.740028  21.174377  \n",
              "91          18.674988  19.090401  18.107943  17.646259  20.054504  20.919004  \n",
              "92          17.837511  15.969807  14.557480  15.579144  14.746252  15.090883  \n",
              "93          19.616318  18.485733  20.257195  15.957960  20.988434  18.879469  \n",
              "94          14.058950  14.598878  15.580369  15.220892  13.057706  13.260307  \n",
              "95          13.411647  14.341468  13.640844  13.835194  12.437253  14.530807  \n",
              "96          19.038450  18.953119  18.858545  19.367758  17.209064  20.537483  \n",
              "97          12.039157  13.846973  14.084284  15.516514  13.435865  12.353072  \n",
              "98          14.300237  15.746277  15.513109  14.367241  16.758753  14.032661  \n",
              "99          15.661239  15.764544  15.198858  13.075729  15.930654  16.185522  \n",
              "100         19.554955  19.511980  20.770077  18.787273  17.955248  18.969299  \n",
              "101         19.117531  19.714758  18.680290  19.268744  18.417000  21.310812  \n",
              "102         17.902594  17.917068  18.820704  16.452084  17.354790  18.846609  \n",
              "103         17.052029  15.957426  16.581987  16.104824  16.775515  16.850704  \n",
              "104         18.341030  15.917773  15.165892  15.608491  15.747896  16.751755  \n",
              "105         21.044664  18.358082  21.944590  20.397308  20.184515  20.762308  \n",
              "106         14.785823  15.118582  13.964343  12.636228  12.440477  15.006018  \n",
              "107         13.340388  14.263691  15.183292  15.089794  16.093937  13.710068  \n",
              "108         20.327332  19.927277  20.737022  18.405361  21.028103  21.168932  \n",
              "109         17.224361  16.964176  17.289495  17.546143  17.342840  17.330698  \n",
              "110         19.094707  18.858198  19.781044  19.780323  18.666178  20.012707  \n",
              "111         15.374018  17.255030  17.445251  17.876595  16.325613  17.846109  \n",
              "112         16.575268  16.688915  18.075123  16.842424  16.197664  15.514608  \n",
              "113         22.131987  20.199621  22.200235  17.248133  19.620770  19.845383  \n",
              "114         14.109294  16.204035  15.792925  13.716809  14.404655  14.814404  \n",
              "115         14.721517  17.103739  16.074038  14.289364  15.554425  16.222134  \n",
              "116         12.997107  13.749556  14.013716  10.372695  11.927193  10.985272  \n",
              "117         16.414146  17.072765  16.466665  17.480803  17.230103  16.883427  \n",
              "118         14.291198  13.916587  11.636734  13.665180  14.407511  12.549096  \n",
              "119         16.745159  17.470987  17.945944  16.336418  17.169342  17.505295  \n",
              "120         15.630411  15.204122  14.950064  15.004929  15.673388  15.213754  \n",
              "121         17.810726  19.284344  19.172670  18.856470  19.541607  19.965685  \n",
              "122         19.647360  19.243868  21.201096  20.084045  20.110874  18.417637  \n",
              "123         14.012307  12.747528  13.420163  17.808979  11.508353  15.156619  \n",
              "124         16.298750  13.906901  16.268400  14.847793  15.116726  14.199623  \n",
              "125         14.851140  13.831740  13.435549  13.726526  14.341100  14.578015  \n",
              "126         15.988145  15.718677  16.556067  15.454973  14.453193  15.170197  \n",
              "127         14.101974  15.293839  15.494998  13.712332  15.096870  13.335627  \n",
              "128         14.791253  16.046762  14.730534  13.598773  15.830203  15.299519  \n",
              "129         14.906216  12.849040  11.975744  12.185539  10.795151  13.907397  \n",
              "130         13.393456  15.215747  15.309612  13.905093  13.376205  14.644567  \n",
              "131         18.625198  19.363024  17.671240  16.533216  16.293737  18.936785  \n",
              "132         15.111024  13.132896  13.931624  11.295387  12.473686  14.278653  \n",
              "133         17.109779  18.483496  17.604773  17.430204  17.774490  18.868977  \n",
              "134         14.947581  14.752124  16.524502  14.073951  15.263971  14.838928  \n",
              "135         18.099485  18.428854  19.635168  19.277618  20.698196  19.148418  \n",
              "136         13.064156  14.234957  11.748864  15.058817  13.095179  14.396820  \n",
              "137         18.638180  19.059113  20.760880  17.410618  18.952610  21.072376  \n",
              "138         16.084507  16.107927  16.948154  17.477104  16.788200  17.111189  \n",
              "139         23.265169  22.569849  22.914104  23.613026  23.739435  25.685558  \n",
              "140         18.796150  18.427479  18.958448  17.340904  17.954952  18.826841  \n",
              "141         15.061109  16.305386  17.156013  15.558686  17.647760  15.456026  \n",
              "142         16.318539  17.719423  17.603010  16.690613  17.317497  17.905115  \n",
              "143         16.340120  16.123327  17.175520  13.561301  17.853672  16.835615  \n",
              "144         21.680519  20.864796  19.198473  22.159407  23.242443  19.984270  \n",
              "145         18.284939  18.713135  20.427660  20.052841  20.923647  18.573553  \n",
              "146         17.349964  16.769785  16.917526  15.062085  16.042135  13.798271  \n",
              "147         23.431351  22.641670  24.929186  23.732643  24.387081  22.957846  \n",
              "148         13.868924  15.060114  15.042121  13.169744  15.078540  13.699736  \n",
              "149         16.284565  17.834591  17.677069  16.385851  17.103159  17.427414  \n",
              "150         14.275929  13.585402  13.376956  15.920220  12.953441  14.310308  \n",
              "151         15.629359  12.930794  15.490872  15.236248  14.847120  14.352596  \n",
              "152         17.896029  16.458750  15.875525  15.997127  17.739792  17.829847  \n",
              "153         15.840024  15.702286  14.997393  16.017178  15.407314  15.862953  \n",
              "154         17.655384  17.449749  18.692944  18.447432  16.591209  18.000166  \n",
              "155         18.299026  17.582544  19.883001  15.633400  18.240993  17.648291  \n",
              "156         12.136477  12.508693  12.120371  11.610428  11.189287  11.865948  \n",
              "157         14.315271  14.850594  16.786749  13.014013  14.727719  14.517136  \n",
              "158         19.125069  19.147783  19.401369  17.621319  19.407167  18.616198  \n",
              "159         16.664917  15.055736  16.188902  14.508099  15.996594  16.542557  \n",
              "160         13.905738  12.970701  12.960770  11.243483  10.451629  13.630434  \n",
              "161         15.077767  15.274019  16.866724  14.863720  13.513193  16.023848  \n",
              "162         11.905570  10.338088  12.293561  10.219535   9.313963  10.212736  \n",
              "163         15.705817  16.977499  16.851252  16.524479  15.763920  16.795004  \n",
              "164         24.579231  23.537191  21.228575  24.530912  23.004763  24.625103  \n",
              "165         16.867836  20.568317  20.344107  18.395348  17.965170  19.410498  \n",
              "166         17.223770  16.833611  18.235550  15.843460  16.101885  18.476952  \n",
              "167         18.261978  18.080011  17.792839  17.336882  18.366009  19.409433  \n",
              "168         15.083026  15.795303  14.535261  15.200462  15.692032  13.432228  \n",
              "169         17.379782  18.859884  18.685171  19.232254  20.504978  19.423815  \n",
              "170         15.679622  16.923880  16.326845  14.271538  14.728286  16.083099  \n",
              "171         21.024809  21.688631  23.281986  20.959579  23.702215  24.094275  \n",
              "172         13.912766  15.070233  15.930956  15.954151  14.565214  15.641470  \n",
              "173         15.267860  15.854618  15.784132  14.822348  16.259447  15.425997  \n",
              "174         13.371299  13.919866  13.419589  12.096485  11.286215  13.916415  \n",
              "175         17.192835  18.174471  17.532591  16.748808  18.420353  17.947203  \n",
              "176         16.461121  17.144526  17.865824  16.162279  16.788891  16.594492  \n",
              "177         22.006868  22.208351  23.537897  22.301935  22.410576  22.626318  \n",
              "178         12.669938  13.150398  13.353300  11.416687  11.689954  13.131042  \n",
              "179         16.627483  15.572855  16.360516  18.115881  17.255943  16.125481  \n",
              "180         17.709021  16.053080  17.255606  16.815731  17.358860  17.006775  \n",
              "181         18.609076  19.851597  19.760954  18.025686  19.007259  19.822062  \n",
              "182         17.910599  17.206158  17.987053  18.052540  18.557220  18.018667  \n",
              "183         17.385801  20.754475  20.019197  20.162527  18.353119  18.437304  \n",
              "184         15.114781  15.941126  15.405235  16.048969  15.975535  16.989092  \n",
              "185         18.831722  20.265886  19.720657  20.438402  20.638552  19.973049  \n",
              "186         20.426895  20.552671  20.153997  20.919634  21.395149  20.560411  \n",
              "187         14.372834  15.638974  14.229697  14.072216  14.269898  15.365809  \n",
              "188         14.308341  15.517136  14.932741  15.099182  15.114908  15.568232  \n",
              "189         16.735199  16.174900  17.250360  15.846347  15.550993  16.584551  \n",
              "190         17.034060  17.677073  16.558111  17.588715  17.649759  17.558111  \n",
              "191         17.882872  20.346523  19.708944  19.171804  22.207342  20.069557  \n",
              "192         18.343943  18.792059  18.316456  19.379578  19.479605  18.485548  \n",
              "193         16.680050  15.400311  16.442144  17.395538  16.100853  15.945303  \n",
              "194         19.788475  18.887751  20.959852  19.780409  18.981382  19.938316  \n",
              "195         15.546432  14.627169  15.276861  15.286697  14.995596  13.456868  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>half_fold0</th>\n",
              "      <th>half_fold1</th>\n",
              "      <th>half_fold2</th>\n",
              "      <th>half_fold3</th>\n",
              "      <th>half_fold4</th>\n",
              "      <th>periocular_fold0</th>\n",
              "      <th>periocular_fold1</th>\n",
              "      <th>periocular_fold2</th>\n",
              "      <th>periocular_fold3</th>\n",
              "      <th>periocular_fold4</th>\n",
              "      <th>eye_fold0</th>\n",
              "      <th>eye_fold1</th>\n",
              "      <th>eye_fold2</th>\n",
              "      <th>eye_fold3</th>\n",
              "      <th>eye_fold4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.0</td>\n",
              "      <td>13.378623</td>\n",
              "      <td>14.685628</td>\n",
              "      <td>13.294028</td>\n",
              "      <td>13.973372</td>\n",
              "      <td>14.123014</td>\n",
              "      <td>14.628881</td>\n",
              "      <td>14.507998</td>\n",
              "      <td>14.967015</td>\n",
              "      <td>13.757207</td>\n",
              "      <td>13.487124</td>\n",
              "      <td>14.274810</td>\n",
              "      <td>14.551019</td>\n",
              "      <td>15.181063</td>\n",
              "      <td>12.912588</td>\n",
              "      <td>13.956070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.527866</td>\n",
              "      <td>17.420464</td>\n",
              "      <td>18.572323</td>\n",
              "      <td>18.026186</td>\n",
              "      <td>17.614462</td>\n",
              "      <td>16.501270</td>\n",
              "      <td>16.867992</td>\n",
              "      <td>17.573963</td>\n",
              "      <td>17.416254</td>\n",
              "      <td>15.861028</td>\n",
              "      <td>16.626665</td>\n",
              "      <td>15.791635</td>\n",
              "      <td>16.090891</td>\n",
              "      <td>18.066759</td>\n",
              "      <td>16.905663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16.0</td>\n",
              "      <td>14.413683</td>\n",
              "      <td>15.394675</td>\n",
              "      <td>13.035702</td>\n",
              "      <td>15.077651</td>\n",
              "      <td>14.736663</td>\n",
              "      <td>15.614464</td>\n",
              "      <td>16.265606</td>\n",
              "      <td>14.597241</td>\n",
              "      <td>16.318535</td>\n",
              "      <td>14.994992</td>\n",
              "      <td>15.825631</td>\n",
              "      <td>17.309702</td>\n",
              "      <td>13.362928</td>\n",
              "      <td>16.382545</td>\n",
              "      <td>16.692049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.310123</td>\n",
              "      <td>20.631779</td>\n",
              "      <td>17.513378</td>\n",
              "      <td>18.866766</td>\n",
              "      <td>18.291342</td>\n",
              "      <td>17.554770</td>\n",
              "      <td>19.608227</td>\n",
              "      <td>17.574945</td>\n",
              "      <td>17.915594</td>\n",
              "      <td>18.106993</td>\n",
              "      <td>18.756432</td>\n",
              "      <td>20.563534</td>\n",
              "      <td>17.468340</td>\n",
              "      <td>19.188147</td>\n",
              "      <td>18.581377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.193491</td>\n",
              "      <td>18.763926</td>\n",
              "      <td>18.263947</td>\n",
              "      <td>18.002565</td>\n",
              "      <td>18.582289</td>\n",
              "      <td>18.368830</td>\n",
              "      <td>18.018055</td>\n",
              "      <td>20.731621</td>\n",
              "      <td>18.140236</td>\n",
              "      <td>19.150448</td>\n",
              "      <td>19.392269</td>\n",
              "      <td>19.751122</td>\n",
              "      <td>18.823761</td>\n",
              "      <td>18.247440</td>\n",
              "      <td>18.200075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21.0</td>\n",
              "      <td>18.632507</td>\n",
              "      <td>20.075674</td>\n",
              "      <td>20.377279</td>\n",
              "      <td>19.139229</td>\n",
              "      <td>20.997280</td>\n",
              "      <td>20.834209</td>\n",
              "      <td>19.681776</td>\n",
              "      <td>20.667870</td>\n",
              "      <td>18.895624</td>\n",
              "      <td>19.149971</td>\n",
              "      <td>20.219944</td>\n",
              "      <td>22.163109</td>\n",
              "      <td>19.191067</td>\n",
              "      <td>18.353054</td>\n",
              "      <td>19.565882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.0</td>\n",
              "      <td>15.866582</td>\n",
              "      <td>14.374516</td>\n",
              "      <td>13.856710</td>\n",
              "      <td>14.705297</td>\n",
              "      <td>17.762205</td>\n",
              "      <td>14.463510</td>\n",
              "      <td>14.217272</td>\n",
              "      <td>15.651177</td>\n",
              "      <td>14.593225</td>\n",
              "      <td>17.372561</td>\n",
              "      <td>15.431116</td>\n",
              "      <td>15.361065</td>\n",
              "      <td>13.709132</td>\n",
              "      <td>13.666772</td>\n",
              "      <td>16.253014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>20.5</td>\n",
              "      <td>19.918442</td>\n",
              "      <td>20.954706</td>\n",
              "      <td>21.114191</td>\n",
              "      <td>19.959185</td>\n",
              "      <td>21.328409</td>\n",
              "      <td>22.539968</td>\n",
              "      <td>20.494875</td>\n",
              "      <td>20.870911</td>\n",
              "      <td>20.251326</td>\n",
              "      <td>20.327513</td>\n",
              "      <td>21.877436</td>\n",
              "      <td>20.002913</td>\n",
              "      <td>19.861214</td>\n",
              "      <td>20.815548</td>\n",
              "      <td>20.882113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.061344</td>\n",
              "      <td>13.705263</td>\n",
              "      <td>13.044377</td>\n",
              "      <td>14.158328</td>\n",
              "      <td>13.660732</td>\n",
              "      <td>14.519747</td>\n",
              "      <td>13.075567</td>\n",
              "      <td>12.439370</td>\n",
              "      <td>15.904149</td>\n",
              "      <td>13.471133</td>\n",
              "      <td>14.589243</td>\n",
              "      <td>13.971314</td>\n",
              "      <td>13.590286</td>\n",
              "      <td>15.549702</td>\n",
              "      <td>13.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.136623</td>\n",
              "      <td>17.785122</td>\n",
              "      <td>16.808434</td>\n",
              "      <td>16.992243</td>\n",
              "      <td>15.816254</td>\n",
              "      <td>16.999086</td>\n",
              "      <td>16.612087</td>\n",
              "      <td>16.711815</td>\n",
              "      <td>17.889015</td>\n",
              "      <td>16.337070</td>\n",
              "      <td>16.643187</td>\n",
              "      <td>17.092567</td>\n",
              "      <td>17.830677</td>\n",
              "      <td>18.145947</td>\n",
              "      <td>15.407084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.522603</td>\n",
              "      <td>15.791290</td>\n",
              "      <td>15.759887</td>\n",
              "      <td>16.061544</td>\n",
              "      <td>15.657650</td>\n",
              "      <td>14.248920</td>\n",
              "      <td>15.313585</td>\n",
              "      <td>15.004320</td>\n",
              "      <td>15.345590</td>\n",
              "      <td>13.985805</td>\n",
              "      <td>14.303629</td>\n",
              "      <td>16.516327</td>\n",
              "      <td>16.150364</td>\n",
              "      <td>15.115948</td>\n",
              "      <td>14.146938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.548234</td>\n",
              "      <td>14.040708</td>\n",
              "      <td>11.965037</td>\n",
              "      <td>13.018007</td>\n",
              "      <td>14.617029</td>\n",
              "      <td>12.243546</td>\n",
              "      <td>13.441017</td>\n",
              "      <td>11.864726</td>\n",
              "      <td>12.637973</td>\n",
              "      <td>13.604434</td>\n",
              "      <td>14.119826</td>\n",
              "      <td>11.922945</td>\n",
              "      <td>14.219202</td>\n",
              "      <td>12.905776</td>\n",
              "      <td>11.435546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>17.0</td>\n",
              "      <td>14.629607</td>\n",
              "      <td>15.688865</td>\n",
              "      <td>16.740273</td>\n",
              "      <td>15.920527</td>\n",
              "      <td>15.881575</td>\n",
              "      <td>16.183681</td>\n",
              "      <td>15.412989</td>\n",
              "      <td>16.605211</td>\n",
              "      <td>16.222584</td>\n",
              "      <td>16.156956</td>\n",
              "      <td>15.513434</td>\n",
              "      <td>14.452009</td>\n",
              "      <td>16.760256</td>\n",
              "      <td>15.432339</td>\n",
              "      <td>14.227433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11.0</td>\n",
              "      <td>11.677833</td>\n",
              "      <td>10.838790</td>\n",
              "      <td>9.319304</td>\n",
              "      <td>12.407351</td>\n",
              "      <td>11.048605</td>\n",
              "      <td>13.044492</td>\n",
              "      <td>11.104789</td>\n",
              "      <td>9.554597</td>\n",
              "      <td>11.319505</td>\n",
              "      <td>11.720358</td>\n",
              "      <td>13.219360</td>\n",
              "      <td>10.565756</td>\n",
              "      <td>10.930639</td>\n",
              "      <td>9.071243</td>\n",
              "      <td>10.346580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20.0</td>\n",
              "      <td>17.516705</td>\n",
              "      <td>20.553877</td>\n",
              "      <td>18.362318</td>\n",
              "      <td>20.199848</td>\n",
              "      <td>19.681187</td>\n",
              "      <td>20.988750</td>\n",
              "      <td>20.506330</td>\n",
              "      <td>20.287525</td>\n",
              "      <td>19.255453</td>\n",
              "      <td>19.822994</td>\n",
              "      <td>18.937723</td>\n",
              "      <td>18.897736</td>\n",
              "      <td>18.827013</td>\n",
              "      <td>17.983629</td>\n",
              "      <td>19.447701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16.0</td>\n",
              "      <td>17.708431</td>\n",
              "      <td>16.385679</td>\n",
              "      <td>14.859704</td>\n",
              "      <td>18.968016</td>\n",
              "      <td>17.258400</td>\n",
              "      <td>16.944719</td>\n",
              "      <td>16.111366</td>\n",
              "      <td>15.687928</td>\n",
              "      <td>16.758253</td>\n",
              "      <td>17.070749</td>\n",
              "      <td>15.329899</td>\n",
              "      <td>15.394428</td>\n",
              "      <td>14.881329</td>\n",
              "      <td>15.775295</td>\n",
              "      <td>16.750942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.047476</td>\n",
              "      <td>14.518665</td>\n",
              "      <td>13.082020</td>\n",
              "      <td>14.970243</td>\n",
              "      <td>14.868305</td>\n",
              "      <td>14.516607</td>\n",
              "      <td>15.281561</td>\n",
              "      <td>14.178105</td>\n",
              "      <td>15.758600</td>\n",
              "      <td>12.927438</td>\n",
              "      <td>16.073662</td>\n",
              "      <td>15.139214</td>\n",
              "      <td>15.010571</td>\n",
              "      <td>15.496675</td>\n",
              "      <td>13.613491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.469520</td>\n",
              "      <td>17.082392</td>\n",
              "      <td>15.009986</td>\n",
              "      <td>16.953379</td>\n",
              "      <td>14.858942</td>\n",
              "      <td>16.724972</td>\n",
              "      <td>16.515635</td>\n",
              "      <td>15.868102</td>\n",
              "      <td>16.486046</td>\n",
              "      <td>16.016472</td>\n",
              "      <td>17.086077</td>\n",
              "      <td>16.276300</td>\n",
              "      <td>15.420680</td>\n",
              "      <td>17.056505</td>\n",
              "      <td>15.683866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>14.0</td>\n",
              "      <td>15.383628</td>\n",
              "      <td>15.075351</td>\n",
              "      <td>14.994046</td>\n",
              "      <td>14.018382</td>\n",
              "      <td>14.425631</td>\n",
              "      <td>13.622437</td>\n",
              "      <td>13.988466</td>\n",
              "      <td>13.342361</td>\n",
              "      <td>13.869771</td>\n",
              "      <td>13.816732</td>\n",
              "      <td>13.724952</td>\n",
              "      <td>13.537360</td>\n",
              "      <td>13.630291</td>\n",
              "      <td>13.367039</td>\n",
              "      <td>13.935292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>24.0</td>\n",
              "      <td>21.722124</td>\n",
              "      <td>21.975380</td>\n",
              "      <td>23.327999</td>\n",
              "      <td>23.090868</td>\n",
              "      <td>24.661572</td>\n",
              "      <td>24.632839</td>\n",
              "      <td>21.624823</td>\n",
              "      <td>26.178135</td>\n",
              "      <td>26.117241</td>\n",
              "      <td>22.968039</td>\n",
              "      <td>22.487616</td>\n",
              "      <td>21.479668</td>\n",
              "      <td>25.478451</td>\n",
              "      <td>24.955587</td>\n",
              "      <td>24.256048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>18.0</td>\n",
              "      <td>16.031721</td>\n",
              "      <td>16.109421</td>\n",
              "      <td>15.816394</td>\n",
              "      <td>15.655579</td>\n",
              "      <td>17.090631</td>\n",
              "      <td>17.300663</td>\n",
              "      <td>15.543173</td>\n",
              "      <td>18.012842</td>\n",
              "      <td>18.382524</td>\n",
              "      <td>16.377371</td>\n",
              "      <td>16.651539</td>\n",
              "      <td>16.770557</td>\n",
              "      <td>17.794308</td>\n",
              "      <td>16.979347</td>\n",
              "      <td>17.049082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>18.0</td>\n",
              "      <td>19.102634</td>\n",
              "      <td>17.056438</td>\n",
              "      <td>17.621603</td>\n",
              "      <td>17.788414</td>\n",
              "      <td>18.857197</td>\n",
              "      <td>18.164101</td>\n",
              "      <td>18.337790</td>\n",
              "      <td>16.614420</td>\n",
              "      <td>18.743999</td>\n",
              "      <td>18.175346</td>\n",
              "      <td>17.354548</td>\n",
              "      <td>17.525187</td>\n",
              "      <td>17.234940</td>\n",
              "      <td>17.425377</td>\n",
              "      <td>17.813021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>19.0</td>\n",
              "      <td>17.255619</td>\n",
              "      <td>18.926838</td>\n",
              "      <td>18.987755</td>\n",
              "      <td>20.596418</td>\n",
              "      <td>18.632954</td>\n",
              "      <td>19.612595</td>\n",
              "      <td>18.230295</td>\n",
              "      <td>19.568745</td>\n",
              "      <td>18.702543</td>\n",
              "      <td>17.536196</td>\n",
              "      <td>18.143642</td>\n",
              "      <td>18.333529</td>\n",
              "      <td>19.019339</td>\n",
              "      <td>19.092112</td>\n",
              "      <td>17.908457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>13.0</td>\n",
              "      <td>12.973821</td>\n",
              "      <td>15.818367</td>\n",
              "      <td>10.536770</td>\n",
              "      <td>13.935122</td>\n",
              "      <td>11.171301</td>\n",
              "      <td>13.138489</td>\n",
              "      <td>14.073352</td>\n",
              "      <td>12.837631</td>\n",
              "      <td>13.603251</td>\n",
              "      <td>13.373225</td>\n",
              "      <td>13.282551</td>\n",
              "      <td>15.528111</td>\n",
              "      <td>11.782574</td>\n",
              "      <td>13.547136</td>\n",
              "      <td>14.660616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>20.0</td>\n",
              "      <td>17.271559</td>\n",
              "      <td>19.924372</td>\n",
              "      <td>18.244999</td>\n",
              "      <td>18.052849</td>\n",
              "      <td>17.481569</td>\n",
              "      <td>19.386703</td>\n",
              "      <td>18.547815</td>\n",
              "      <td>18.664896</td>\n",
              "      <td>18.857344</td>\n",
              "      <td>16.567917</td>\n",
              "      <td>18.338924</td>\n",
              "      <td>19.612276</td>\n",
              "      <td>20.245289</td>\n",
              "      <td>20.037903</td>\n",
              "      <td>18.387369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.486809</td>\n",
              "      <td>13.468585</td>\n",
              "      <td>11.514981</td>\n",
              "      <td>12.488697</td>\n",
              "      <td>13.688466</td>\n",
              "      <td>13.913793</td>\n",
              "      <td>10.742638</td>\n",
              "      <td>11.333972</td>\n",
              "      <td>13.030467</td>\n",
              "      <td>12.147717</td>\n",
              "      <td>12.672179</td>\n",
              "      <td>10.751394</td>\n",
              "      <td>12.341632</td>\n",
              "      <td>12.393042</td>\n",
              "      <td>10.098878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>19.0</td>\n",
              "      <td>17.736250</td>\n",
              "      <td>19.385244</td>\n",
              "      <td>17.831570</td>\n",
              "      <td>18.329245</td>\n",
              "      <td>18.747551</td>\n",
              "      <td>19.045212</td>\n",
              "      <td>18.828514</td>\n",
              "      <td>19.000971</td>\n",
              "      <td>18.597195</td>\n",
              "      <td>17.756920</td>\n",
              "      <td>20.073200</td>\n",
              "      <td>19.055943</td>\n",
              "      <td>18.644924</td>\n",
              "      <td>19.172749</td>\n",
              "      <td>17.413557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>18.0</td>\n",
              "      <td>18.750006</td>\n",
              "      <td>17.519140</td>\n",
              "      <td>18.658243</td>\n",
              "      <td>18.325762</td>\n",
              "      <td>18.970346</td>\n",
              "      <td>18.714369</td>\n",
              "      <td>17.978840</td>\n",
              "      <td>18.249096</td>\n",
              "      <td>16.872839</td>\n",
              "      <td>18.102043</td>\n",
              "      <td>17.598238</td>\n",
              "      <td>17.289528</td>\n",
              "      <td>17.775970</td>\n",
              "      <td>18.095301</td>\n",
              "      <td>17.829952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>11.0</td>\n",
              "      <td>12.000104</td>\n",
              "      <td>11.165142</td>\n",
              "      <td>10.432534</td>\n",
              "      <td>12.097527</td>\n",
              "      <td>10.197731</td>\n",
              "      <td>11.144415</td>\n",
              "      <td>11.647454</td>\n",
              "      <td>10.341166</td>\n",
              "      <td>11.788428</td>\n",
              "      <td>11.335707</td>\n",
              "      <td>12.730444</td>\n",
              "      <td>11.762501</td>\n",
              "      <td>10.346120</td>\n",
              "      <td>9.327543</td>\n",
              "      <td>11.315918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>18.0</td>\n",
              "      <td>16.010939</td>\n",
              "      <td>18.640636</td>\n",
              "      <td>16.737968</td>\n",
              "      <td>18.007343</td>\n",
              "      <td>17.600397</td>\n",
              "      <td>16.610077</td>\n",
              "      <td>17.198915</td>\n",
              "      <td>16.898172</td>\n",
              "      <td>17.616724</td>\n",
              "      <td>19.009506</td>\n",
              "      <td>17.518831</td>\n",
              "      <td>17.157833</td>\n",
              "      <td>16.383596</td>\n",
              "      <td>18.974602</td>\n",
              "      <td>17.499184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>12.0</td>\n",
              "      <td>13.460879</td>\n",
              "      <td>12.299352</td>\n",
              "      <td>12.730235</td>\n",
              "      <td>12.924224</td>\n",
              "      <td>12.861664</td>\n",
              "      <td>12.026237</td>\n",
              "      <td>12.527395</td>\n",
              "      <td>13.294838</td>\n",
              "      <td>12.608129</td>\n",
              "      <td>12.472093</td>\n",
              "      <td>13.797016</td>\n",
              "      <td>13.349607</td>\n",
              "      <td>14.710016</td>\n",
              "      <td>11.561083</td>\n",
              "      <td>13.174564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>17.0</td>\n",
              "      <td>17.096197</td>\n",
              "      <td>17.247944</td>\n",
              "      <td>17.533447</td>\n",
              "      <td>18.622046</td>\n",
              "      <td>18.955292</td>\n",
              "      <td>17.720116</td>\n",
              "      <td>18.117304</td>\n",
              "      <td>17.109341</td>\n",
              "      <td>17.165464</td>\n",
              "      <td>17.720928</td>\n",
              "      <td>16.522379</td>\n",
              "      <td>16.882860</td>\n",
              "      <td>17.292444</td>\n",
              "      <td>17.232567</td>\n",
              "      <td>16.440983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.659000</td>\n",
              "      <td>15.800076</td>\n",
              "      <td>14.273402</td>\n",
              "      <td>14.578097</td>\n",
              "      <td>14.725208</td>\n",
              "      <td>14.966218</td>\n",
              "      <td>16.003679</td>\n",
              "      <td>15.573401</td>\n",
              "      <td>15.689538</td>\n",
              "      <td>15.614478</td>\n",
              "      <td>16.513536</td>\n",
              "      <td>16.279016</td>\n",
              "      <td>12.826980</td>\n",
              "      <td>15.106087</td>\n",
              "      <td>16.898932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.311764</td>\n",
              "      <td>17.195402</td>\n",
              "      <td>15.020043</td>\n",
              "      <td>16.367628</td>\n",
              "      <td>16.041491</td>\n",
              "      <td>16.786741</td>\n",
              "      <td>15.626900</td>\n",
              "      <td>14.794608</td>\n",
              "      <td>16.103279</td>\n",
              "      <td>16.376207</td>\n",
              "      <td>15.401998</td>\n",
              "      <td>14.607372</td>\n",
              "      <td>15.667814</td>\n",
              "      <td>14.196516</td>\n",
              "      <td>15.944733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>17.0</td>\n",
              "      <td>18.674849</td>\n",
              "      <td>17.429712</td>\n",
              "      <td>18.653074</td>\n",
              "      <td>19.794067</td>\n",
              "      <td>18.402611</td>\n",
              "      <td>18.106737</td>\n",
              "      <td>17.721672</td>\n",
              "      <td>15.507354</td>\n",
              "      <td>20.388166</td>\n",
              "      <td>17.932936</td>\n",
              "      <td>18.054737</td>\n",
              "      <td>18.160099</td>\n",
              "      <td>18.281828</td>\n",
              "      <td>19.237141</td>\n",
              "      <td>16.333475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>14.0</td>\n",
              "      <td>15.063380</td>\n",
              "      <td>16.471708</td>\n",
              "      <td>13.893177</td>\n",
              "      <td>16.282831</td>\n",
              "      <td>14.372046</td>\n",
              "      <td>14.242184</td>\n",
              "      <td>15.473936</td>\n",
              "      <td>14.582948</td>\n",
              "      <td>14.620574</td>\n",
              "      <td>13.957411</td>\n",
              "      <td>15.585864</td>\n",
              "      <td>16.118095</td>\n",
              "      <td>13.482611</td>\n",
              "      <td>13.998526</td>\n",
              "      <td>15.377835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>10.0</td>\n",
              "      <td>15.929156</td>\n",
              "      <td>11.749814</td>\n",
              "      <td>14.190647</td>\n",
              "      <td>14.377904</td>\n",
              "      <td>11.620865</td>\n",
              "      <td>16.791941</td>\n",
              "      <td>10.310551</td>\n",
              "      <td>13.376885</td>\n",
              "      <td>11.281787</td>\n",
              "      <td>11.905852</td>\n",
              "      <td>15.985344</td>\n",
              "      <td>12.500313</td>\n",
              "      <td>9.043743</td>\n",
              "      <td>10.957983</td>\n",
              "      <td>12.759283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>22.0</td>\n",
              "      <td>19.954533</td>\n",
              "      <td>23.259424</td>\n",
              "      <td>21.624434</td>\n",
              "      <td>20.013008</td>\n",
              "      <td>22.180674</td>\n",
              "      <td>20.115894</td>\n",
              "      <td>22.307640</td>\n",
              "      <td>21.779461</td>\n",
              "      <td>20.058226</td>\n",
              "      <td>20.591259</td>\n",
              "      <td>20.536217</td>\n",
              "      <td>21.320547</td>\n",
              "      <td>20.877834</td>\n",
              "      <td>21.055281</td>\n",
              "      <td>20.584452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>19.0</td>\n",
              "      <td>17.015457</td>\n",
              "      <td>18.171274</td>\n",
              "      <td>17.846905</td>\n",
              "      <td>19.292269</td>\n",
              "      <td>16.229349</td>\n",
              "      <td>18.455141</td>\n",
              "      <td>18.710060</td>\n",
              "      <td>19.098074</td>\n",
              "      <td>18.763845</td>\n",
              "      <td>16.633511</td>\n",
              "      <td>19.098383</td>\n",
              "      <td>18.197874</td>\n",
              "      <td>18.524269</td>\n",
              "      <td>17.162685</td>\n",
              "      <td>18.106840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>9.0</td>\n",
              "      <td>9.107154</td>\n",
              "      <td>9.802299</td>\n",
              "      <td>7.524721</td>\n",
              "      <td>10.926050</td>\n",
              "      <td>8.608464</td>\n",
              "      <td>8.628554</td>\n",
              "      <td>8.366113</td>\n",
              "      <td>9.780136</td>\n",
              "      <td>9.999585</td>\n",
              "      <td>9.937102</td>\n",
              "      <td>10.454482</td>\n",
              "      <td>9.878193</td>\n",
              "      <td>9.276745</td>\n",
              "      <td>8.533656</td>\n",
              "      <td>10.181276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>17.0</td>\n",
              "      <td>14.499024</td>\n",
              "      <td>17.556055</td>\n",
              "      <td>15.800036</td>\n",
              "      <td>15.371421</td>\n",
              "      <td>16.679575</td>\n",
              "      <td>16.511866</td>\n",
              "      <td>16.411034</td>\n",
              "      <td>17.106390</td>\n",
              "      <td>16.519764</td>\n",
              "      <td>16.193165</td>\n",
              "      <td>15.639833</td>\n",
              "      <td>16.264223</td>\n",
              "      <td>14.518070</td>\n",
              "      <td>15.666966</td>\n",
              "      <td>16.671841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.650383</td>\n",
              "      <td>19.351572</td>\n",
              "      <td>17.880341</td>\n",
              "      <td>17.638901</td>\n",
              "      <td>20.056206</td>\n",
              "      <td>17.829126</td>\n",
              "      <td>17.749088</td>\n",
              "      <td>17.225000</td>\n",
              "      <td>17.677038</td>\n",
              "      <td>17.929848</td>\n",
              "      <td>16.982676</td>\n",
              "      <td>17.167124</td>\n",
              "      <td>17.922649</td>\n",
              "      <td>16.915779</td>\n",
              "      <td>18.005463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>24.0</td>\n",
              "      <td>22.163698</td>\n",
              "      <td>24.172239</td>\n",
              "      <td>22.358864</td>\n",
              "      <td>20.693138</td>\n",
              "      <td>19.157946</td>\n",
              "      <td>24.360102</td>\n",
              "      <td>23.935865</td>\n",
              "      <td>25.646303</td>\n",
              "      <td>24.353662</td>\n",
              "      <td>18.593773</td>\n",
              "      <td>23.661455</td>\n",
              "      <td>24.748644</td>\n",
              "      <td>23.649305</td>\n",
              "      <td>25.363045</td>\n",
              "      <td>18.875423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.295153</td>\n",
              "      <td>13.523074</td>\n",
              "      <td>13.151304</td>\n",
              "      <td>15.168266</td>\n",
              "      <td>15.430214</td>\n",
              "      <td>14.536941</td>\n",
              "      <td>15.247784</td>\n",
              "      <td>13.697691</td>\n",
              "      <td>15.836574</td>\n",
              "      <td>15.261174</td>\n",
              "      <td>15.732479</td>\n",
              "      <td>15.682404</td>\n",
              "      <td>13.994151</td>\n",
              "      <td>13.474389</td>\n",
              "      <td>14.631439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.998239</td>\n",
              "      <td>14.551076</td>\n",
              "      <td>12.997133</td>\n",
              "      <td>13.976496</td>\n",
              "      <td>15.119221</td>\n",
              "      <td>13.723742</td>\n",
              "      <td>12.739313</td>\n",
              "      <td>13.166018</td>\n",
              "      <td>12.730762</td>\n",
              "      <td>12.998941</td>\n",
              "      <td>13.278191</td>\n",
              "      <td>12.952986</td>\n",
              "      <td>12.486665</td>\n",
              "      <td>12.314035</td>\n",
              "      <td>12.922033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.632641</td>\n",
              "      <td>20.715481</td>\n",
              "      <td>20.028692</td>\n",
              "      <td>20.496227</td>\n",
              "      <td>20.264244</td>\n",
              "      <td>20.532089</td>\n",
              "      <td>19.959583</td>\n",
              "      <td>19.151796</td>\n",
              "      <td>20.360538</td>\n",
              "      <td>20.506062</td>\n",
              "      <td>18.524500</td>\n",
              "      <td>18.686649</td>\n",
              "      <td>19.057964</td>\n",
              "      <td>19.539009</td>\n",
              "      <td>19.724142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>19.0</td>\n",
              "      <td>17.370024</td>\n",
              "      <td>18.671225</td>\n",
              "      <td>16.876211</td>\n",
              "      <td>17.012009</td>\n",
              "      <td>17.729921</td>\n",
              "      <td>18.511530</td>\n",
              "      <td>18.333344</td>\n",
              "      <td>19.476656</td>\n",
              "      <td>19.378344</td>\n",
              "      <td>17.823067</td>\n",
              "      <td>17.391676</td>\n",
              "      <td>18.299767</td>\n",
              "      <td>17.184555</td>\n",
              "      <td>18.324238</td>\n",
              "      <td>17.569887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>21.0</td>\n",
              "      <td>19.545668</td>\n",
              "      <td>22.429083</td>\n",
              "      <td>22.122698</td>\n",
              "      <td>20.139862</td>\n",
              "      <td>22.100269</td>\n",
              "      <td>18.350037</td>\n",
              "      <td>20.610500</td>\n",
              "      <td>22.435776</td>\n",
              "      <td>22.209272</td>\n",
              "      <td>22.162458</td>\n",
              "      <td>20.680803</td>\n",
              "      <td>20.623741</td>\n",
              "      <td>20.505507</td>\n",
              "      <td>20.886307</td>\n",
              "      <td>21.455515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.270207</td>\n",
              "      <td>16.172794</td>\n",
              "      <td>16.527843</td>\n",
              "      <td>16.083225</td>\n",
              "      <td>17.036819</td>\n",
              "      <td>15.305883</td>\n",
              "      <td>15.397141</td>\n",
              "      <td>15.044633</td>\n",
              "      <td>15.689833</td>\n",
              "      <td>15.715986</td>\n",
              "      <td>15.078802</td>\n",
              "      <td>15.174941</td>\n",
              "      <td>15.545979</td>\n",
              "      <td>16.138443</td>\n",
              "      <td>15.832549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>12.0</td>\n",
              "      <td>13.529584</td>\n",
              "      <td>13.393477</td>\n",
              "      <td>10.963497</td>\n",
              "      <td>12.996196</td>\n",
              "      <td>12.773609</td>\n",
              "      <td>13.597820</td>\n",
              "      <td>13.187181</td>\n",
              "      <td>12.225644</td>\n",
              "      <td>11.552445</td>\n",
              "      <td>12.682956</td>\n",
              "      <td>12.915602</td>\n",
              "      <td>11.882187</td>\n",
              "      <td>12.200031</td>\n",
              "      <td>11.783596</td>\n",
              "      <td>12.549768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>18.0</td>\n",
              "      <td>18.156004</td>\n",
              "      <td>19.980135</td>\n",
              "      <td>19.242186</td>\n",
              "      <td>18.312149</td>\n",
              "      <td>19.839521</td>\n",
              "      <td>19.312429</td>\n",
              "      <td>17.413691</td>\n",
              "      <td>16.555969</td>\n",
              "      <td>18.821276</td>\n",
              "      <td>19.416414</td>\n",
              "      <td>18.463596</td>\n",
              "      <td>18.192904</td>\n",
              "      <td>17.650389</td>\n",
              "      <td>17.874939</td>\n",
              "      <td>18.244631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.600533</td>\n",
              "      <td>17.262775</td>\n",
              "      <td>15.227246</td>\n",
              "      <td>14.840694</td>\n",
              "      <td>16.934450</td>\n",
              "      <td>14.634020</td>\n",
              "      <td>16.045580</td>\n",
              "      <td>16.711262</td>\n",
              "      <td>16.084055</td>\n",
              "      <td>16.018074</td>\n",
              "      <td>15.481498</td>\n",
              "      <td>15.909639</td>\n",
              "      <td>16.872824</td>\n",
              "      <td>15.876369</td>\n",
              "      <td>15.420093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>19.0</td>\n",
              "      <td>17.717543</td>\n",
              "      <td>19.299875</td>\n",
              "      <td>18.949003</td>\n",
              "      <td>17.242058</td>\n",
              "      <td>18.199465</td>\n",
              "      <td>18.000790</td>\n",
              "      <td>18.883318</td>\n",
              "      <td>18.719837</td>\n",
              "      <td>19.084002</td>\n",
              "      <td>18.092539</td>\n",
              "      <td>19.691950</td>\n",
              "      <td>18.799528</td>\n",
              "      <td>17.701302</td>\n",
              "      <td>17.638187</td>\n",
              "      <td>19.664036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>20.0</td>\n",
              "      <td>17.804230</td>\n",
              "      <td>19.396788</td>\n",
              "      <td>16.465361</td>\n",
              "      <td>18.109066</td>\n",
              "      <td>19.420012</td>\n",
              "      <td>19.533716</td>\n",
              "      <td>20.121250</td>\n",
              "      <td>16.654278</td>\n",
              "      <td>18.881573</td>\n",
              "      <td>19.347694</td>\n",
              "      <td>18.988909</td>\n",
              "      <td>19.176096</td>\n",
              "      <td>16.655128</td>\n",
              "      <td>19.042583</td>\n",
              "      <td>18.601881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.788042</td>\n",
              "      <td>17.143147</td>\n",
              "      <td>16.955376</td>\n",
              "      <td>16.729162</td>\n",
              "      <td>16.026024</td>\n",
              "      <td>17.203480</td>\n",
              "      <td>17.416317</td>\n",
              "      <td>16.558975</td>\n",
              "      <td>17.568600</td>\n",
              "      <td>16.056961</td>\n",
              "      <td>15.758860</td>\n",
              "      <td>15.528811</td>\n",
              "      <td>17.307396</td>\n",
              "      <td>15.720948</td>\n",
              "      <td>15.627150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>21.0</td>\n",
              "      <td>19.318007</td>\n",
              "      <td>20.997274</td>\n",
              "      <td>19.663721</td>\n",
              "      <td>20.871473</td>\n",
              "      <td>21.455597</td>\n",
              "      <td>21.098988</td>\n",
              "      <td>22.095234</td>\n",
              "      <td>21.978210</td>\n",
              "      <td>21.728188</td>\n",
              "      <td>20.789938</td>\n",
              "      <td>21.219679</td>\n",
              "      <td>21.197365</td>\n",
              "      <td>21.057579</td>\n",
              "      <td>19.929359</td>\n",
              "      <td>20.432877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.947807</td>\n",
              "      <td>18.202932</td>\n",
              "      <td>16.327032</td>\n",
              "      <td>16.791550</td>\n",
              "      <td>18.988745</td>\n",
              "      <td>17.828665</td>\n",
              "      <td>18.486780</td>\n",
              "      <td>17.163084</td>\n",
              "      <td>17.825018</td>\n",
              "      <td>18.419476</td>\n",
              "      <td>16.984722</td>\n",
              "      <td>17.102310</td>\n",
              "      <td>16.774094</td>\n",
              "      <td>16.653841</td>\n",
              "      <td>18.727053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>16.0</td>\n",
              "      <td>14.775107</td>\n",
              "      <td>16.087587</td>\n",
              "      <td>14.202390</td>\n",
              "      <td>14.481210</td>\n",
              "      <td>14.054086</td>\n",
              "      <td>14.952366</td>\n",
              "      <td>14.356369</td>\n",
              "      <td>13.215174</td>\n",
              "      <td>15.356174</td>\n",
              "      <td>15.137986</td>\n",
              "      <td>14.326322</td>\n",
              "      <td>14.993634</td>\n",
              "      <td>14.245468</td>\n",
              "      <td>14.420192</td>\n",
              "      <td>15.192637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.745102</td>\n",
              "      <td>20.208601</td>\n",
              "      <td>19.859838</td>\n",
              "      <td>18.580120</td>\n",
              "      <td>19.657810</td>\n",
              "      <td>20.849588</td>\n",
              "      <td>20.947172</td>\n",
              "      <td>19.593424</td>\n",
              "      <td>20.565172</td>\n",
              "      <td>20.073389</td>\n",
              "      <td>20.714273</td>\n",
              "      <td>20.447802</td>\n",
              "      <td>19.551035</td>\n",
              "      <td>18.888151</td>\n",
              "      <td>19.743498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>18.0</td>\n",
              "      <td>15.801980</td>\n",
              "      <td>15.065242</td>\n",
              "      <td>15.927911</td>\n",
              "      <td>16.535433</td>\n",
              "      <td>17.149719</td>\n",
              "      <td>18.568817</td>\n",
              "      <td>15.058388</td>\n",
              "      <td>19.007841</td>\n",
              "      <td>17.639315</td>\n",
              "      <td>16.937702</td>\n",
              "      <td>15.334141</td>\n",
              "      <td>15.009532</td>\n",
              "      <td>17.754236</td>\n",
              "      <td>17.370213</td>\n",
              "      <td>16.150246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>19.0</td>\n",
              "      <td>19.627504</td>\n",
              "      <td>20.154867</td>\n",
              "      <td>19.572161</td>\n",
              "      <td>20.170290</td>\n",
              "      <td>18.956127</td>\n",
              "      <td>19.014236</td>\n",
              "      <td>18.218279</td>\n",
              "      <td>20.199995</td>\n",
              "      <td>20.947311</td>\n",
              "      <td>20.355782</td>\n",
              "      <td>18.980156</td>\n",
              "      <td>18.810745</td>\n",
              "      <td>18.015308</td>\n",
              "      <td>20.764074</td>\n",
              "      <td>19.665220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.810636</td>\n",
              "      <td>17.553711</td>\n",
              "      <td>15.826330</td>\n",
              "      <td>18.098150</td>\n",
              "      <td>16.370750</td>\n",
              "      <td>17.288031</td>\n",
              "      <td>17.221249</td>\n",
              "      <td>16.823380</td>\n",
              "      <td>17.132397</td>\n",
              "      <td>16.048351</td>\n",
              "      <td>16.837603</td>\n",
              "      <td>17.209518</td>\n",
              "      <td>16.826164</td>\n",
              "      <td>16.741520</td>\n",
              "      <td>16.474344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>12.0</td>\n",
              "      <td>12.446628</td>\n",
              "      <td>14.410253</td>\n",
              "      <td>11.627754</td>\n",
              "      <td>13.825035</td>\n",
              "      <td>13.439529</td>\n",
              "      <td>11.711887</td>\n",
              "      <td>11.583888</td>\n",
              "      <td>11.604568</td>\n",
              "      <td>12.158920</td>\n",
              "      <td>12.975930</td>\n",
              "      <td>13.113430</td>\n",
              "      <td>13.503294</td>\n",
              "      <td>10.657827</td>\n",
              "      <td>11.066300</td>\n",
              "      <td>13.136365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>19.0</td>\n",
              "      <td>19.801182</td>\n",
              "      <td>18.163044</td>\n",
              "      <td>17.045841</td>\n",
              "      <td>17.733816</td>\n",
              "      <td>18.545794</td>\n",
              "      <td>20.058847</td>\n",
              "      <td>18.654970</td>\n",
              "      <td>18.284294</td>\n",
              "      <td>18.650532</td>\n",
              "      <td>17.163870</td>\n",
              "      <td>19.994102</td>\n",
              "      <td>19.747293</td>\n",
              "      <td>18.003857</td>\n",
              "      <td>17.976900</td>\n",
              "      <td>18.831484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.081377</td>\n",
              "      <td>15.295362</td>\n",
              "      <td>15.540390</td>\n",
              "      <td>15.066266</td>\n",
              "      <td>13.271460</td>\n",
              "      <td>14.965826</td>\n",
              "      <td>14.932250</td>\n",
              "      <td>15.921086</td>\n",
              "      <td>15.549075</td>\n",
              "      <td>13.340388</td>\n",
              "      <td>14.961090</td>\n",
              "      <td>13.729846</td>\n",
              "      <td>15.408571</td>\n",
              "      <td>15.403586</td>\n",
              "      <td>15.565741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.222950</td>\n",
              "      <td>18.541761</td>\n",
              "      <td>18.913450</td>\n",
              "      <td>18.605051</td>\n",
              "      <td>18.408512</td>\n",
              "      <td>19.236889</td>\n",
              "      <td>19.314754</td>\n",
              "      <td>20.088148</td>\n",
              "      <td>18.985424</td>\n",
              "      <td>18.472523</td>\n",
              "      <td>21.296585</td>\n",
              "      <td>19.221098</td>\n",
              "      <td>18.761751</td>\n",
              "      <td>18.817520</td>\n",
              "      <td>19.542006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>19.0</td>\n",
              "      <td>16.552792</td>\n",
              "      <td>17.515215</td>\n",
              "      <td>17.166183</td>\n",
              "      <td>19.798037</td>\n",
              "      <td>19.634186</td>\n",
              "      <td>17.013847</td>\n",
              "      <td>17.087328</td>\n",
              "      <td>17.908186</td>\n",
              "      <td>19.691286</td>\n",
              "      <td>18.592529</td>\n",
              "      <td>15.882755</td>\n",
              "      <td>17.471411</td>\n",
              "      <td>18.161449</td>\n",
              "      <td>19.120003</td>\n",
              "      <td>17.991695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.876671</td>\n",
              "      <td>20.514748</td>\n",
              "      <td>19.687992</td>\n",
              "      <td>19.829268</td>\n",
              "      <td>19.764217</td>\n",
              "      <td>20.698696</td>\n",
              "      <td>20.213478</td>\n",
              "      <td>19.137295</td>\n",
              "      <td>19.770252</td>\n",
              "      <td>19.113983</td>\n",
              "      <td>19.896721</td>\n",
              "      <td>18.435385</td>\n",
              "      <td>17.971863</td>\n",
              "      <td>18.276350</td>\n",
              "      <td>19.331652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>15.0</td>\n",
              "      <td>16.880955</td>\n",
              "      <td>16.031755</td>\n",
              "      <td>15.664324</td>\n",
              "      <td>16.272444</td>\n",
              "      <td>15.212618</td>\n",
              "      <td>14.150229</td>\n",
              "      <td>15.252031</td>\n",
              "      <td>14.023492</td>\n",
              "      <td>16.222433</td>\n",
              "      <td>16.235882</td>\n",
              "      <td>16.650421</td>\n",
              "      <td>15.114342</td>\n",
              "      <td>15.298063</td>\n",
              "      <td>15.893363</td>\n",
              "      <td>15.627771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>9.0</td>\n",
              "      <td>8.981208</td>\n",
              "      <td>10.203212</td>\n",
              "      <td>8.664715</td>\n",
              "      <td>10.068004</td>\n",
              "      <td>9.997063</td>\n",
              "      <td>9.371231</td>\n",
              "      <td>8.508332</td>\n",
              "      <td>9.183559</td>\n",
              "      <td>9.491282</td>\n",
              "      <td>10.014640</td>\n",
              "      <td>10.351007</td>\n",
              "      <td>10.202777</td>\n",
              "      <td>9.856271</td>\n",
              "      <td>8.680709</td>\n",
              "      <td>9.017848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.718888</td>\n",
              "      <td>16.137630</td>\n",
              "      <td>17.399063</td>\n",
              "      <td>16.677647</td>\n",
              "      <td>15.042620</td>\n",
              "      <td>16.476988</td>\n",
              "      <td>16.433208</td>\n",
              "      <td>16.740553</td>\n",
              "      <td>16.651012</td>\n",
              "      <td>15.296760</td>\n",
              "      <td>16.859655</td>\n",
              "      <td>17.002056</td>\n",
              "      <td>17.182262</td>\n",
              "      <td>15.775515</td>\n",
              "      <td>16.639914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>16.0</td>\n",
              "      <td>17.429861</td>\n",
              "      <td>17.173769</td>\n",
              "      <td>14.670510</td>\n",
              "      <td>14.019113</td>\n",
              "      <td>15.954159</td>\n",
              "      <td>15.556892</td>\n",
              "      <td>16.039589</td>\n",
              "      <td>18.462616</td>\n",
              "      <td>16.250582</td>\n",
              "      <td>14.743722</td>\n",
              "      <td>15.373385</td>\n",
              "      <td>18.208101</td>\n",
              "      <td>16.761637</td>\n",
              "      <td>13.922328</td>\n",
              "      <td>16.186548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.434525</td>\n",
              "      <td>14.826165</td>\n",
              "      <td>16.572517</td>\n",
              "      <td>14.204306</td>\n",
              "      <td>12.455984</td>\n",
              "      <td>12.669462</td>\n",
              "      <td>13.514441</td>\n",
              "      <td>15.623628</td>\n",
              "      <td>12.887915</td>\n",
              "      <td>13.757453</td>\n",
              "      <td>14.307617</td>\n",
              "      <td>14.090839</td>\n",
              "      <td>15.569736</td>\n",
              "      <td>14.256449</td>\n",
              "      <td>13.803965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>20.0</td>\n",
              "      <td>17.742685</td>\n",
              "      <td>20.352535</td>\n",
              "      <td>20.306265</td>\n",
              "      <td>18.540133</td>\n",
              "      <td>20.291185</td>\n",
              "      <td>21.281702</td>\n",
              "      <td>18.846563</td>\n",
              "      <td>19.671055</td>\n",
              "      <td>20.078466</td>\n",
              "      <td>20.559238</td>\n",
              "      <td>19.931515</td>\n",
              "      <td>18.848400</td>\n",
              "      <td>19.345093</td>\n",
              "      <td>20.227673</td>\n",
              "      <td>21.121088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.639606</td>\n",
              "      <td>15.701774</td>\n",
              "      <td>14.921044</td>\n",
              "      <td>14.789254</td>\n",
              "      <td>14.881083</td>\n",
              "      <td>14.293040</td>\n",
              "      <td>15.803938</td>\n",
              "      <td>15.697816</td>\n",
              "      <td>15.171117</td>\n",
              "      <td>14.600053</td>\n",
              "      <td>15.098475</td>\n",
              "      <td>14.859530</td>\n",
              "      <td>16.653254</td>\n",
              "      <td>12.956518</td>\n",
              "      <td>15.458138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>20.0</td>\n",
              "      <td>18.624105</td>\n",
              "      <td>20.478455</td>\n",
              "      <td>18.657927</td>\n",
              "      <td>16.123985</td>\n",
              "      <td>18.745687</td>\n",
              "      <td>20.532948</td>\n",
              "      <td>19.187893</td>\n",
              "      <td>17.729475</td>\n",
              "      <td>20.145765</td>\n",
              "      <td>17.543434</td>\n",
              "      <td>19.284327</td>\n",
              "      <td>20.020079</td>\n",
              "      <td>20.713921</td>\n",
              "      <td>20.723600</td>\n",
              "      <td>17.147121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>20.0</td>\n",
              "      <td>17.051630</td>\n",
              "      <td>19.490273</td>\n",
              "      <td>18.934885</td>\n",
              "      <td>18.423679</td>\n",
              "      <td>19.744778</td>\n",
              "      <td>18.788286</td>\n",
              "      <td>19.020283</td>\n",
              "      <td>20.144550</td>\n",
              "      <td>20.997332</td>\n",
              "      <td>18.765572</td>\n",
              "      <td>16.942690</td>\n",
              "      <td>18.239008</td>\n",
              "      <td>19.967186</td>\n",
              "      <td>19.310268</td>\n",
              "      <td>19.043873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.350200</td>\n",
              "      <td>17.084431</td>\n",
              "      <td>14.866444</td>\n",
              "      <td>13.656182</td>\n",
              "      <td>15.348877</td>\n",
              "      <td>15.714656</td>\n",
              "      <td>15.494652</td>\n",
              "      <td>13.317635</td>\n",
              "      <td>14.997259</td>\n",
              "      <td>15.005263</td>\n",
              "      <td>16.564766</td>\n",
              "      <td>15.038461</td>\n",
              "      <td>15.302752</td>\n",
              "      <td>14.521314</td>\n",
              "      <td>14.385362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.616489</td>\n",
              "      <td>16.891802</td>\n",
              "      <td>15.924697</td>\n",
              "      <td>16.594709</td>\n",
              "      <td>17.226166</td>\n",
              "      <td>18.735447</td>\n",
              "      <td>15.646786</td>\n",
              "      <td>14.803500</td>\n",
              "      <td>17.198198</td>\n",
              "      <td>16.404284</td>\n",
              "      <td>15.011387</td>\n",
              "      <td>14.078819</td>\n",
              "      <td>16.439119</td>\n",
              "      <td>17.136992</td>\n",
              "      <td>15.658569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>14.0</td>\n",
              "      <td>16.531332</td>\n",
              "      <td>14.787730</td>\n",
              "      <td>15.255507</td>\n",
              "      <td>15.952616</td>\n",
              "      <td>16.019878</td>\n",
              "      <td>16.901943</td>\n",
              "      <td>13.890097</td>\n",
              "      <td>14.856192</td>\n",
              "      <td>14.477530</td>\n",
              "      <td>15.395231</td>\n",
              "      <td>15.622538</td>\n",
              "      <td>14.618567</td>\n",
              "      <td>14.059141</td>\n",
              "      <td>13.168842</td>\n",
              "      <td>14.822374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.996792</td>\n",
              "      <td>16.581591</td>\n",
              "      <td>15.352725</td>\n",
              "      <td>15.377995</td>\n",
              "      <td>16.114653</td>\n",
              "      <td>14.762013</td>\n",
              "      <td>14.857765</td>\n",
              "      <td>14.443054</td>\n",
              "      <td>15.017770</td>\n",
              "      <td>15.307230</td>\n",
              "      <td>14.881834</td>\n",
              "      <td>17.105925</td>\n",
              "      <td>14.055600</td>\n",
              "      <td>15.160906</td>\n",
              "      <td>16.149475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.905897</td>\n",
              "      <td>15.523425</td>\n",
              "      <td>15.102746</td>\n",
              "      <td>14.564447</td>\n",
              "      <td>15.405523</td>\n",
              "      <td>15.423151</td>\n",
              "      <td>14.630257</td>\n",
              "      <td>14.869218</td>\n",
              "      <td>16.035784</td>\n",
              "      <td>14.358626</td>\n",
              "      <td>16.949654</td>\n",
              "      <td>14.629050</td>\n",
              "      <td>15.377235</td>\n",
              "      <td>15.001814</td>\n",
              "      <td>16.227140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>12.0</td>\n",
              "      <td>13.083528</td>\n",
              "      <td>12.930374</td>\n",
              "      <td>10.683826</td>\n",
              "      <td>13.097983</td>\n",
              "      <td>11.153977</td>\n",
              "      <td>12.932888</td>\n",
              "      <td>14.319544</td>\n",
              "      <td>12.859988</td>\n",
              "      <td>11.596682</td>\n",
              "      <td>12.017550</td>\n",
              "      <td>13.443398</td>\n",
              "      <td>13.681669</td>\n",
              "      <td>10.106813</td>\n",
              "      <td>12.300837</td>\n",
              "      <td>12.854052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>20.0</td>\n",
              "      <td>18.558292</td>\n",
              "      <td>20.788132</td>\n",
              "      <td>20.482178</td>\n",
              "      <td>20.995062</td>\n",
              "      <td>20.450191</td>\n",
              "      <td>19.307186</td>\n",
              "      <td>19.159975</td>\n",
              "      <td>21.121853</td>\n",
              "      <td>21.137178</td>\n",
              "      <td>20.061499</td>\n",
              "      <td>20.798206</td>\n",
              "      <td>22.153612</td>\n",
              "      <td>20.244562</td>\n",
              "      <td>22.240379</td>\n",
              "      <td>22.191500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.751754</td>\n",
              "      <td>13.908798</td>\n",
              "      <td>13.734552</td>\n",
              "      <td>14.342483</td>\n",
              "      <td>13.000732</td>\n",
              "      <td>16.054569</td>\n",
              "      <td>13.658901</td>\n",
              "      <td>13.343998</td>\n",
              "      <td>13.660720</td>\n",
              "      <td>13.940194</td>\n",
              "      <td>17.450058</td>\n",
              "      <td>15.473287</td>\n",
              "      <td>12.487250</td>\n",
              "      <td>13.989946</td>\n",
              "      <td>14.919375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>14.0</td>\n",
              "      <td>15.031129</td>\n",
              "      <td>15.114771</td>\n",
              "      <td>13.772435</td>\n",
              "      <td>15.060906</td>\n",
              "      <td>14.349575</td>\n",
              "      <td>15.037434</td>\n",
              "      <td>13.762219</td>\n",
              "      <td>13.383395</td>\n",
              "      <td>14.028338</td>\n",
              "      <td>13.796208</td>\n",
              "      <td>17.247681</td>\n",
              "      <td>15.165790</td>\n",
              "      <td>14.843243</td>\n",
              "      <td>14.411689</td>\n",
              "      <td>14.362759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.196518</td>\n",
              "      <td>18.886425</td>\n",
              "      <td>18.497211</td>\n",
              "      <td>17.204891</td>\n",
              "      <td>18.881599</td>\n",
              "      <td>19.088055</td>\n",
              "      <td>18.571501</td>\n",
              "      <td>19.181599</td>\n",
              "      <td>19.331451</td>\n",
              "      <td>18.821510</td>\n",
              "      <td>19.830971</td>\n",
              "      <td>18.885170</td>\n",
              "      <td>17.916826</td>\n",
              "      <td>18.039738</td>\n",
              "      <td>20.542759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>13.0</td>\n",
              "      <td>14.259568</td>\n",
              "      <td>15.745381</td>\n",
              "      <td>13.767362</td>\n",
              "      <td>15.342570</td>\n",
              "      <td>15.208616</td>\n",
              "      <td>14.349669</td>\n",
              "      <td>15.368238</td>\n",
              "      <td>15.072402</td>\n",
              "      <td>14.509226</td>\n",
              "      <td>14.101257</td>\n",
              "      <td>14.654187</td>\n",
              "      <td>15.459135</td>\n",
              "      <td>13.004227</td>\n",
              "      <td>13.757391</td>\n",
              "      <td>14.135849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>16.0</td>\n",
              "      <td>16.351345</td>\n",
              "      <td>16.333231</td>\n",
              "      <td>17.286863</td>\n",
              "      <td>17.012785</td>\n",
              "      <td>17.121038</td>\n",
              "      <td>15.839384</td>\n",
              "      <td>15.530210</td>\n",
              "      <td>15.563415</td>\n",
              "      <td>16.138378</td>\n",
              "      <td>16.471975</td>\n",
              "      <td>16.844343</td>\n",
              "      <td>15.939867</td>\n",
              "      <td>16.424520</td>\n",
              "      <td>13.699211</td>\n",
              "      <td>15.406459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>16.0</td>\n",
              "      <td>14.850819</td>\n",
              "      <td>16.068043</td>\n",
              "      <td>15.173154</td>\n",
              "      <td>16.000347</td>\n",
              "      <td>15.291598</td>\n",
              "      <td>15.810497</td>\n",
              "      <td>15.943932</td>\n",
              "      <td>15.801341</td>\n",
              "      <td>16.602005</td>\n",
              "      <td>16.350616</td>\n",
              "      <td>16.105104</td>\n",
              "      <td>16.122105</td>\n",
              "      <td>14.975595</td>\n",
              "      <td>14.693605</td>\n",
              "      <td>15.715784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.143242</td>\n",
              "      <td>20.630028</td>\n",
              "      <td>20.109547</td>\n",
              "      <td>17.628563</td>\n",
              "      <td>20.020170</td>\n",
              "      <td>20.493275</td>\n",
              "      <td>18.158710</td>\n",
              "      <td>21.165142</td>\n",
              "      <td>20.488102</td>\n",
              "      <td>17.815222</td>\n",
              "      <td>20.069004</td>\n",
              "      <td>19.374189</td>\n",
              "      <td>19.722439</td>\n",
              "      <td>18.740028</td>\n",
              "      <td>21.174377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.617836</td>\n",
              "      <td>19.047892</td>\n",
              "      <td>18.044704</td>\n",
              "      <td>18.261036</td>\n",
              "      <td>17.299269</td>\n",
              "      <td>19.084530</td>\n",
              "      <td>18.789234</td>\n",
              "      <td>18.050550</td>\n",
              "      <td>18.005165</td>\n",
              "      <td>18.674988</td>\n",
              "      <td>19.090401</td>\n",
              "      <td>18.107943</td>\n",
              "      <td>17.646259</td>\n",
              "      <td>20.054504</td>\n",
              "      <td>20.919004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>16.0</td>\n",
              "      <td>14.807504</td>\n",
              "      <td>17.258949</td>\n",
              "      <td>15.959424</td>\n",
              "      <td>15.707125</td>\n",
              "      <td>16.713694</td>\n",
              "      <td>15.676069</td>\n",
              "      <td>15.428617</td>\n",
              "      <td>15.740199</td>\n",
              "      <td>16.588556</td>\n",
              "      <td>17.837511</td>\n",
              "      <td>15.969807</td>\n",
              "      <td>14.557480</td>\n",
              "      <td>15.579144</td>\n",
              "      <td>14.746252</td>\n",
              "      <td>15.090883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>20.0</td>\n",
              "      <td>16.382084</td>\n",
              "      <td>19.782688</td>\n",
              "      <td>15.386102</td>\n",
              "      <td>19.156275</td>\n",
              "      <td>17.966675</td>\n",
              "      <td>19.761395</td>\n",
              "      <td>19.550489</td>\n",
              "      <td>17.338873</td>\n",
              "      <td>21.197283</td>\n",
              "      <td>19.616318</td>\n",
              "      <td>18.485733</td>\n",
              "      <td>20.257195</td>\n",
              "      <td>15.957960</td>\n",
              "      <td>20.988434</td>\n",
              "      <td>18.879469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.908904</td>\n",
              "      <td>15.846946</td>\n",
              "      <td>13.861213</td>\n",
              "      <td>14.999058</td>\n",
              "      <td>14.759367</td>\n",
              "      <td>15.704677</td>\n",
              "      <td>14.686828</td>\n",
              "      <td>15.323499</td>\n",
              "      <td>15.339183</td>\n",
              "      <td>14.058950</td>\n",
              "      <td>14.598878</td>\n",
              "      <td>15.580369</td>\n",
              "      <td>15.220892</td>\n",
              "      <td>13.057706</td>\n",
              "      <td>13.260307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>14.0</td>\n",
              "      <td>13.823689</td>\n",
              "      <td>15.209287</td>\n",
              "      <td>13.986428</td>\n",
              "      <td>13.858667</td>\n",
              "      <td>13.026795</td>\n",
              "      <td>14.078362</td>\n",
              "      <td>13.715359</td>\n",
              "      <td>12.626691</td>\n",
              "      <td>14.807911</td>\n",
              "      <td>13.411647</td>\n",
              "      <td>14.341468</td>\n",
              "      <td>13.640844</td>\n",
              "      <td>13.835194</td>\n",
              "      <td>12.437253</td>\n",
              "      <td>14.530807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>19.0</td>\n",
              "      <td>20.060217</td>\n",
              "      <td>19.210367</td>\n",
              "      <td>18.404139</td>\n",
              "      <td>17.225912</td>\n",
              "      <td>20.680990</td>\n",
              "      <td>19.513662</td>\n",
              "      <td>18.110865</td>\n",
              "      <td>20.363853</td>\n",
              "      <td>17.470575</td>\n",
              "      <td>19.038450</td>\n",
              "      <td>18.953119</td>\n",
              "      <td>18.858545</td>\n",
              "      <td>19.367758</td>\n",
              "      <td>17.209064</td>\n",
              "      <td>20.537483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>13.0</td>\n",
              "      <td>13.181026</td>\n",
              "      <td>13.512937</td>\n",
              "      <td>12.350112</td>\n",
              "      <td>13.174659</td>\n",
              "      <td>13.695223</td>\n",
              "      <td>13.149683</td>\n",
              "      <td>13.163710</td>\n",
              "      <td>13.181358</td>\n",
              "      <td>12.802911</td>\n",
              "      <td>12.039157</td>\n",
              "      <td>13.846973</td>\n",
              "      <td>14.084284</td>\n",
              "      <td>15.516514</td>\n",
              "      <td>13.435865</td>\n",
              "      <td>12.353072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.028329</td>\n",
              "      <td>17.992580</td>\n",
              "      <td>16.939325</td>\n",
              "      <td>16.271935</td>\n",
              "      <td>15.652617</td>\n",
              "      <td>16.493561</td>\n",
              "      <td>15.355536</td>\n",
              "      <td>15.432858</td>\n",
              "      <td>17.649641</td>\n",
              "      <td>14.300237</td>\n",
              "      <td>15.746277</td>\n",
              "      <td>15.513109</td>\n",
              "      <td>14.367241</td>\n",
              "      <td>16.758753</td>\n",
              "      <td>14.032661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.086997</td>\n",
              "      <td>15.615657</td>\n",
              "      <td>14.360969</td>\n",
              "      <td>14.364262</td>\n",
              "      <td>17.624975</td>\n",
              "      <td>15.236846</td>\n",
              "      <td>15.405605</td>\n",
              "      <td>14.379397</td>\n",
              "      <td>15.444663</td>\n",
              "      <td>15.661239</td>\n",
              "      <td>15.764544</td>\n",
              "      <td>15.198858</td>\n",
              "      <td>13.075729</td>\n",
              "      <td>15.930654</td>\n",
              "      <td>16.185522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>21.0</td>\n",
              "      <td>18.638048</td>\n",
              "      <td>21.176109</td>\n",
              "      <td>18.807835</td>\n",
              "      <td>17.325092</td>\n",
              "      <td>20.200136</td>\n",
              "      <td>20.443295</td>\n",
              "      <td>20.925205</td>\n",
              "      <td>20.987070</td>\n",
              "      <td>18.583937</td>\n",
              "      <td>19.554955</td>\n",
              "      <td>19.511980</td>\n",
              "      <td>20.770077</td>\n",
              "      <td>18.787273</td>\n",
              "      <td>17.955248</td>\n",
              "      <td>18.969299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>20.0</td>\n",
              "      <td>18.864462</td>\n",
              "      <td>19.417206</td>\n",
              "      <td>18.168188</td>\n",
              "      <td>19.343380</td>\n",
              "      <td>18.921673</td>\n",
              "      <td>20.606117</td>\n",
              "      <td>19.179138</td>\n",
              "      <td>19.219522</td>\n",
              "      <td>19.412638</td>\n",
              "      <td>19.117531</td>\n",
              "      <td>19.714758</td>\n",
              "      <td>18.680290</td>\n",
              "      <td>19.268744</td>\n",
              "      <td>18.417000</td>\n",
              "      <td>21.310812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.098911</td>\n",
              "      <td>18.633390</td>\n",
              "      <td>18.321386</td>\n",
              "      <td>18.948175</td>\n",
              "      <td>17.692709</td>\n",
              "      <td>18.486105</td>\n",
              "      <td>17.939966</td>\n",
              "      <td>17.923208</td>\n",
              "      <td>17.134146</td>\n",
              "      <td>17.902594</td>\n",
              "      <td>17.917068</td>\n",
              "      <td>18.820704</td>\n",
              "      <td>16.452084</td>\n",
              "      <td>17.354790</td>\n",
              "      <td>18.846609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.447002</td>\n",
              "      <td>17.879698</td>\n",
              "      <td>16.542421</td>\n",
              "      <td>18.590281</td>\n",
              "      <td>18.157232</td>\n",
              "      <td>17.086639</td>\n",
              "      <td>16.629871</td>\n",
              "      <td>17.023520</td>\n",
              "      <td>17.988283</td>\n",
              "      <td>17.052029</td>\n",
              "      <td>15.957426</td>\n",
              "      <td>16.581987</td>\n",
              "      <td>16.104824</td>\n",
              "      <td>16.775515</td>\n",
              "      <td>16.850704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.360436</td>\n",
              "      <td>16.078901</td>\n",
              "      <td>15.590281</td>\n",
              "      <td>15.930055</td>\n",
              "      <td>15.799952</td>\n",
              "      <td>16.262114</td>\n",
              "      <td>14.508701</td>\n",
              "      <td>15.887969</td>\n",
              "      <td>17.249607</td>\n",
              "      <td>18.341030</td>\n",
              "      <td>15.917773</td>\n",
              "      <td>15.165892</td>\n",
              "      <td>15.608491</td>\n",
              "      <td>15.747896</td>\n",
              "      <td>16.751755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>21.0</td>\n",
              "      <td>21.297897</td>\n",
              "      <td>21.585949</td>\n",
              "      <td>20.642149</td>\n",
              "      <td>19.966974</td>\n",
              "      <td>20.772736</td>\n",
              "      <td>20.571589</td>\n",
              "      <td>21.691191</td>\n",
              "      <td>23.027718</td>\n",
              "      <td>22.271929</td>\n",
              "      <td>21.044664</td>\n",
              "      <td>18.358082</td>\n",
              "      <td>21.944590</td>\n",
              "      <td>20.397308</td>\n",
              "      <td>20.184515</td>\n",
              "      <td>20.762308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>14.0</td>\n",
              "      <td>13.352776</td>\n",
              "      <td>14.594517</td>\n",
              "      <td>13.410445</td>\n",
              "      <td>15.585615</td>\n",
              "      <td>13.877559</td>\n",
              "      <td>15.040571</td>\n",
              "      <td>14.198187</td>\n",
              "      <td>13.693387</td>\n",
              "      <td>14.259379</td>\n",
              "      <td>14.785823</td>\n",
              "      <td>15.118582</td>\n",
              "      <td>13.964343</td>\n",
              "      <td>12.636228</td>\n",
              "      <td>12.440477</td>\n",
              "      <td>15.006018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.025788</td>\n",
              "      <td>15.484532</td>\n",
              "      <td>13.311967</td>\n",
              "      <td>14.626474</td>\n",
              "      <td>12.615044</td>\n",
              "      <td>14.965826</td>\n",
              "      <td>14.932250</td>\n",
              "      <td>15.921086</td>\n",
              "      <td>15.549075</td>\n",
              "      <td>13.340388</td>\n",
              "      <td>14.263691</td>\n",
              "      <td>15.183292</td>\n",
              "      <td>15.089794</td>\n",
              "      <td>16.093937</td>\n",
              "      <td>13.710068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.878788</td>\n",
              "      <td>20.901239</td>\n",
              "      <td>19.074677</td>\n",
              "      <td>19.885807</td>\n",
              "      <td>20.103422</td>\n",
              "      <td>19.657938</td>\n",
              "      <td>20.434942</td>\n",
              "      <td>21.051067</td>\n",
              "      <td>20.817110</td>\n",
              "      <td>20.327332</td>\n",
              "      <td>19.927277</td>\n",
              "      <td>20.737022</td>\n",
              "      <td>18.405361</td>\n",
              "      <td>21.028103</td>\n",
              "      <td>21.168932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.984420</td>\n",
              "      <td>18.255655</td>\n",
              "      <td>14.924376</td>\n",
              "      <td>16.602781</td>\n",
              "      <td>16.294741</td>\n",
              "      <td>17.894037</td>\n",
              "      <td>16.809717</td>\n",
              "      <td>18.089190</td>\n",
              "      <td>18.573507</td>\n",
              "      <td>17.224361</td>\n",
              "      <td>16.964176</td>\n",
              "      <td>17.289495</td>\n",
              "      <td>17.546143</td>\n",
              "      <td>17.342840</td>\n",
              "      <td>17.330698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.780916</td>\n",
              "      <td>20.032724</td>\n",
              "      <td>19.728315</td>\n",
              "      <td>19.836271</td>\n",
              "      <td>18.448744</td>\n",
              "      <td>18.661188</td>\n",
              "      <td>19.892614</td>\n",
              "      <td>18.039982</td>\n",
              "      <td>18.805685</td>\n",
              "      <td>19.094707</td>\n",
              "      <td>18.858198</td>\n",
              "      <td>19.781044</td>\n",
              "      <td>19.780323</td>\n",
              "      <td>18.666178</td>\n",
              "      <td>20.012707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.898147</td>\n",
              "      <td>16.779900</td>\n",
              "      <td>16.388227</td>\n",
              "      <td>16.195053</td>\n",
              "      <td>15.727386</td>\n",
              "      <td>17.617300</td>\n",
              "      <td>17.375399</td>\n",
              "      <td>16.898617</td>\n",
              "      <td>16.807962</td>\n",
              "      <td>15.374018</td>\n",
              "      <td>17.255030</td>\n",
              "      <td>17.445251</td>\n",
              "      <td>17.876595</td>\n",
              "      <td>16.325613</td>\n",
              "      <td>17.846109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.396921</td>\n",
              "      <td>16.101025</td>\n",
              "      <td>16.518602</td>\n",
              "      <td>15.145787</td>\n",
              "      <td>18.067350</td>\n",
              "      <td>14.733345</td>\n",
              "      <td>17.480612</td>\n",
              "      <td>15.910026</td>\n",
              "      <td>16.836212</td>\n",
              "      <td>16.575268</td>\n",
              "      <td>16.688915</td>\n",
              "      <td>18.075123</td>\n",
              "      <td>16.842424</td>\n",
              "      <td>16.197664</td>\n",
              "      <td>15.514608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>21.0</td>\n",
              "      <td>20.415398</td>\n",
              "      <td>20.575958</td>\n",
              "      <td>17.751497</td>\n",
              "      <td>18.864243</td>\n",
              "      <td>21.180555</td>\n",
              "      <td>20.319645</td>\n",
              "      <td>19.803225</td>\n",
              "      <td>16.190231</td>\n",
              "      <td>21.687798</td>\n",
              "      <td>22.131987</td>\n",
              "      <td>20.199621</td>\n",
              "      <td>22.200235</td>\n",
              "      <td>17.248133</td>\n",
              "      <td>19.620770</td>\n",
              "      <td>19.845383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>14.0</td>\n",
              "      <td>14.348152</td>\n",
              "      <td>15.434422</td>\n",
              "      <td>13.682901</td>\n",
              "      <td>13.657386</td>\n",
              "      <td>14.457260</td>\n",
              "      <td>14.919415</td>\n",
              "      <td>15.733963</td>\n",
              "      <td>13.988755</td>\n",
              "      <td>14.334916</td>\n",
              "      <td>14.109294</td>\n",
              "      <td>16.204035</td>\n",
              "      <td>15.792925</td>\n",
              "      <td>13.716809</td>\n",
              "      <td>14.404655</td>\n",
              "      <td>14.814404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>15.0</td>\n",
              "      <td>17.765886</td>\n",
              "      <td>16.267172</td>\n",
              "      <td>15.677871</td>\n",
              "      <td>16.346952</td>\n",
              "      <td>15.458024</td>\n",
              "      <td>17.299557</td>\n",
              "      <td>15.068815</td>\n",
              "      <td>16.353140</td>\n",
              "      <td>16.310656</td>\n",
              "      <td>14.721517</td>\n",
              "      <td>17.103739</td>\n",
              "      <td>16.074038</td>\n",
              "      <td>14.289364</td>\n",
              "      <td>15.554425</td>\n",
              "      <td>16.222134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>12.0</td>\n",
              "      <td>15.194238</td>\n",
              "      <td>13.312358</td>\n",
              "      <td>11.530807</td>\n",
              "      <td>13.821135</td>\n",
              "      <td>12.009678</td>\n",
              "      <td>14.329511</td>\n",
              "      <td>13.002660</td>\n",
              "      <td>12.132273</td>\n",
              "      <td>12.887012</td>\n",
              "      <td>12.997107</td>\n",
              "      <td>13.749556</td>\n",
              "      <td>14.013716</td>\n",
              "      <td>10.372695</td>\n",
              "      <td>11.927193</td>\n",
              "      <td>10.985272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>18.0</td>\n",
              "      <td>16.752169</td>\n",
              "      <td>18.511625</td>\n",
              "      <td>17.439846</td>\n",
              "      <td>17.924599</td>\n",
              "      <td>16.363886</td>\n",
              "      <td>19.031120</td>\n",
              "      <td>17.739796</td>\n",
              "      <td>18.060125</td>\n",
              "      <td>18.148172</td>\n",
              "      <td>16.414146</td>\n",
              "      <td>17.072765</td>\n",
              "      <td>16.466665</td>\n",
              "      <td>17.480803</td>\n",
              "      <td>17.230103</td>\n",
              "      <td>16.883427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.799852</td>\n",
              "      <td>15.311362</td>\n",
              "      <td>13.884873</td>\n",
              "      <td>13.584968</td>\n",
              "      <td>14.753664</td>\n",
              "      <td>15.764261</td>\n",
              "      <td>14.384544</td>\n",
              "      <td>13.940983</td>\n",
              "      <td>14.219676</td>\n",
              "      <td>14.291198</td>\n",
              "      <td>13.916587</td>\n",
              "      <td>11.636734</td>\n",
              "      <td>13.665180</td>\n",
              "      <td>14.407511</td>\n",
              "      <td>12.549096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.475554</td>\n",
              "      <td>16.917189</td>\n",
              "      <td>17.140219</td>\n",
              "      <td>16.556530</td>\n",
              "      <td>17.689756</td>\n",
              "      <td>16.671846</td>\n",
              "      <td>17.413422</td>\n",
              "      <td>17.718996</td>\n",
              "      <td>16.108139</td>\n",
              "      <td>16.745159</td>\n",
              "      <td>17.470987</td>\n",
              "      <td>17.945944</td>\n",
              "      <td>16.336418</td>\n",
              "      <td>17.169342</td>\n",
              "      <td>17.505295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.098559</td>\n",
              "      <td>15.376578</td>\n",
              "      <td>14.634631</td>\n",
              "      <td>14.272134</td>\n",
              "      <td>16.056553</td>\n",
              "      <td>16.051395</td>\n",
              "      <td>14.597668</td>\n",
              "      <td>14.442822</td>\n",
              "      <td>15.375036</td>\n",
              "      <td>15.630411</td>\n",
              "      <td>15.204122</td>\n",
              "      <td>14.950064</td>\n",
              "      <td>15.004929</td>\n",
              "      <td>15.673388</td>\n",
              "      <td>15.213754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>18.0</td>\n",
              "      <td>18.291595</td>\n",
              "      <td>18.744356</td>\n",
              "      <td>18.194330</td>\n",
              "      <td>18.585718</td>\n",
              "      <td>18.458769</td>\n",
              "      <td>18.781454</td>\n",
              "      <td>18.536146</td>\n",
              "      <td>18.058222</td>\n",
              "      <td>19.258188</td>\n",
              "      <td>17.810726</td>\n",
              "      <td>19.284344</td>\n",
              "      <td>19.172670</td>\n",
              "      <td>18.856470</td>\n",
              "      <td>19.541607</td>\n",
              "      <td>19.965685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>20.0</td>\n",
              "      <td>20.241253</td>\n",
              "      <td>20.047888</td>\n",
              "      <td>19.125957</td>\n",
              "      <td>18.768213</td>\n",
              "      <td>18.106382</td>\n",
              "      <td>19.471380</td>\n",
              "      <td>16.831236</td>\n",
              "      <td>18.785355</td>\n",
              "      <td>22.478306</td>\n",
              "      <td>19.647360</td>\n",
              "      <td>19.243868</td>\n",
              "      <td>21.201096</td>\n",
              "      <td>20.084045</td>\n",
              "      <td>20.110874</td>\n",
              "      <td>18.417637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>14.0</td>\n",
              "      <td>13.412836</td>\n",
              "      <td>13.860312</td>\n",
              "      <td>14.856524</td>\n",
              "      <td>13.390033</td>\n",
              "      <td>13.885411</td>\n",
              "      <td>13.772001</td>\n",
              "      <td>13.456437</td>\n",
              "      <td>15.024978</td>\n",
              "      <td>14.645157</td>\n",
              "      <td>14.012307</td>\n",
              "      <td>12.747528</td>\n",
              "      <td>13.420163</td>\n",
              "      <td>17.808979</td>\n",
              "      <td>11.508353</td>\n",
              "      <td>15.156619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.202406</td>\n",
              "      <td>14.459836</td>\n",
              "      <td>15.373020</td>\n",
              "      <td>14.631548</td>\n",
              "      <td>15.403196</td>\n",
              "      <td>12.901976</td>\n",
              "      <td>13.784710</td>\n",
              "      <td>13.944419</td>\n",
              "      <td>15.591275</td>\n",
              "      <td>16.298750</td>\n",
              "      <td>13.906901</td>\n",
              "      <td>16.268400</td>\n",
              "      <td>14.847793</td>\n",
              "      <td>15.116726</td>\n",
              "      <td>14.199623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.882224</td>\n",
              "      <td>15.612880</td>\n",
              "      <td>14.610225</td>\n",
              "      <td>15.140519</td>\n",
              "      <td>15.736253</td>\n",
              "      <td>16.173225</td>\n",
              "      <td>14.506193</td>\n",
              "      <td>14.763242</td>\n",
              "      <td>14.773370</td>\n",
              "      <td>14.851140</td>\n",
              "      <td>13.831740</td>\n",
              "      <td>13.435549</td>\n",
              "      <td>13.726526</td>\n",
              "      <td>14.341100</td>\n",
              "      <td>14.578015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>16.0</td>\n",
              "      <td>14.869064</td>\n",
              "      <td>15.536246</td>\n",
              "      <td>14.549211</td>\n",
              "      <td>13.131105</td>\n",
              "      <td>16.442209</td>\n",
              "      <td>15.145329</td>\n",
              "      <td>15.654291</td>\n",
              "      <td>15.889692</td>\n",
              "      <td>14.365667</td>\n",
              "      <td>15.988145</td>\n",
              "      <td>15.718677</td>\n",
              "      <td>16.556067</td>\n",
              "      <td>15.454973</td>\n",
              "      <td>14.453193</td>\n",
              "      <td>15.170197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.124471</td>\n",
              "      <td>16.087503</td>\n",
              "      <td>13.799226</td>\n",
              "      <td>15.121427</td>\n",
              "      <td>14.210649</td>\n",
              "      <td>14.264853</td>\n",
              "      <td>15.098917</td>\n",
              "      <td>14.927148</td>\n",
              "      <td>14.581848</td>\n",
              "      <td>14.101974</td>\n",
              "      <td>15.293839</td>\n",
              "      <td>15.494998</td>\n",
              "      <td>13.712332</td>\n",
              "      <td>15.096870</td>\n",
              "      <td>13.335627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.249518</td>\n",
              "      <td>15.971235</td>\n",
              "      <td>14.473754</td>\n",
              "      <td>15.020272</td>\n",
              "      <td>16.120718</td>\n",
              "      <td>15.480841</td>\n",
              "      <td>14.984310</td>\n",
              "      <td>14.353437</td>\n",
              "      <td>15.948468</td>\n",
              "      <td>14.791253</td>\n",
              "      <td>16.046762</td>\n",
              "      <td>14.730534</td>\n",
              "      <td>13.598773</td>\n",
              "      <td>15.830203</td>\n",
              "      <td>15.299519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>12.0</td>\n",
              "      <td>12.753238</td>\n",
              "      <td>12.835464</td>\n",
              "      <td>10.760091</td>\n",
              "      <td>12.127373</td>\n",
              "      <td>13.712824</td>\n",
              "      <td>11.667554</td>\n",
              "      <td>11.085661</td>\n",
              "      <td>10.026088</td>\n",
              "      <td>11.567861</td>\n",
              "      <td>14.906216</td>\n",
              "      <td>12.849040</td>\n",
              "      <td>11.975744</td>\n",
              "      <td>12.185539</td>\n",
              "      <td>10.795151</td>\n",
              "      <td>13.907397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.445794</td>\n",
              "      <td>15.192214</td>\n",
              "      <td>13.986918</td>\n",
              "      <td>13.264548</td>\n",
              "      <td>14.465387</td>\n",
              "      <td>13.554104</td>\n",
              "      <td>14.145494</td>\n",
              "      <td>12.829890</td>\n",
              "      <td>13.284381</td>\n",
              "      <td>13.393456</td>\n",
              "      <td>15.215747</td>\n",
              "      <td>15.309612</td>\n",
              "      <td>13.905093</td>\n",
              "      <td>13.376205</td>\n",
              "      <td>14.644567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>18.0</td>\n",
              "      <td>19.059650</td>\n",
              "      <td>17.987600</td>\n",
              "      <td>18.044706</td>\n",
              "      <td>16.933945</td>\n",
              "      <td>17.625566</td>\n",
              "      <td>17.995028</td>\n",
              "      <td>17.922344</td>\n",
              "      <td>18.686195</td>\n",
              "      <td>17.383821</td>\n",
              "      <td>18.625198</td>\n",
              "      <td>19.363024</td>\n",
              "      <td>17.671240</td>\n",
              "      <td>16.533216</td>\n",
              "      <td>16.293737</td>\n",
              "      <td>18.936785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>12.0</td>\n",
              "      <td>12.285661</td>\n",
              "      <td>12.982884</td>\n",
              "      <td>9.747102</td>\n",
              "      <td>12.254848</td>\n",
              "      <td>11.421160</td>\n",
              "      <td>12.836797</td>\n",
              "      <td>11.608201</td>\n",
              "      <td>12.469773</td>\n",
              "      <td>12.479422</td>\n",
              "      <td>15.111024</td>\n",
              "      <td>13.132896</td>\n",
              "      <td>13.931624</td>\n",
              "      <td>11.295387</td>\n",
              "      <td>12.473686</td>\n",
              "      <td>14.278653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.694237</td>\n",
              "      <td>19.089067</td>\n",
              "      <td>17.192537</td>\n",
              "      <td>16.906252</td>\n",
              "      <td>18.042591</td>\n",
              "      <td>17.358809</td>\n",
              "      <td>18.473795</td>\n",
              "      <td>19.054863</td>\n",
              "      <td>18.505905</td>\n",
              "      <td>17.109779</td>\n",
              "      <td>18.483496</td>\n",
              "      <td>17.604773</td>\n",
              "      <td>17.430204</td>\n",
              "      <td>17.774490</td>\n",
              "      <td>18.868977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.440679</td>\n",
              "      <td>14.767140</td>\n",
              "      <td>13.867667</td>\n",
              "      <td>15.243022</td>\n",
              "      <td>15.077799</td>\n",
              "      <td>14.327374</td>\n",
              "      <td>13.528830</td>\n",
              "      <td>14.093220</td>\n",
              "      <td>15.135263</td>\n",
              "      <td>14.947581</td>\n",
              "      <td>14.752124</td>\n",
              "      <td>16.524502</td>\n",
              "      <td>14.073951</td>\n",
              "      <td>15.263971</td>\n",
              "      <td>14.838928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>20.0</td>\n",
              "      <td>18.877007</td>\n",
              "      <td>21.089241</td>\n",
              "      <td>21.078741</td>\n",
              "      <td>20.634472</td>\n",
              "      <td>21.063847</td>\n",
              "      <td>20.852699</td>\n",
              "      <td>21.062891</td>\n",
              "      <td>19.971523</td>\n",
              "      <td>20.810244</td>\n",
              "      <td>18.099485</td>\n",
              "      <td>18.428854</td>\n",
              "      <td>19.635168</td>\n",
              "      <td>19.277618</td>\n",
              "      <td>20.698196</td>\n",
              "      <td>19.148418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>14.0</td>\n",
              "      <td>13.531987</td>\n",
              "      <td>15.184554</td>\n",
              "      <td>13.745651</td>\n",
              "      <td>13.876119</td>\n",
              "      <td>13.902111</td>\n",
              "      <td>14.301983</td>\n",
              "      <td>14.152697</td>\n",
              "      <td>15.046363</td>\n",
              "      <td>14.899579</td>\n",
              "      <td>13.064156</td>\n",
              "      <td>14.234957</td>\n",
              "      <td>11.748864</td>\n",
              "      <td>15.058817</td>\n",
              "      <td>13.095179</td>\n",
              "      <td>14.396820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>18.0</td>\n",
              "      <td>18.148500</td>\n",
              "      <td>21.264231</td>\n",
              "      <td>19.884722</td>\n",
              "      <td>19.032616</td>\n",
              "      <td>18.862614</td>\n",
              "      <td>19.788206</td>\n",
              "      <td>22.172640</td>\n",
              "      <td>21.414703</td>\n",
              "      <td>18.543257</td>\n",
              "      <td>18.638180</td>\n",
              "      <td>19.059113</td>\n",
              "      <td>20.760880</td>\n",
              "      <td>17.410618</td>\n",
              "      <td>18.952610</td>\n",
              "      <td>21.072376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>17.0</td>\n",
              "      <td>17.245813</td>\n",
              "      <td>17.335249</td>\n",
              "      <td>16.987471</td>\n",
              "      <td>16.680485</td>\n",
              "      <td>16.994375</td>\n",
              "      <td>17.735214</td>\n",
              "      <td>17.258974</td>\n",
              "      <td>16.933256</td>\n",
              "      <td>17.917749</td>\n",
              "      <td>16.084507</td>\n",
              "      <td>16.107927</td>\n",
              "      <td>16.948154</td>\n",
              "      <td>17.477104</td>\n",
              "      <td>16.788200</td>\n",
              "      <td>17.111189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>23.0</td>\n",
              "      <td>22.981985</td>\n",
              "      <td>23.232721</td>\n",
              "      <td>22.106230</td>\n",
              "      <td>21.159370</td>\n",
              "      <td>23.601875</td>\n",
              "      <td>23.798874</td>\n",
              "      <td>23.401407</td>\n",
              "      <td>22.487732</td>\n",
              "      <td>21.829874</td>\n",
              "      <td>23.265169</td>\n",
              "      <td>22.569849</td>\n",
              "      <td>22.914104</td>\n",
              "      <td>23.613026</td>\n",
              "      <td>23.739435</td>\n",
              "      <td>25.685558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>18.0</td>\n",
              "      <td>18.420778</td>\n",
              "      <td>17.791380</td>\n",
              "      <td>18.692806</td>\n",
              "      <td>18.077353</td>\n",
              "      <td>17.572319</td>\n",
              "      <td>18.695293</td>\n",
              "      <td>18.947659</td>\n",
              "      <td>21.891418</td>\n",
              "      <td>16.626087</td>\n",
              "      <td>18.796150</td>\n",
              "      <td>18.427479</td>\n",
              "      <td>18.958448</td>\n",
              "      <td>17.340904</td>\n",
              "      <td>17.954952</td>\n",
              "      <td>18.826841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.622041</td>\n",
              "      <td>17.187464</td>\n",
              "      <td>15.563935</td>\n",
              "      <td>16.846245</td>\n",
              "      <td>14.765182</td>\n",
              "      <td>17.672035</td>\n",
              "      <td>16.536583</td>\n",
              "      <td>15.204309</td>\n",
              "      <td>17.085436</td>\n",
              "      <td>15.061109</td>\n",
              "      <td>16.305386</td>\n",
              "      <td>17.156013</td>\n",
              "      <td>15.558686</td>\n",
              "      <td>17.647760</td>\n",
              "      <td>15.456026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.741795</td>\n",
              "      <td>17.009197</td>\n",
              "      <td>14.956962</td>\n",
              "      <td>17.064259</td>\n",
              "      <td>16.840876</td>\n",
              "      <td>17.920731</td>\n",
              "      <td>16.818827</td>\n",
              "      <td>17.851246</td>\n",
              "      <td>16.888235</td>\n",
              "      <td>16.318539</td>\n",
              "      <td>17.719423</td>\n",
              "      <td>17.603010</td>\n",
              "      <td>16.690613</td>\n",
              "      <td>17.317497</td>\n",
              "      <td>17.905115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.908775</td>\n",
              "      <td>17.138792</td>\n",
              "      <td>16.153131</td>\n",
              "      <td>15.315886</td>\n",
              "      <td>16.197733</td>\n",
              "      <td>16.281738</td>\n",
              "      <td>17.172140</td>\n",
              "      <td>14.160371</td>\n",
              "      <td>16.425190</td>\n",
              "      <td>16.340120</td>\n",
              "      <td>16.123327</td>\n",
              "      <td>17.175520</td>\n",
              "      <td>13.561301</td>\n",
              "      <td>17.853672</td>\n",
              "      <td>16.835615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>23.0</td>\n",
              "      <td>21.565781</td>\n",
              "      <td>21.541309</td>\n",
              "      <td>22.878180</td>\n",
              "      <td>20.558395</td>\n",
              "      <td>22.468512</td>\n",
              "      <td>22.660545</td>\n",
              "      <td>19.723120</td>\n",
              "      <td>24.719385</td>\n",
              "      <td>24.156837</td>\n",
              "      <td>21.680519</td>\n",
              "      <td>20.864796</td>\n",
              "      <td>19.198473</td>\n",
              "      <td>22.159407</td>\n",
              "      <td>23.242443</td>\n",
              "      <td>19.984270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>20.0</td>\n",
              "      <td>18.193138</td>\n",
              "      <td>19.952080</td>\n",
              "      <td>19.559202</td>\n",
              "      <td>19.450968</td>\n",
              "      <td>19.928667</td>\n",
              "      <td>20.616032</td>\n",
              "      <td>21.104029</td>\n",
              "      <td>20.246727</td>\n",
              "      <td>20.441389</td>\n",
              "      <td>18.284939</td>\n",
              "      <td>18.713135</td>\n",
              "      <td>20.427660</td>\n",
              "      <td>20.052841</td>\n",
              "      <td>20.923647</td>\n",
              "      <td>18.573553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.771754</td>\n",
              "      <td>17.990484</td>\n",
              "      <td>15.276317</td>\n",
              "      <td>17.273472</td>\n",
              "      <td>16.205570</td>\n",
              "      <td>18.607220</td>\n",
              "      <td>16.402576</td>\n",
              "      <td>16.470734</td>\n",
              "      <td>16.072285</td>\n",
              "      <td>17.349964</td>\n",
              "      <td>16.769785</td>\n",
              "      <td>16.917526</td>\n",
              "      <td>15.062085</td>\n",
              "      <td>16.042135</td>\n",
              "      <td>13.798271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>23.0</td>\n",
              "      <td>21.753462</td>\n",
              "      <td>23.090391</td>\n",
              "      <td>22.704580</td>\n",
              "      <td>21.541676</td>\n",
              "      <td>24.161610</td>\n",
              "      <td>22.267651</td>\n",
              "      <td>23.226456</td>\n",
              "      <td>24.333838</td>\n",
              "      <td>23.796125</td>\n",
              "      <td>23.431351</td>\n",
              "      <td>22.641670</td>\n",
              "      <td>24.929186</td>\n",
              "      <td>23.732643</td>\n",
              "      <td>24.387081</td>\n",
              "      <td>22.957846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>14.0</td>\n",
              "      <td>14.747776</td>\n",
              "      <td>15.262019</td>\n",
              "      <td>13.843303</td>\n",
              "      <td>17.112844</td>\n",
              "      <td>14.330640</td>\n",
              "      <td>13.261964</td>\n",
              "      <td>13.529705</td>\n",
              "      <td>15.098651</td>\n",
              "      <td>15.034329</td>\n",
              "      <td>13.868924</td>\n",
              "      <td>15.060114</td>\n",
              "      <td>15.042121</td>\n",
              "      <td>13.169744</td>\n",
              "      <td>15.078540</td>\n",
              "      <td>13.699736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>17.0</td>\n",
              "      <td>18.115503</td>\n",
              "      <td>17.656206</td>\n",
              "      <td>17.728838</td>\n",
              "      <td>16.610216</td>\n",
              "      <td>15.988777</td>\n",
              "      <td>17.013485</td>\n",
              "      <td>17.241848</td>\n",
              "      <td>18.800375</td>\n",
              "      <td>17.830242</td>\n",
              "      <td>16.284565</td>\n",
              "      <td>17.834591</td>\n",
              "      <td>17.677069</td>\n",
              "      <td>16.385851</td>\n",
              "      <td>17.103159</td>\n",
              "      <td>17.427414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>14.0</td>\n",
              "      <td>14.342243</td>\n",
              "      <td>13.649552</td>\n",
              "      <td>15.713218</td>\n",
              "      <td>14.653938</td>\n",
              "      <td>14.625059</td>\n",
              "      <td>13.371374</td>\n",
              "      <td>14.265774</td>\n",
              "      <td>14.608607</td>\n",
              "      <td>14.222448</td>\n",
              "      <td>14.275929</td>\n",
              "      <td>13.585402</td>\n",
              "      <td>13.376956</td>\n",
              "      <td>15.920220</td>\n",
              "      <td>12.953441</td>\n",
              "      <td>14.310308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.598826</td>\n",
              "      <td>15.131998</td>\n",
              "      <td>15.590769</td>\n",
              "      <td>13.863337</td>\n",
              "      <td>14.178958</td>\n",
              "      <td>12.852365</td>\n",
              "      <td>12.520601</td>\n",
              "      <td>16.158298</td>\n",
              "      <td>13.204428</td>\n",
              "      <td>15.629359</td>\n",
              "      <td>12.930794</td>\n",
              "      <td>15.490872</td>\n",
              "      <td>15.236248</td>\n",
              "      <td>14.847120</td>\n",
              "      <td>14.352596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>18.0</td>\n",
              "      <td>15.722518</td>\n",
              "      <td>18.015635</td>\n",
              "      <td>15.965040</td>\n",
              "      <td>17.059731</td>\n",
              "      <td>16.794769</td>\n",
              "      <td>18.884508</td>\n",
              "      <td>16.890114</td>\n",
              "      <td>16.730564</td>\n",
              "      <td>17.353325</td>\n",
              "      <td>17.896029</td>\n",
              "      <td>16.458750</td>\n",
              "      <td>15.875525</td>\n",
              "      <td>15.997127</td>\n",
              "      <td>17.739792</td>\n",
              "      <td>17.829847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.184433</td>\n",
              "      <td>16.903803</td>\n",
              "      <td>14.983273</td>\n",
              "      <td>15.870845</td>\n",
              "      <td>16.622766</td>\n",
              "      <td>17.275204</td>\n",
              "      <td>16.146082</td>\n",
              "      <td>15.229789</td>\n",
              "      <td>16.782652</td>\n",
              "      <td>15.840024</td>\n",
              "      <td>15.702286</td>\n",
              "      <td>14.997393</td>\n",
              "      <td>16.017178</td>\n",
              "      <td>15.407314</td>\n",
              "      <td>15.862953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.545658</td>\n",
              "      <td>19.382166</td>\n",
              "      <td>17.910528</td>\n",
              "      <td>17.776884</td>\n",
              "      <td>19.394764</td>\n",
              "      <td>19.486778</td>\n",
              "      <td>18.617241</td>\n",
              "      <td>19.114006</td>\n",
              "      <td>17.588797</td>\n",
              "      <td>17.655384</td>\n",
              "      <td>17.449749</td>\n",
              "      <td>18.692944</td>\n",
              "      <td>18.447432</td>\n",
              "      <td>16.591209</td>\n",
              "      <td>18.000166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>18.0</td>\n",
              "      <td>18.502022</td>\n",
              "      <td>18.708866</td>\n",
              "      <td>16.648998</td>\n",
              "      <td>17.161612</td>\n",
              "      <td>18.250038</td>\n",
              "      <td>18.566711</td>\n",
              "      <td>16.109606</td>\n",
              "      <td>17.245033</td>\n",
              "      <td>18.036596</td>\n",
              "      <td>18.299026</td>\n",
              "      <td>17.582544</td>\n",
              "      <td>19.883001</td>\n",
              "      <td>15.633400</td>\n",
              "      <td>18.240993</td>\n",
              "      <td>17.648291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>12.0</td>\n",
              "      <td>13.285331</td>\n",
              "      <td>13.485398</td>\n",
              "      <td>11.954938</td>\n",
              "      <td>12.597207</td>\n",
              "      <td>12.949023</td>\n",
              "      <td>11.700385</td>\n",
              "      <td>13.286694</td>\n",
              "      <td>10.854680</td>\n",
              "      <td>13.055973</td>\n",
              "      <td>12.136477</td>\n",
              "      <td>12.508693</td>\n",
              "      <td>12.120371</td>\n",
              "      <td>11.610428</td>\n",
              "      <td>11.189287</td>\n",
              "      <td>11.865948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.198696</td>\n",
              "      <td>14.919567</td>\n",
              "      <td>11.242394</td>\n",
              "      <td>13.576180</td>\n",
              "      <td>14.835239</td>\n",
              "      <td>13.961906</td>\n",
              "      <td>14.392944</td>\n",
              "      <td>13.370080</td>\n",
              "      <td>15.026933</td>\n",
              "      <td>14.315271</td>\n",
              "      <td>14.850594</td>\n",
              "      <td>16.786749</td>\n",
              "      <td>13.014013</td>\n",
              "      <td>14.727719</td>\n",
              "      <td>14.517136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.245588</td>\n",
              "      <td>18.932291</td>\n",
              "      <td>16.931675</td>\n",
              "      <td>18.611197</td>\n",
              "      <td>19.819712</td>\n",
              "      <td>19.462013</td>\n",
              "      <td>19.287527</td>\n",
              "      <td>18.394209</td>\n",
              "      <td>20.953188</td>\n",
              "      <td>19.125069</td>\n",
              "      <td>19.147783</td>\n",
              "      <td>19.401369</td>\n",
              "      <td>17.621319</td>\n",
              "      <td>19.407167</td>\n",
              "      <td>18.616198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>16.0</td>\n",
              "      <td>14.737025</td>\n",
              "      <td>16.687754</td>\n",
              "      <td>15.102037</td>\n",
              "      <td>15.399813</td>\n",
              "      <td>15.920429</td>\n",
              "      <td>15.805330</td>\n",
              "      <td>15.858692</td>\n",
              "      <td>14.890807</td>\n",
              "      <td>16.850431</td>\n",
              "      <td>16.664917</td>\n",
              "      <td>15.055736</td>\n",
              "      <td>16.188902</td>\n",
              "      <td>14.508099</td>\n",
              "      <td>15.996594</td>\n",
              "      <td>16.542557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>12.0</td>\n",
              "      <td>12.756974</td>\n",
              "      <td>11.964728</td>\n",
              "      <td>12.773540</td>\n",
              "      <td>12.936831</td>\n",
              "      <td>14.169472</td>\n",
              "      <td>12.555597</td>\n",
              "      <td>11.420494</td>\n",
              "      <td>12.787109</td>\n",
              "      <td>11.825041</td>\n",
              "      <td>13.905738</td>\n",
              "      <td>12.970701</td>\n",
              "      <td>12.960770</td>\n",
              "      <td>11.243483</td>\n",
              "      <td>10.451629</td>\n",
              "      <td>13.630434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.713203</td>\n",
              "      <td>15.782406</td>\n",
              "      <td>16.828037</td>\n",
              "      <td>15.399594</td>\n",
              "      <td>15.364683</td>\n",
              "      <td>16.791475</td>\n",
              "      <td>14.627212</td>\n",
              "      <td>15.500779</td>\n",
              "      <td>14.602944</td>\n",
              "      <td>15.077767</td>\n",
              "      <td>15.274019</td>\n",
              "      <td>16.866724</td>\n",
              "      <td>14.863720</td>\n",
              "      <td>13.513193</td>\n",
              "      <td>16.023848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>10.0</td>\n",
              "      <td>11.717183</td>\n",
              "      <td>12.896450</td>\n",
              "      <td>9.001308</td>\n",
              "      <td>11.456105</td>\n",
              "      <td>10.730436</td>\n",
              "      <td>10.442030</td>\n",
              "      <td>13.060412</td>\n",
              "      <td>10.474332</td>\n",
              "      <td>11.758207</td>\n",
              "      <td>11.905570</td>\n",
              "      <td>10.338088</td>\n",
              "      <td>12.293561</td>\n",
              "      <td>10.219535</td>\n",
              "      <td>9.313963</td>\n",
              "      <td>10.212736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.722887</td>\n",
              "      <td>17.005983</td>\n",
              "      <td>16.347675</td>\n",
              "      <td>13.626843</td>\n",
              "      <td>16.679037</td>\n",
              "      <td>16.878830</td>\n",
              "      <td>15.923225</td>\n",
              "      <td>15.096434</td>\n",
              "      <td>13.964755</td>\n",
              "      <td>15.705817</td>\n",
              "      <td>16.977499</td>\n",
              "      <td>16.851252</td>\n",
              "      <td>16.524479</td>\n",
              "      <td>15.763920</td>\n",
              "      <td>16.795004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>25.0</td>\n",
              "      <td>23.579699</td>\n",
              "      <td>21.654158</td>\n",
              "      <td>23.531755</td>\n",
              "      <td>22.609715</td>\n",
              "      <td>25.512274</td>\n",
              "      <td>24.814274</td>\n",
              "      <td>19.400486</td>\n",
              "      <td>25.490408</td>\n",
              "      <td>25.403139</td>\n",
              "      <td>24.579231</td>\n",
              "      <td>23.537191</td>\n",
              "      <td>21.228575</td>\n",
              "      <td>24.530912</td>\n",
              "      <td>23.004763</td>\n",
              "      <td>24.625103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>18.0</td>\n",
              "      <td>19.941765</td>\n",
              "      <td>19.453693</td>\n",
              "      <td>16.728209</td>\n",
              "      <td>17.508717</td>\n",
              "      <td>18.619324</td>\n",
              "      <td>19.364697</td>\n",
              "      <td>17.110109</td>\n",
              "      <td>18.714136</td>\n",
              "      <td>18.465761</td>\n",
              "      <td>16.867836</td>\n",
              "      <td>20.568317</td>\n",
              "      <td>20.344107</td>\n",
              "      <td>18.395348</td>\n",
              "      <td>17.965170</td>\n",
              "      <td>19.410498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>17.0</td>\n",
              "      <td>17.853767</td>\n",
              "      <td>17.711685</td>\n",
              "      <td>17.491310</td>\n",
              "      <td>16.810427</td>\n",
              "      <td>17.227379</td>\n",
              "      <td>16.796055</td>\n",
              "      <td>17.772554</td>\n",
              "      <td>18.832869</td>\n",
              "      <td>17.723480</td>\n",
              "      <td>17.223770</td>\n",
              "      <td>16.833611</td>\n",
              "      <td>18.235550</td>\n",
              "      <td>15.843460</td>\n",
              "      <td>16.101885</td>\n",
              "      <td>18.476952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.772358</td>\n",
              "      <td>17.396465</td>\n",
              "      <td>17.650469</td>\n",
              "      <td>17.710220</td>\n",
              "      <td>18.568296</td>\n",
              "      <td>17.802296</td>\n",
              "      <td>16.039776</td>\n",
              "      <td>17.801203</td>\n",
              "      <td>16.655733</td>\n",
              "      <td>18.261978</td>\n",
              "      <td>18.080011</td>\n",
              "      <td>17.792839</td>\n",
              "      <td>17.336882</td>\n",
              "      <td>18.366009</td>\n",
              "      <td>19.409433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.123564</td>\n",
              "      <td>16.714666</td>\n",
              "      <td>15.984319</td>\n",
              "      <td>15.498320</td>\n",
              "      <td>15.894736</td>\n",
              "      <td>16.684517</td>\n",
              "      <td>15.717626</td>\n",
              "      <td>14.470608</td>\n",
              "      <td>16.271568</td>\n",
              "      <td>15.083026</td>\n",
              "      <td>15.795303</td>\n",
              "      <td>14.535261</td>\n",
              "      <td>15.200462</td>\n",
              "      <td>15.692032</td>\n",
              "      <td>13.432228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.146376</td>\n",
              "      <td>19.726698</td>\n",
              "      <td>18.476334</td>\n",
              "      <td>18.572832</td>\n",
              "      <td>19.165113</td>\n",
              "      <td>18.368469</td>\n",
              "      <td>18.775406</td>\n",
              "      <td>17.700766</td>\n",
              "      <td>20.356167</td>\n",
              "      <td>17.379782</td>\n",
              "      <td>18.859884</td>\n",
              "      <td>18.685171</td>\n",
              "      <td>19.232254</td>\n",
              "      <td>20.504978</td>\n",
              "      <td>19.423815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.679877</td>\n",
              "      <td>15.947832</td>\n",
              "      <td>14.104043</td>\n",
              "      <td>14.999710</td>\n",
              "      <td>15.038914</td>\n",
              "      <td>14.553293</td>\n",
              "      <td>15.235319</td>\n",
              "      <td>15.476308</td>\n",
              "      <td>15.347179</td>\n",
              "      <td>15.679622</td>\n",
              "      <td>16.923880</td>\n",
              "      <td>16.326845</td>\n",
              "      <td>14.271538</td>\n",
              "      <td>14.728286</td>\n",
              "      <td>16.083099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>21.0</td>\n",
              "      <td>20.682327</td>\n",
              "      <td>21.040085</td>\n",
              "      <td>21.996138</td>\n",
              "      <td>21.234104</td>\n",
              "      <td>22.955542</td>\n",
              "      <td>21.242752</td>\n",
              "      <td>21.090660</td>\n",
              "      <td>21.708891</td>\n",
              "      <td>22.004087</td>\n",
              "      <td>21.024809</td>\n",
              "      <td>21.688631</td>\n",
              "      <td>23.281986</td>\n",
              "      <td>20.959579</td>\n",
              "      <td>23.702215</td>\n",
              "      <td>24.094275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>14.0</td>\n",
              "      <td>14.081783</td>\n",
              "      <td>16.316137</td>\n",
              "      <td>14.236874</td>\n",
              "      <td>13.721917</td>\n",
              "      <td>14.920563</td>\n",
              "      <td>14.955633</td>\n",
              "      <td>15.031363</td>\n",
              "      <td>15.108020</td>\n",
              "      <td>14.847644</td>\n",
              "      <td>13.912766</td>\n",
              "      <td>15.070233</td>\n",
              "      <td>15.930956</td>\n",
              "      <td>15.954151</td>\n",
              "      <td>14.565214</td>\n",
              "      <td>15.641470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.283908</td>\n",
              "      <td>14.361036</td>\n",
              "      <td>15.049790</td>\n",
              "      <td>16.290926</td>\n",
              "      <td>15.813463</td>\n",
              "      <td>14.513535</td>\n",
              "      <td>14.549343</td>\n",
              "      <td>15.037898</td>\n",
              "      <td>15.637522</td>\n",
              "      <td>15.267860</td>\n",
              "      <td>15.854618</td>\n",
              "      <td>15.784132</td>\n",
              "      <td>14.822348</td>\n",
              "      <td>16.259447</td>\n",
              "      <td>15.425997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>15.0</td>\n",
              "      <td>15.169580</td>\n",
              "      <td>13.831416</td>\n",
              "      <td>12.465230</td>\n",
              "      <td>13.404502</td>\n",
              "      <td>13.580338</td>\n",
              "      <td>13.842150</td>\n",
              "      <td>12.816131</td>\n",
              "      <td>12.959144</td>\n",
              "      <td>13.540159</td>\n",
              "      <td>13.371299</td>\n",
              "      <td>13.919866</td>\n",
              "      <td>13.419589</td>\n",
              "      <td>12.096485</td>\n",
              "      <td>11.286215</td>\n",
              "      <td>13.916415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.601206</td>\n",
              "      <td>17.014296</td>\n",
              "      <td>16.862167</td>\n",
              "      <td>17.094461</td>\n",
              "      <td>17.411974</td>\n",
              "      <td>17.766369</td>\n",
              "      <td>18.516867</td>\n",
              "      <td>16.800285</td>\n",
              "      <td>17.217720</td>\n",
              "      <td>17.192835</td>\n",
              "      <td>18.174471</td>\n",
              "      <td>17.532591</td>\n",
              "      <td>16.748808</td>\n",
              "      <td>18.420353</td>\n",
              "      <td>17.947203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>17.0</td>\n",
              "      <td>16.173536</td>\n",
              "      <td>17.417570</td>\n",
              "      <td>16.055334</td>\n",
              "      <td>16.362955</td>\n",
              "      <td>17.636421</td>\n",
              "      <td>16.143549</td>\n",
              "      <td>15.132627</td>\n",
              "      <td>16.348934</td>\n",
              "      <td>15.554867</td>\n",
              "      <td>16.461121</td>\n",
              "      <td>17.144526</td>\n",
              "      <td>17.865824</td>\n",
              "      <td>16.162279</td>\n",
              "      <td>16.788891</td>\n",
              "      <td>16.594492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>21.0</td>\n",
              "      <td>20.799650</td>\n",
              "      <td>23.424494</td>\n",
              "      <td>21.556377</td>\n",
              "      <td>21.416651</td>\n",
              "      <td>22.642544</td>\n",
              "      <td>22.017376</td>\n",
              "      <td>22.021044</td>\n",
              "      <td>23.042858</td>\n",
              "      <td>22.518988</td>\n",
              "      <td>22.006868</td>\n",
              "      <td>22.208351</td>\n",
              "      <td>23.537897</td>\n",
              "      <td>22.301935</td>\n",
              "      <td>22.410576</td>\n",
              "      <td>22.626318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>12.0</td>\n",
              "      <td>13.542101</td>\n",
              "      <td>13.806045</td>\n",
              "      <td>11.338939</td>\n",
              "      <td>14.050797</td>\n",
              "      <td>14.727674</td>\n",
              "      <td>11.843203</td>\n",
              "      <td>12.967569</td>\n",
              "      <td>12.134704</td>\n",
              "      <td>13.193330</td>\n",
              "      <td>12.669938</td>\n",
              "      <td>13.150398</td>\n",
              "      <td>13.353300</td>\n",
              "      <td>11.416687</td>\n",
              "      <td>11.689954</td>\n",
              "      <td>13.131042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>19.0</td>\n",
              "      <td>17.730577</td>\n",
              "      <td>18.939526</td>\n",
              "      <td>18.355873</td>\n",
              "      <td>18.757765</td>\n",
              "      <td>19.612286</td>\n",
              "      <td>17.932442</td>\n",
              "      <td>17.478149</td>\n",
              "      <td>17.080858</td>\n",
              "      <td>18.046850</td>\n",
              "      <td>16.627483</td>\n",
              "      <td>15.572855</td>\n",
              "      <td>16.360516</td>\n",
              "      <td>18.115881</td>\n",
              "      <td>17.255943</td>\n",
              "      <td>16.125481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>18.0</td>\n",
              "      <td>17.296686</td>\n",
              "      <td>19.056641</td>\n",
              "      <td>19.009691</td>\n",
              "      <td>18.744556</td>\n",
              "      <td>18.000357</td>\n",
              "      <td>18.300846</td>\n",
              "      <td>18.260881</td>\n",
              "      <td>18.304726</td>\n",
              "      <td>18.502983</td>\n",
              "      <td>17.709021</td>\n",
              "      <td>16.053080</td>\n",
              "      <td>17.255606</td>\n",
              "      <td>16.815731</td>\n",
              "      <td>17.358860</td>\n",
              "      <td>17.006775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>19.0</td>\n",
              "      <td>18.988354</td>\n",
              "      <td>19.460320</td>\n",
              "      <td>19.138590</td>\n",
              "      <td>19.560942</td>\n",
              "      <td>19.348114</td>\n",
              "      <td>19.540974</td>\n",
              "      <td>19.265949</td>\n",
              "      <td>20.617126</td>\n",
              "      <td>20.384748</td>\n",
              "      <td>18.609076</td>\n",
              "      <td>19.851597</td>\n",
              "      <td>19.760954</td>\n",
              "      <td>18.025686</td>\n",
              "      <td>19.007259</td>\n",
              "      <td>19.822062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>15.0</td>\n",
              "      <td>17.626106</td>\n",
              "      <td>18.111706</td>\n",
              "      <td>16.890810</td>\n",
              "      <td>17.524708</td>\n",
              "      <td>17.863947</td>\n",
              "      <td>17.392155</td>\n",
              "      <td>16.650349</td>\n",
              "      <td>16.613667</td>\n",
              "      <td>17.136173</td>\n",
              "      <td>17.910599</td>\n",
              "      <td>17.206158</td>\n",
              "      <td>17.987053</td>\n",
              "      <td>18.052540</td>\n",
              "      <td>18.557220</td>\n",
              "      <td>18.018667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>20.0</td>\n",
              "      <td>17.405796</td>\n",
              "      <td>18.618668</td>\n",
              "      <td>17.579794</td>\n",
              "      <td>18.059498</td>\n",
              "      <td>16.602160</td>\n",
              "      <td>18.458616</td>\n",
              "      <td>17.565907</td>\n",
              "      <td>17.233433</td>\n",
              "      <td>18.920244</td>\n",
              "      <td>17.385801</td>\n",
              "      <td>20.754475</td>\n",
              "      <td>20.019197</td>\n",
              "      <td>20.162527</td>\n",
              "      <td>18.353119</td>\n",
              "      <td>18.437304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>17.0</td>\n",
              "      <td>14.868840</td>\n",
              "      <td>15.925675</td>\n",
              "      <td>13.585452</td>\n",
              "      <td>15.440026</td>\n",
              "      <td>16.448004</td>\n",
              "      <td>15.997107</td>\n",
              "      <td>14.989971</td>\n",
              "      <td>17.009195</td>\n",
              "      <td>15.642363</td>\n",
              "      <td>15.114781</td>\n",
              "      <td>15.941126</td>\n",
              "      <td>15.405235</td>\n",
              "      <td>16.048969</td>\n",
              "      <td>15.975535</td>\n",
              "      <td>16.989092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>19.0</td>\n",
              "      <td>20.629482</td>\n",
              "      <td>21.309982</td>\n",
              "      <td>22.547499</td>\n",
              "      <td>21.798948</td>\n",
              "      <td>22.236031</td>\n",
              "      <td>20.572357</td>\n",
              "      <td>20.097321</td>\n",
              "      <td>22.055424</td>\n",
              "      <td>20.518080</td>\n",
              "      <td>18.831722</td>\n",
              "      <td>20.265886</td>\n",
              "      <td>19.720657</td>\n",
              "      <td>20.438402</td>\n",
              "      <td>20.638552</td>\n",
              "      <td>19.973049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>21.0</td>\n",
              "      <td>18.606709</td>\n",
              "      <td>21.710974</td>\n",
              "      <td>19.252287</td>\n",
              "      <td>19.713871</td>\n",
              "      <td>23.118177</td>\n",
              "      <td>20.311005</td>\n",
              "      <td>20.287062</td>\n",
              "      <td>20.461077</td>\n",
              "      <td>20.580431</td>\n",
              "      <td>20.426895</td>\n",
              "      <td>20.552671</td>\n",
              "      <td>20.153997</td>\n",
              "      <td>20.919634</td>\n",
              "      <td>21.395149</td>\n",
              "      <td>20.560411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.855022</td>\n",
              "      <td>14.477091</td>\n",
              "      <td>13.823819</td>\n",
              "      <td>14.630529</td>\n",
              "      <td>13.599232</td>\n",
              "      <td>13.656416</td>\n",
              "      <td>14.540446</td>\n",
              "      <td>14.656211</td>\n",
              "      <td>15.798012</td>\n",
              "      <td>14.372834</td>\n",
              "      <td>15.638974</td>\n",
              "      <td>14.229697</td>\n",
              "      <td>14.072216</td>\n",
              "      <td>14.269898</td>\n",
              "      <td>15.365809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>17.0</td>\n",
              "      <td>14.243763</td>\n",
              "      <td>15.850630</td>\n",
              "      <td>14.578341</td>\n",
              "      <td>14.833133</td>\n",
              "      <td>14.790435</td>\n",
              "      <td>15.031107</td>\n",
              "      <td>15.618595</td>\n",
              "      <td>14.221186</td>\n",
              "      <td>16.396439</td>\n",
              "      <td>14.308341</td>\n",
              "      <td>15.517136</td>\n",
              "      <td>14.932741</td>\n",
              "      <td>15.099182</td>\n",
              "      <td>15.114908</td>\n",
              "      <td>15.568232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>15.0</td>\n",
              "      <td>14.652842</td>\n",
              "      <td>15.439577</td>\n",
              "      <td>14.483457</td>\n",
              "      <td>16.622023</td>\n",
              "      <td>14.280610</td>\n",
              "      <td>16.630512</td>\n",
              "      <td>14.993143</td>\n",
              "      <td>17.060226</td>\n",
              "      <td>16.214722</td>\n",
              "      <td>16.735199</td>\n",
              "      <td>16.174900</td>\n",
              "      <td>17.250360</td>\n",
              "      <td>15.846347</td>\n",
              "      <td>15.550993</td>\n",
              "      <td>16.584551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>21.0</td>\n",
              "      <td>17.878487</td>\n",
              "      <td>18.302361</td>\n",
              "      <td>16.927153</td>\n",
              "      <td>16.978041</td>\n",
              "      <td>18.880093</td>\n",
              "      <td>18.344734</td>\n",
              "      <td>17.721203</td>\n",
              "      <td>18.155409</td>\n",
              "      <td>18.658598</td>\n",
              "      <td>17.034060</td>\n",
              "      <td>17.677073</td>\n",
              "      <td>16.558111</td>\n",
              "      <td>17.588715</td>\n",
              "      <td>17.649759</td>\n",
              "      <td>17.558111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>19.0</td>\n",
              "      <td>16.332293</td>\n",
              "      <td>18.511236</td>\n",
              "      <td>16.109682</td>\n",
              "      <td>16.466679</td>\n",
              "      <td>17.397280</td>\n",
              "      <td>18.765438</td>\n",
              "      <td>17.932436</td>\n",
              "      <td>17.759705</td>\n",
              "      <td>19.999836</td>\n",
              "      <td>17.882872</td>\n",
              "      <td>20.346523</td>\n",
              "      <td>19.708944</td>\n",
              "      <td>19.171804</td>\n",
              "      <td>22.207342</td>\n",
              "      <td>20.069557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>17.0</td>\n",
              "      <td>18.421608</td>\n",
              "      <td>19.502392</td>\n",
              "      <td>19.346493</td>\n",
              "      <td>18.655588</td>\n",
              "      <td>20.087976</td>\n",
              "      <td>17.838074</td>\n",
              "      <td>18.120354</td>\n",
              "      <td>18.588303</td>\n",
              "      <td>19.190153</td>\n",
              "      <td>18.343943</td>\n",
              "      <td>18.792059</td>\n",
              "      <td>18.316456</td>\n",
              "      <td>19.379578</td>\n",
              "      <td>19.479605</td>\n",
              "      <td>18.485548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.132609</td>\n",
              "      <td>16.224701</td>\n",
              "      <td>15.771392</td>\n",
              "      <td>16.424994</td>\n",
              "      <td>15.888462</td>\n",
              "      <td>16.716595</td>\n",
              "      <td>16.471413</td>\n",
              "      <td>16.551378</td>\n",
              "      <td>16.688139</td>\n",
              "      <td>16.680050</td>\n",
              "      <td>15.400311</td>\n",
              "      <td>16.442144</td>\n",
              "      <td>17.395538</td>\n",
              "      <td>16.100853</td>\n",
              "      <td>15.945303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>20.0</td>\n",
              "      <td>19.492592</td>\n",
              "      <td>22.159861</td>\n",
              "      <td>20.409103</td>\n",
              "      <td>19.957970</td>\n",
              "      <td>20.678410</td>\n",
              "      <td>21.200632</td>\n",
              "      <td>20.790155</td>\n",
              "      <td>18.605188</td>\n",
              "      <td>20.717855</td>\n",
              "      <td>19.788475</td>\n",
              "      <td>18.887751</td>\n",
              "      <td>20.959852</td>\n",
              "      <td>19.780409</td>\n",
              "      <td>18.981382</td>\n",
              "      <td>19.938316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>16.0</td>\n",
              "      <td>15.496405</td>\n",
              "      <td>16.312719</td>\n",
              "      <td>13.045691</td>\n",
              "      <td>15.777010</td>\n",
              "      <td>15.306217</td>\n",
              "      <td>15.312069</td>\n",
              "      <td>14.831355</td>\n",
              "      <td>16.326694</td>\n",
              "      <td>15.811578</td>\n",
              "      <td>15.546432</td>\n",
              "      <td>14.627169</td>\n",
              "      <td>15.276861</td>\n",
              "      <td>15.286697</td>\n",
              "      <td>14.995596</td>\n",
              "      <td>13.456868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 387
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#すべての平均\n",
        "df[\"average_all\"]=df.iloc[:,1:15].mean(axis='columns')\n",
        "\n",
        "# #各fold毎の（3部位）平均\n",
        "# for fold in [0,1,2,3,4]:\n",
        "#     pred_loc = df.loc[:, [f\"half_fold{fold}\",f\"periocular_fold{fold}\" , f\"eye_fold{fold}\"]]\n",
        "#     average = pred_loc.mean(axis='columns')\n",
        "#     df[f\"average_fold{fold}\"] = average\n",
        "\n",
        "#各部位における推定のうち、最も5-foldのばらつき（標準偏差）が少ないものの平均をとる\n",
        "AREA = [\"half\", \"periocular\", \"eye\"]\n",
        "pred_stable = []\n",
        "for row in range(len(df)):\n",
        "    pred_all = [df.iloc[row, 1:6], df.iloc[row, 6:11], df.iloc[row, 11:16]] #[half, periocular, eye]\n",
        "    stdev = [statistics.stdev(pred_all[0]), statistics.stdev(pred_all[1]), statistics.stdev(pred_all[2])]\n",
        "    pred = statistics.mean(pred_all[np.argmax(stdev)])\n",
        "    pred_stable.append(pred)\n",
        "df['stable_ave'] = pred_stable"
      ],
      "metadata": {
        "id": "mn8kJy2KfLjn"
      },
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis\n",
        "df_analysis = pd.DataFrame(index=[\"AveError\", \"StdError\", \"AveAbsError\", \"StdAbsError\", \"Corrected_AveAbsError\", \"Corrected_StdAbsError\", \"Corr_coef\", \"<=1mm_rate\", \"<=2mm_rate\", \">2mm_rate\", \">18mm_sensitivity\", \">18mm_specificity\", \">18mm_positive_predictive\", \">18mm_negative_predictive\"], columns=df.columns[1:])\n",
        "targets = df['targets']\n",
        "\n",
        "for column in df.columns[1:]:\n",
        "    outputs = df[column]\n",
        "    df_analysis.loc[df_analysis.index[0],column] = statistics.mean(outputs-targets) #AveError\n",
        "    df_analysis.loc[df_analysis.index[1],column] = statistics.stdev(outputs-targets) #StdError\n",
        "    df_analysis.loc[df_analysis.index[2],column] = statistics.mean(abs(outputs-targets)) #AveAbsError\n",
        "    df_analysis.loc[df_analysis.index[3],column] = statistics.stdev(abs(outputs-targets)) #StdAbsError\n",
        "\n",
        "    #平均からの差分を補正\n",
        "    corrected_error = (np.array(outputs)-np.array(targets)-np.array(statistics.mean(outputs-targets))).tolist()\n",
        "    corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "    df_analysis.loc[df_analysis.index[4],column] = statistics.mean(corrected_AbsError) #Corrected_AveAbsError\n",
        "    df_analysis.loc[df_analysis.index[5],column] = statistics.stdev(corrected_AbsError) #Corrected_StdAbsError\n",
        "    df_analysis.loc[df_analysis.index[6],column] = np.corrcoef(outputs, targets)[0,1] #Corr_coef\n",
        "    total = len(df)\n",
        "    within1 = sum((i <= 1 and i >= -1 for i in outputs-targets))\n",
        "    within2 = sum((i <= 2 and i >= -2 for i in outputs-targets))\n",
        "    over2 = sum((i > 2 or i < -2 for i in outputs-targets))\n",
        "\n",
        "    df_analysis.loc[df_analysis.index[7],column] = my_round(within1/total*100) #<=1mm_rate\n",
        "    df_analysis.loc[df_analysis.index[8],column] = my_round(within2/total*100) #<=2mm_rate\n",
        "    df_analysis.loc[df_analysis.index[9],column] = my_round(over2/total*100) #>2mm_rate\n",
        "\n",
        "    TP, FP, TN, FN = 0,0,0,0\n",
        "    for i in range(len(df)):\n",
        "      if df[\"targets\"][i]>=18 and df[column][i]>= 17.5:\n",
        "          TP += 1\n",
        "      if df[\"targets\"][i]<18 and df[column][i]>= 18.5:\n",
        "          FP += 1\n",
        "      if df[\"targets\"][i]>=18 and df[column][i]< 17.5:\n",
        "          FN += 1 \n",
        "      if df[\"targets\"][i]<18 and df[column][i]< 17.5:\n",
        "          TN += 1    \n",
        "\n",
        "    df_analysis.loc[df_analysis.index[10],column] = TP/(TP+FN) #Sensitivity\n",
        "    df_analysis.loc[df_analysis.index[11],column] = TN/(FP+TN) #Specificity\n",
        "    df_analysis.loc[df_analysis.index[12],column] = TP/(TP+FP) #Positive predictive value\n",
        "    df_analysis.loc[df_analysis.index[13],column] = TN/(TN+FN) #Negative predictive value\n",
        "\n",
        "\n",
        "df_analysis.to_csv( f\"./models_Hertel_estimation/5-fold-crossvalidation/analysis.csv\", header=True, index=True)"
      ],
      "metadata": {
        "id": "8QgA6XT_hMCY"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate average & stdev in each area\n",
        "AREA = [\"half\", \"periocular\", \"eye\"]\n",
        "fold = [0,1,2,3,4]\n",
        "\n",
        "all_fold_average, all_fold_stdev, all_fold = [], [], []\n",
        "for i, area in enumerate(AREA):\n",
        "    all_fold_average.append(df_analysis.loc[:, [f\"{area}_fold{str(i)}\" for i in fold]].mean(axis='columns'))\n",
        "    all_fold_stdev.append(df_analysis.loc[:, [f\"{area}_fold{str(i)}\" for i in fold]].std(axis='columns'))\n",
        "\n",
        "    all_fold.append([f\"{my_round(average)}±{my_round(stdev)}\" for average, stdev in zip(all_fold_average[i],all_fold_stdev[i])])\n",
        "    df_analysis[f\"{area}_all_fold\"] = all_fold[i]\n",
        "    #print(all_fold[i])\n",
        "\n",
        "\n",
        "#Freidman analysis (各部位の精度に有意差があるかどうか)\n",
        "half =  df_analysis.loc[:, [f\"half_fold{str(i)}\" for i in fold]]\n",
        "periocular =  df_analysis.loc[:, [f\"periocular_fold{str(i)}\" for i in fold]]\n",
        "eye =  df_analysis.loc[:, [f\"eye_fold{str(i)}\" for i in fold]]\n",
        "freidman = []\n",
        "\n",
        "for i in range(len(df_analysis)):\n",
        "    freidman.append(my_round(stats.friedmanchisquare(half.iloc[i,:],periocular.iloc[i,:], eye.iloc[i,:])[1]))\n",
        "df_analysis[\"p-value_Freidman\"] = freidman\n",
        "\n",
        "df_analysis.to_csv( f\"./models_Hertel_estimation/5-fold-crossvalidation/analysis.csv\", header=True, index=True, encoding = \"shift-jis\")"
      ],
      "metadata": {
        "id": "EQV0w4BjWoq2"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u86wisiyQyHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "p-JhpT5G1UOu",
        "outputId": "da1063d2-0101-42c1-ca3a-902ad364dd4c"
      },
      "execution_count": 392,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          half_fold0 half_fold1 half_fold2 half_fold3  \\\n",
              "AveError                   -0.311591   0.417779   -0.56754  -0.211157   \n",
              "StdError                    1.335633   1.042371   1.310668   1.373913   \n",
              "AveAbsError                  1.06121   0.842036   1.098227   1.075447   \n",
              "StdAbsError                 0.865772   0.741156   0.910668    0.87746   \n",
              "Corrected_AveAbsError       1.013526   0.760527   0.988659   1.062684   \n",
              "Corrected_StdAbsError       0.866842   0.710753    0.85755   0.867495   \n",
              "Corr_coef                   0.894089   0.935201   0.903746   0.885183   \n",
              "<=1mm_rate                     54.08      66.84       55.1      52.55   \n",
              "<=2mm_rate                     86.73      91.33      86.22      87.76   \n",
              ">2mm_rate                      13.27       8.67      13.78      12.24   \n",
              ">18mm_sensitivity            0.78481   0.949367    0.78481   0.810127   \n",
              ">18mm_specificity           0.990991   0.990654   0.973913    0.95614   \n",
              ">18mm_positive_predictive   0.984127   0.986842   0.953846   0.927536   \n",
              ">18mm_negative_predictive   0.866142   0.963636   0.868217   0.879032   \n",
              "\n",
              "                          half_fold4 periocular_fold0 periocular_fold1  \\\n",
              "AveError                    0.041006         0.119423        -0.233782   \n",
              "StdError                    1.210769         1.103213         1.135609   \n",
              "AveAbsError                 0.897991         0.810026         0.813119   \n",
              "StdAbsError                 0.810635         0.756245         0.824614   \n",
              "Corrected_AveAbsError       0.898931         0.808215         0.807477   \n",
              "Corrected_StdAbsError       0.808544         0.748677         0.796395   \n",
              "Corr_coef                   0.915333         0.928942         0.923216   \n",
              "<=1mm_rate                     67.86            74.49            71.43   \n",
              "<=2mm_rate                     88.27            92.35            91.33   \n",
              ">2mm_rate                      11.73             7.65             8.67   \n",
              ">18mm_sensitivity           0.886076         0.949367         0.860759   \n",
              ">18mm_specificity           0.981481         0.981481          0.99115   \n",
              ">18mm_positive_predictive   0.972222         0.974026         0.985507   \n",
              ">18mm_negative_predictive   0.921739         0.963636         0.910569   \n",
              "\n",
              "                          periocular_fold2 periocular_fold3 periocular_fold4  \\\n",
              "AveError                         -0.109735         0.190205        -0.258897   \n",
              "StdError                          1.307174         0.963924           1.1881   \n",
              "AveAbsError                       1.000586         0.769067         0.913971   \n",
              "StdAbsError                       0.845282          0.60911         0.799578   \n",
              "Corrected_AveAbsError             0.996806         0.737531         0.894902   \n",
              "Corrected_StdAbsError             0.842607         0.618391         0.778861   \n",
              "Corr_coef                         0.911439         0.947002         0.914935   \n",
              "<=1mm_rate                           57.14            73.47            65.82   \n",
              "<=2mm_rate                           87.76             94.9            92.86   \n",
              ">2mm_rate                            12.24              5.1             7.14   \n",
              ">18mm_sensitivity                 0.848101         0.911392         0.848101   \n",
              ">18mm_specificity                 0.973214         0.972727              1.0   \n",
              ">18mm_positive_predictive         0.957143             0.96              1.0   \n",
              ">18mm_negative_predictive         0.900826         0.938596         0.902439   \n",
              "\n",
              "                          eye_fold0 eye_fold1 eye_fold2 eye_fold3 eye_fold4  \\\n",
              "AveError                     0.0262  0.077994 -0.448521 -0.279955 -0.069103   \n",
              "StdError                   1.297795  1.347517  1.197901  1.193373  1.311887   \n",
              "AveAbsError                0.994437  1.047419  0.976815  0.961311  1.033183   \n",
              "StdAbsError                0.831254  0.848058  0.823475  0.757666  0.808027   \n",
              "Corrected_AveAbsError      0.995507   1.04406  0.877378  0.943396  1.031191   \n",
              "Corrected_StdAbsError      0.829551  0.848617  0.813159  0.727721  0.807613   \n",
              "Corr_coef                   0.89777  0.891774   0.91788  0.927838  0.899911   \n",
              "<=1mm_rate                     55.1     57.65     63.27     58.67     55.61   \n",
              "<=2mm_rate                    91.33     84.18      89.8     90.82      89.8   \n",
              ">2mm_rate                      8.67     15.82      10.2      9.18      10.2   \n",
              ">18mm_sensitivity          0.835443  0.848101  0.772152  0.835443  0.898734   \n",
              ">18mm_specificity           0.99115       1.0  0.990991  0.973214       1.0   \n",
              ">18mm_positive_predictive  0.985075       1.0  0.983871  0.956522       1.0   \n",
              ">18mm_negative_predictive     0.896  0.898305  0.859375  0.893443  0.932203   \n",
              "\n",
              "                          average_all stable_ave half_all_fold  \\\n",
              "AveError                    -0.110612  -0.090621    -0.13±0.37   \n",
              "StdError                     0.795741   0.874702     1.25±0.13   \n",
              "AveAbsError                  0.604727    0.67638     0.99±0.12   \n",
              "StdAbsError                  0.527192   0.559931     0.84±0.07   \n",
              "Corrected_AveAbsError        0.597326   0.670332     0.94±0.12   \n",
              "Corrected_StdAbsError           0.524    0.55987     0.82±0.07   \n",
              "Corr_coef                     0.96402   0.954918     0.91±0.02   \n",
              "<=1mm_rate                      81.12      75.51    59.29±7.43   \n",
              "<=2mm_rate                      98.47      96.43     88.06±2.0   \n",
              ">2mm_rate                        1.53       3.57     11.94±2.0   \n",
              ">18mm_sensitivity            0.936709   0.924051     0.84±0.07   \n",
              ">18mm_specificity            0.991228   0.991228     0.98±0.01   \n",
              ">18mm_positive_predictive    0.986667   0.986486     0.96±0.02   \n",
              ">18mm_negative_predictive    0.957627    0.94958      0.9±0.04   \n",
              "\n",
              "                          periocular_all_fold eye_all_fold  p-value_Freidman  \n",
              "AveError                            -0.06±0.2   -0.14±0.22              0.82  \n",
              "StdError                            1.14±0.13    1.27±0.07              0.25  \n",
              "AveAbsError                         0.86±0.09     1.0±0.04              0.25  \n",
              "StdAbsError                         0.77±0.09    0.81±0.03              0.17  \n",
              "Corrected_AveAbsError                0.85±0.1    0.98±0.07              0.55  \n",
              "Corrected_StdAbsError               0.76±0.08    0.81±0.05              0.17  \n",
              "Corr_coef                           0.93±0.01    0.91±0.02              0.55  \n",
              "<=1mm_rate                         68.47±7.17   58.06±3.26              0.25  \n",
              "<=2mm_rate                         91.84±2.63   89.19±2.88              0.08  \n",
              ">2mm_rate                           8.16±2.63   10.81±2.88              0.08  \n",
              ">18mm_sensitivity                   0.88±0.05    0.84±0.05              0.55  \n",
              ">18mm_specificity                   0.98±0.01    0.99±0.01              0.04  \n",
              ">18mm_positive_predictive           0.98±0.02    0.99±0.02              0.10  \n",
              ">18mm_negative_predictive           0.92±0.03     0.9±0.03              0.55  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>half_fold0</th>\n",
              "      <th>half_fold1</th>\n",
              "      <th>half_fold2</th>\n",
              "      <th>half_fold3</th>\n",
              "      <th>half_fold4</th>\n",
              "      <th>periocular_fold0</th>\n",
              "      <th>periocular_fold1</th>\n",
              "      <th>periocular_fold2</th>\n",
              "      <th>periocular_fold3</th>\n",
              "      <th>periocular_fold4</th>\n",
              "      <th>eye_fold0</th>\n",
              "      <th>eye_fold1</th>\n",
              "      <th>eye_fold2</th>\n",
              "      <th>eye_fold3</th>\n",
              "      <th>eye_fold4</th>\n",
              "      <th>average_all</th>\n",
              "      <th>stable_ave</th>\n",
              "      <th>half_all_fold</th>\n",
              "      <th>periocular_all_fold</th>\n",
              "      <th>eye_all_fold</th>\n",
              "      <th>p-value_Freidman</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AveError</th>\n",
              "      <td>-0.311591</td>\n",
              "      <td>0.417779</td>\n",
              "      <td>-0.56754</td>\n",
              "      <td>-0.211157</td>\n",
              "      <td>0.041006</td>\n",
              "      <td>0.119423</td>\n",
              "      <td>-0.233782</td>\n",
              "      <td>-0.109735</td>\n",
              "      <td>0.190205</td>\n",
              "      <td>-0.258897</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.077994</td>\n",
              "      <td>-0.448521</td>\n",
              "      <td>-0.279955</td>\n",
              "      <td>-0.069103</td>\n",
              "      <td>-0.110612</td>\n",
              "      <td>-0.090621</td>\n",
              "      <td>-0.13±0.37</td>\n",
              "      <td>-0.06±0.2</td>\n",
              "      <td>-0.14±0.22</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StdError</th>\n",
              "      <td>1.335633</td>\n",
              "      <td>1.042371</td>\n",
              "      <td>1.310668</td>\n",
              "      <td>1.373913</td>\n",
              "      <td>1.210769</td>\n",
              "      <td>1.103213</td>\n",
              "      <td>1.135609</td>\n",
              "      <td>1.307174</td>\n",
              "      <td>0.963924</td>\n",
              "      <td>1.1881</td>\n",
              "      <td>1.297795</td>\n",
              "      <td>1.347517</td>\n",
              "      <td>1.197901</td>\n",
              "      <td>1.193373</td>\n",
              "      <td>1.311887</td>\n",
              "      <td>0.795741</td>\n",
              "      <td>0.874702</td>\n",
              "      <td>1.25±0.13</td>\n",
              "      <td>1.14±0.13</td>\n",
              "      <td>1.27±0.07</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveAbsError</th>\n",
              "      <td>1.06121</td>\n",
              "      <td>0.842036</td>\n",
              "      <td>1.098227</td>\n",
              "      <td>1.075447</td>\n",
              "      <td>0.897991</td>\n",
              "      <td>0.810026</td>\n",
              "      <td>0.813119</td>\n",
              "      <td>1.000586</td>\n",
              "      <td>0.769067</td>\n",
              "      <td>0.913971</td>\n",
              "      <td>0.994437</td>\n",
              "      <td>1.047419</td>\n",
              "      <td>0.976815</td>\n",
              "      <td>0.961311</td>\n",
              "      <td>1.033183</td>\n",
              "      <td>0.604727</td>\n",
              "      <td>0.67638</td>\n",
              "      <td>0.99±0.12</td>\n",
              "      <td>0.86±0.09</td>\n",
              "      <td>1.0±0.04</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StdAbsError</th>\n",
              "      <td>0.865772</td>\n",
              "      <td>0.741156</td>\n",
              "      <td>0.910668</td>\n",
              "      <td>0.87746</td>\n",
              "      <td>0.810635</td>\n",
              "      <td>0.756245</td>\n",
              "      <td>0.824614</td>\n",
              "      <td>0.845282</td>\n",
              "      <td>0.60911</td>\n",
              "      <td>0.799578</td>\n",
              "      <td>0.831254</td>\n",
              "      <td>0.848058</td>\n",
              "      <td>0.823475</td>\n",
              "      <td>0.757666</td>\n",
              "      <td>0.808027</td>\n",
              "      <td>0.527192</td>\n",
              "      <td>0.559931</td>\n",
              "      <td>0.84±0.07</td>\n",
              "      <td>0.77±0.09</td>\n",
              "      <td>0.81±0.03</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Corrected_AveAbsError</th>\n",
              "      <td>1.013526</td>\n",
              "      <td>0.760527</td>\n",
              "      <td>0.988659</td>\n",
              "      <td>1.062684</td>\n",
              "      <td>0.898931</td>\n",
              "      <td>0.808215</td>\n",
              "      <td>0.807477</td>\n",
              "      <td>0.996806</td>\n",
              "      <td>0.737531</td>\n",
              "      <td>0.894902</td>\n",
              "      <td>0.995507</td>\n",
              "      <td>1.04406</td>\n",
              "      <td>0.877378</td>\n",
              "      <td>0.943396</td>\n",
              "      <td>1.031191</td>\n",
              "      <td>0.597326</td>\n",
              "      <td>0.670332</td>\n",
              "      <td>0.94±0.12</td>\n",
              "      <td>0.85±0.1</td>\n",
              "      <td>0.98±0.07</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Corrected_StdAbsError</th>\n",
              "      <td>0.866842</td>\n",
              "      <td>0.710753</td>\n",
              "      <td>0.85755</td>\n",
              "      <td>0.867495</td>\n",
              "      <td>0.808544</td>\n",
              "      <td>0.748677</td>\n",
              "      <td>0.796395</td>\n",
              "      <td>0.842607</td>\n",
              "      <td>0.618391</td>\n",
              "      <td>0.778861</td>\n",
              "      <td>0.829551</td>\n",
              "      <td>0.848617</td>\n",
              "      <td>0.813159</td>\n",
              "      <td>0.727721</td>\n",
              "      <td>0.807613</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.55987</td>\n",
              "      <td>0.82±0.07</td>\n",
              "      <td>0.76±0.08</td>\n",
              "      <td>0.81±0.05</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Corr_coef</th>\n",
              "      <td>0.894089</td>\n",
              "      <td>0.935201</td>\n",
              "      <td>0.903746</td>\n",
              "      <td>0.885183</td>\n",
              "      <td>0.915333</td>\n",
              "      <td>0.928942</td>\n",
              "      <td>0.923216</td>\n",
              "      <td>0.911439</td>\n",
              "      <td>0.947002</td>\n",
              "      <td>0.914935</td>\n",
              "      <td>0.89777</td>\n",
              "      <td>0.891774</td>\n",
              "      <td>0.91788</td>\n",
              "      <td>0.927838</td>\n",
              "      <td>0.899911</td>\n",
              "      <td>0.96402</td>\n",
              "      <td>0.954918</td>\n",
              "      <td>0.91±0.02</td>\n",
              "      <td>0.93±0.01</td>\n",
              "      <td>0.91±0.02</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;=1mm_rate</th>\n",
              "      <td>54.08</td>\n",
              "      <td>66.84</td>\n",
              "      <td>55.1</td>\n",
              "      <td>52.55</td>\n",
              "      <td>67.86</td>\n",
              "      <td>74.49</td>\n",
              "      <td>71.43</td>\n",
              "      <td>57.14</td>\n",
              "      <td>73.47</td>\n",
              "      <td>65.82</td>\n",
              "      <td>55.1</td>\n",
              "      <td>57.65</td>\n",
              "      <td>63.27</td>\n",
              "      <td>58.67</td>\n",
              "      <td>55.61</td>\n",
              "      <td>81.12</td>\n",
              "      <td>75.51</td>\n",
              "      <td>59.29±7.43</td>\n",
              "      <td>68.47±7.17</td>\n",
              "      <td>58.06±3.26</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;=2mm_rate</th>\n",
              "      <td>86.73</td>\n",
              "      <td>91.33</td>\n",
              "      <td>86.22</td>\n",
              "      <td>87.76</td>\n",
              "      <td>88.27</td>\n",
              "      <td>92.35</td>\n",
              "      <td>91.33</td>\n",
              "      <td>87.76</td>\n",
              "      <td>94.9</td>\n",
              "      <td>92.86</td>\n",
              "      <td>91.33</td>\n",
              "      <td>84.18</td>\n",
              "      <td>89.8</td>\n",
              "      <td>90.82</td>\n",
              "      <td>89.8</td>\n",
              "      <td>98.47</td>\n",
              "      <td>96.43</td>\n",
              "      <td>88.06±2.0</td>\n",
              "      <td>91.84±2.63</td>\n",
              "      <td>89.19±2.88</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;2mm_rate</th>\n",
              "      <td>13.27</td>\n",
              "      <td>8.67</td>\n",
              "      <td>13.78</td>\n",
              "      <td>12.24</td>\n",
              "      <td>11.73</td>\n",
              "      <td>7.65</td>\n",
              "      <td>8.67</td>\n",
              "      <td>12.24</td>\n",
              "      <td>5.1</td>\n",
              "      <td>7.14</td>\n",
              "      <td>8.67</td>\n",
              "      <td>15.82</td>\n",
              "      <td>10.2</td>\n",
              "      <td>9.18</td>\n",
              "      <td>10.2</td>\n",
              "      <td>1.53</td>\n",
              "      <td>3.57</td>\n",
              "      <td>11.94±2.0</td>\n",
              "      <td>8.16±2.63</td>\n",
              "      <td>10.81±2.88</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;18mm_sensitivity</th>\n",
              "      <td>0.78481</td>\n",
              "      <td>0.949367</td>\n",
              "      <td>0.78481</td>\n",
              "      <td>0.810127</td>\n",
              "      <td>0.886076</td>\n",
              "      <td>0.949367</td>\n",
              "      <td>0.860759</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.911392</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.835443</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.772152</td>\n",
              "      <td>0.835443</td>\n",
              "      <td>0.898734</td>\n",
              "      <td>0.936709</td>\n",
              "      <td>0.924051</td>\n",
              "      <td>0.84±0.07</td>\n",
              "      <td>0.88±0.05</td>\n",
              "      <td>0.84±0.05</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;18mm_specificity</th>\n",
              "      <td>0.990991</td>\n",
              "      <td>0.990654</td>\n",
              "      <td>0.973913</td>\n",
              "      <td>0.95614</td>\n",
              "      <td>0.981481</td>\n",
              "      <td>0.981481</td>\n",
              "      <td>0.99115</td>\n",
              "      <td>0.973214</td>\n",
              "      <td>0.972727</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.99115</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990991</td>\n",
              "      <td>0.973214</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.98±0.01</td>\n",
              "      <td>0.98±0.01</td>\n",
              "      <td>0.99±0.01</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;18mm_positive_predictive</th>\n",
              "      <td>0.984127</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.927536</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.974026</td>\n",
              "      <td>0.985507</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985075</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.983871</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.986486</td>\n",
              "      <td>0.96±0.02</td>\n",
              "      <td>0.98±0.02</td>\n",
              "      <td>0.99±0.02</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;18mm_negative_predictive</th>\n",
              "      <td>0.866142</td>\n",
              "      <td>0.963636</td>\n",
              "      <td>0.868217</td>\n",
              "      <td>0.879032</td>\n",
              "      <td>0.921739</td>\n",
              "      <td>0.963636</td>\n",
              "      <td>0.910569</td>\n",
              "      <td>0.900826</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.902439</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.898305</td>\n",
              "      <td>0.859375</td>\n",
              "      <td>0.893443</td>\n",
              "      <td>0.932203</td>\n",
              "      <td>0.957627</td>\n",
              "      <td>0.94958</td>\n",
              "      <td>0.9±0.04</td>\n",
              "      <td>0.92±0.03</td>\n",
              "      <td>0.9±0.03</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 392
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw scatterplot\n",
        "\n",
        "targets = df[\"targets\"]\n",
        "outputs = df[\"average_all\"]\n",
        "\n",
        "#Draw Graphs（もともとの散布図\n",
        "df_plot = pd.DataFrame({'estimate':outputs, 'target':targets, })\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(y='target', x='estimate', data=df_plot)\n",
        "plt.xlim(8,26)\n",
        "plt.ylim(8,26)\n",
        "\n",
        "plt.savefig(\"scatter_all.png\", format=\"png\", dpi=300)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df_plot['target']-df_plot['estimate'], bins=15, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "\n",
        "plt.savefig(\"hist_all.png\", format=\"png\", dpi=300)\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Bland-Altman-Plot \n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "    print(\"Brand-Altman Plot:\")\n",
        "    print(f\"md+1.96sd: {md+1.96*sd}\")\n",
        "    print(f\"md-1.96sd: {md-1.96*sd}\")\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "\n",
        "plt.savefig(\"brandalt_all.png\", format=\"png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V1G9vgI2Djy3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "1f244fa9-614f-44eb-9301-4e2507ecbbe1"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brand-Altman Plot:\n",
            "md+1.96sd: 1.4450573260786541\n",
            "md-1.96sd: -1.6662816914381469\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBy1BeytJGel"
      },
      "source": [
        "#**Evaluation using testset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "area_num\n",
        "0: half \n",
        "1: periocular\n",
        "2: eye\n",
        "\"\"\"\n",
        "area_num = 0\n",
        "\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft = mod_RepVGG()\n",
        "model_ft.to(device)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "#PATH = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2.pth\"\n",
        "PATH = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2.pth\"\n",
        "#PATH = f\"./models_Hertel_estimation/5-fold-crossvalidation/half_fold0_RepVGGA2.pth\"\n",
        "\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "r3FmwmZ8P9ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "#evaluation using validation dataset\n",
        "\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = Decimal(str(x)).quantize(Decimal(str(1/10**d)), rounding=ROUND_HALF_UP)\n",
        "    p = float(p)\n",
        "    return p\n",
        "\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in test_loader:  \n",
        "          target = target.view(len(target), 1)         \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft(image_tensor[:,area_num]) #dim0はbach_size、dim1がarea_num\n",
        "\n",
        "          outputs.append(output[0].item())      \n",
        "          targets.append(target[0].item())\n",
        "          print(f\"estimate: {my_round(output[0].item())} mm, target: {target[0].item()} mm\")\n",
        "\n",
        "          errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))\n"
      ],
      "metadata": {
        "id": "9uZfA263UUhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lamJcFxkjkxA"
      },
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['target']-df['estimate'], bins=15, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示"
      ],
      "metadata": {
        "id": "IumsUK1p29SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "IfRogutV3Y4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(df)\n",
        "within1 = sum((i <= 1 and i >= -1 for i in df['estimate']-df['target']))\n",
        "within2 = sum((i <= 2 and i >= -2 for i in df['estimate']-df['target']))\n",
        "over2 = sum((i > 2 or i < -2 for i in df['estimate']-df['target']))\n",
        "\n",
        "print(f'-1<Error<1: {within1}, ({my_round(within1/total*100)}%)')\n",
        "print(f'-2<Error<2: {within1}, ({my_round(within2/total*100)}%)')\n",
        "print(f'Error over 2: {within1}, ({my_round(over2/total*100)}%)')\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,0]>=18 and df.iloc[i,1]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,0]<18 and df.iloc[i,1]>= 18:\n",
        "        FN += 1\n",
        "    if df.iloc[i,0]>=18 and df.iloc[i,1]< 18:\n",
        "        FP += 1 \n",
        "    if df.iloc[i,0]<18 and df.iloc[i,1]< 18:\n",
        "        TN += 1     \n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,0]>=16 and df.iloc[i,1]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,0]<16 and df.iloc[i,1]>= 18:\n",
        "        minogashi += 1\n",
        "    if df.iloc[i,0]>=18 and df.iloc[i,1]<= 16:\n",
        "        kajyou += 1 \n",
        "    if df.iloc[i,0]<18 and df.iloc[i,1]<= 16:\n",
        "        oknegative += 1     \n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')\n",
        "\n"
      ],
      "metadata": {
        "id": "QDNfYarsk3eI",
        "outputId": "f8c2f6bf-3f34-465f-d783-0d4627cf67b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1<Error<1: 136, (69.39%)\n",
            "-2<Error<2: 136, (91.84%)\n",
            "Error over 2: 136, (8.16%)\n",
            "\n",
            "Hertel 18mm以上の検出精度\n",
            "TP: 68\n",
            "FP: 5\n",
            "FN: 11\n",
            "TN: 112\n",
            "Sensitivity: 0.8607594936708861\n",
            "Specificity: 0.9572649572649573\n",
            "Positive predictive value: 0.9315068493150684\n",
            "Negative predictive value: 0.9105691056910569\n",
            "\n",
            "推測18mm以上だが実は16mm未満(過剰): 1例\n",
            "推測16mm未満だが実は18mm以上（見逃がし）: 0例\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSxjXrglZc4k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6d56d2a3-1437-4fe2-b843-386e9278ee32"
      },
      "source": [
        "#Bland-Altman-Plot \n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**平均値、線形近似による補正**"
      ],
      "metadata": {
        "id": "ctOezTjprVyF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R6aSRMkWEZO",
        "outputId": "92602d57-c35f-4e7f-bed2-1d6f617ace49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#線形近似式算出\n",
        "from sklearn import linear_model\n",
        "\n",
        "estimate = df.loc[:,'estimate']\n",
        "target = df.loc[:,'target']\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# 説明変数xに \"x1\"のデータを使用\n",
        "x = np.array([estimate]).T\n",
        "\n",
        "# 目的変数yに \"x2\"のデータを使用\n",
        "y = target.values\n",
        "\n",
        "# 予測モデルを作成（単回帰）\n",
        "clf.fit(x, y)\n",
        "\n",
        "# パラメータ（回帰係数、切片）を抽出\n",
        "[a] = clf.coef_\n",
        "b = clf.intercept_\n",
        "\n",
        "# パラメータの表示\n",
        "print(\"回帰係数:\", a)\n",
        "print(\"切片:\", b)\n",
        "print(\"決定係数:\", clf.score(x, y))\n",
        "\n",
        "#平均値により補正した値\n",
        "df['Corrected_estimate_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,2] = corrected_output[i]\n",
        "\n",
        "#回帰直線により補正した値\n",
        "df['Corrected_estimate_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,3] = df.iloc[i,0]*a+b\n",
        "\n",
        "#残差\n",
        "df['Residual_error_1']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,4] = df.iloc[i,2]-df.iloc[i,1]\n",
        "\n",
        "#残差\n",
        "df['Residual_error_2']=0\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,5] = df.iloc[i,3]-df.iloc[i,1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回帰係数: 0.9243867958397015\n",
            "切片: 1.2198555301736356\n",
            "決定係数: 0.8391249673052268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlWXLynoKxy"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSiUi44-jGIZ"
      },
      "source": [
        "#平均近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_1'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_1', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_1']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_1'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_1'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "\n",
        "print('')\n",
        "print('-1<Error<1: '+ str(sum((i < 1 and i > -1 for i in df['Residual_error_2']))))\n",
        "print('-2<Error<2: '+ str(sum((i < 2 and i > -2 for i in df['Residual_error_2']))))\n",
        "print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]< 18:\n",
        "        FN += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]< 18:\n",
        "        TN += 1     \n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,2]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,2]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,2]<= 16:\n",
        "        minogashi += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,2]<= 16:\n",
        "        oknegative += 1     \n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x96i5oZDM0dm"
      },
      "source": [
        "#Bland-Altman-Plot using corrected value (平均値により補正)\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "\n",
        "corrected_estimate = df.loc[:,'Corrected_estimate_1']\n",
        "target = df.loc[:,'target']\n",
        "\n",
        "bland_altman_plot(corrected_estimate, target)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBFhobtCbv6t"
      },
      "source": [
        "#線形近似バージョン\n",
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df['Residual_error_2'], bins=13, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示\n",
        "\n",
        "\n",
        "#Draw Graphs\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='Corrected_estimate_2', y='target', data=df)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)\n",
        "\n",
        "corrected_AbsError = [abs(i) for i in df['Residual_error_2']]\n",
        "print('AveError: '+str(statistics.mean(df['Residual_error_2'])))\n",
        "print('StdError: '+str(statistics.stdev(df['Residual_error_2'])))\n",
        "print('AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "\n",
        "print('')\n",
        "print('-1<Error<1: '+ str(sum((i < 1 and i > -1 for i in df['Residual_error_2']))))\n",
        "print('-2<Error<2: '+ str(sum((i < 2 and i > -2 for i in df['Residual_error_2']))))\n",
        "print('Error<=-2: ' +  str(sum((i <= -2 for i in df['Residual_error_2']))))\n",
        "print('Error>=2: ' +  str(sum((i >= 2 for i in df['Residual_error_2']))))\n",
        "\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,3]>= 18:\n",
        "        TP += 1\n",
        "    if df.iloc[i,1]<18 and df.iloc[i,3]>= 18:\n",
        "        FP += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,3]< 18:\n",
        "        FN += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,3]< 18:\n",
        "        TN += 1     \n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,1]>=16 and df.iloc[i,3]> 18:\n",
        "        okpositive += 1\n",
        "    if df.iloc[i,1]<16 and df.iloc[i,3]>= 18:\n",
        "        kajyou += 1\n",
        "    if df.iloc[i,1]>=18 and df.iloc[i,3]<= 16:\n",
        "        minogashi += 1 \n",
        "    if df.iloc[i,1]<18 and df.iloc[i,3]<= 16:\n",
        "        oknegative += 1     \n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPJMCKTFqFnQ"
      },
      "source": [
        "#Bland-Altman-Plot using corrected value (線形近似により補正)\n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "\n",
        "corrected_estimate = df.loc[:,'Corrected_estimate_2']\n",
        "target = df.loc[:,'target']\n",
        "\n",
        "bland_altman_plot(corrected_estimate, target)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ensemble learning (Stacking)**"
      ],
      "metadata": {
        "id": "qf7jBK7E3ns_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training"
      ],
      "metadata": {
        "id": "0hfU2ZFPKwwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load backup models\n",
        "for area_num in [0,1,2]:\n",
        "    orig_path = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2_backup.pth\"\n",
        "    dst_path = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2.pth\"\n",
        "    shutil.copy(orig_path, dst_path)\n",
        "    print(f\"loading: {os.path.basename(dst_path)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgP6XtWzak6R",
        "outputId": "c3835c57-ba50-414f-8782-7ee5d8bad0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading: half_RepVGGA2.pth\n",
            "loading: periocular_RepVGGA2.pth\n",
            "loading: eye_RepVGGA2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#1つずつ解析するバージョン\n",
        "def train_model(model, loss_func, batch_size, optimizer, patience, n_epochs, device, alpha=0):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    # define scaler (for fastening)\n",
        "    scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor[:,0], image_tensor[:,1], image_tensor[:,2])  #16,3,3,224,224 --> 16,3,224,224 (バッチサイズの次の次元でスライスすることによりtensorを取り出す)\n",
        "            \n",
        "            with torch.cuda.amp.autocast(): \n",
        "                loss = loss_func(output, target)\n",
        "\n",
        "                ################\n",
        "                ##l2_normalization##\n",
        "                ################\n",
        "                l2 = torch.tensor(0., requires_grad=True)\n",
        "                for w in model.parameters():\n",
        "                    l2 = l2 + torch.norm(w)**2\n",
        "                loss = loss + alpha*l2\n",
        "\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            scaler.scale(loss).backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            scaler.step(optimizer) \n",
        "            scaler.update() \n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "       \n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:  \n",
        "            #target = target.squeeze(1)         \n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor[:,0], image_tensor[:,1], image_tensor[:,2])\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            #print(f\"val_output: {output}\")\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "        \n",
        "        print(print_msg)\n",
        "\n",
        "        \n",
        "        #Scheduler step for SGD\n",
        "        #scheduler.step() #val_lossが下がらなければ減衰\n",
        "        \n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "DZny1BiR3dsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select model"
      ],
      "metadata": {
        "id": "jjrVJYPyLJCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output(half, periocular, eye, 各1層)を連結してトレーニング"
      ],
      "metadata": {
        "id": "-Bvoufo1yr04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "##########################\n",
        "# Load model \n",
        "##########################\n",
        "area_num\n",
        "1: half \n",
        "2: periocular\n",
        "3: eye\n",
        "\"\"\"\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft = mod_RepVGG()\n",
        "model_ft.to(device)\n",
        "\n",
        "model_ft_half = model_ft\n",
        "model_ft_periocular = model_ft\n",
        "model_ft_eye = model_ft\n",
        "\n",
        "class Ensemble_three(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ensemble_three, self).__init__()\n",
        "        self.model_0 = model_ft_half\n",
        "        self.model_1 = model_ft_periocular\n",
        "        self.model_2 = model_ft_eye\n",
        "        self.fc = nn.Linear(in_features=3, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x0, x1, x2):\n",
        "        x0 = self.model_0(x0)\n",
        "        x1 = self.model_1(x1)\n",
        "        x2 = self.model_2(x2)\n",
        "        x = torch.cat([x0, x1, x2], dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSvjVD8U3n8T",
        "outputId": "d6fe3b14-6f09-40ba-b68a-30a7882da5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fc layer（各1408層）を直接連結してトレーニング（dropoutを追加）"
      ],
      "metadata": {
        "id": "wm0ywdRwy7VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "##########################\n",
        "# Load model \n",
        "##########################\n",
        "area_num\n",
        "1: half \n",
        "2: periocular\n",
        "3: eye\n",
        "\"\"\"\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft = mod_RepVGG()\n",
        "model_ft.to(device)\n",
        "\n",
        "model_ft_half = model_ft\n",
        "model_ft_periocular = model_ft\n",
        "model_ft_eye = model_ft\n",
        "\n",
        "class Ensemble_three(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ensemble_three, self).__init__()\n",
        "        model_0 = model_ft_half\n",
        "        model_1 = model_ft_periocular\n",
        "        model_2 = model_ft_eye\n",
        "        self.model_0 = nn.Sequential(*list(model_0.children())[:-1])\n",
        "        self.model_1 = nn.Sequential(*list(model_1.children())[:-1])\n",
        "        self.model_2 = nn.Sequential(*list(model_2.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc = nn.Linear(in_features=1408*3, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x0, x1, x2):\n",
        "        x0 = self.model_0(x0)\n",
        "        x0 = torch.flatten(x0, 1)\n",
        "        x1 = self.model_1(x1)\n",
        "        x1 = torch.flatten(x1, 1)\n",
        "        x2 = self.model_2(x2)\n",
        "        x2 = torch.flatten(x2, 1)\n",
        "        x = torch.cat([x0, x1, x2], dim=1)\n",
        "        x = self.dropout(x) #dropoutを1層追加\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FJSZRKuQzKvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### さらに全結合層を追加"
      ],
      "metadata": {
        "id": "XnSzr6k3jJms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "##########################\n",
        "# Load model \n",
        "##########################\n",
        "area_num\n",
        "1: half \n",
        "2: periocular\n",
        "3: eye\n",
        "\"\"\"\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft = mod_RepVGG()\n",
        "model_ft.to(device)\n",
        "\n",
        "model_ft_half = model_ft\n",
        "model_ft_periocular = model_ft\n",
        "model_ft_eye = model_ft\n",
        "\n",
        "class Ensemble_three(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ensemble_three, self).__init__()\n",
        "        model_0 = model_ft_half\n",
        "        model_1 = model_ft_periocular\n",
        "        model_2 = model_ft_eye\n",
        "        self.model_0 = nn.Sequential(*list(model_0.children())[:-1])\n",
        "        self.model_1 = nn.Sequential(*list(model_1.children())[:-1])\n",
        "        self.model_2 = nn.Sequential(*list(model_2.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc1 = nn.Linear(in_features=1408*3, out_features=20)\n",
        "        self.fc2 = nn.Linear(in_features=20, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x0, x1, x2):\n",
        "        x0 = self.model_0(x0)\n",
        "        x0 = torch.flatten(x0, 1)\n",
        "        x1 = self.model_1(x1)\n",
        "        x1 = torch.flatten(x1, 1)\n",
        "        x2 = self.model_2(x2)\n",
        "        x2 = torch.flatten(x2, 1)\n",
        "        x = torch.cat([x0, x1, x2], dim=1)\n",
        "        x = self.fc1(x) \n",
        "        x = self.dropout(x) #dropoutを1層追加\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4m8_K8siyes",
        "outputId": "37043144-c2d4-4948-972b-12b4c8e29f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ensemble model"
      ],
      "metadata": {
        "id": "Q4cFAvKSMHIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    del model_ft_ensemble\n",
        "    print(\"renewing modell_ft_ensemble...\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "model_ft_ensemble = Ensemble_three()\n",
        "model_ft_ensemble.to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "#optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "def read_path(area_num):\n",
        "    #ネットワークの読み込み\n",
        "    PATH = f\"./models_Hertel_estimation/{AREA[area_num]}_RepVGGA2.pth\"\n",
        "    print(\"...\")\n",
        "    print(f\"loading: {os.path.basename(PATH)}\")\n",
        "    return PATH\n",
        "\n",
        "model_ft_half = model_ft.load_state_dict(torch.load(read_path(0)))\n",
        "model_ft_periocular = model_ft.load_state_dict(torch.load(read_path(1)))\n",
        "model_ft_eye = model_ft.load_state_dict(torch.load(read_path(2)))\n",
        "\n",
        "# パラメータを固定\n",
        "\n",
        "# Fix model parameters\n",
        "for param in model_ft_ensemble.parameters():\n",
        "    param.requres_grad = False\n",
        "\n",
        "try:\n",
        "    # Let fc parameters rewritable\n",
        "    for param in model_ft_ensemble.fc1.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model_ft_ensemble.fc2.parameters():\n",
        "        param.requires_grad = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    # Let fc parameters rewritable\n",
        "    for param in model_ft_ensemble.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCiRr7ss_G3i",
        "outputId": "962fff88-b173-49a3-c061-b9ee18b76718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "renewing modell_ft_ensemble...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ranger_adabelief in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from ranger_adabelief) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.1.1)\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "...\n",
            "loading: half_RepVGGA2.pth\n",
            "...\n",
            "loading: periocular_RepVGGA2.pth\n",
            "...\n",
            "loading: eye_RepVGGA2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model, train_loss, valid_loss = train_model(model_ft_ensemble, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n"
      ],
      "metadata": {
        "id": "aI1dpdwc2VSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "PATH = f\"./models_Hertel_estimation/fc_ensemble.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "backup_path = f\"./models_Hertel_estimation/fc_ensemble_backup.pth\"\n",
        "shutil.copy(PATH, backup_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "suqWif1v2VU_",
        "outputId": "43ff8145-ab18-4a4c-a163-78b2e2071d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./models_Hertel_estimation/fc_ensemble_backup.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = f\"./models_Hertel_estimation/fc_ensemble.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQwmK8WDnfOh",
        "outputId": "ae2ed462-7aea-4386-865a-8bcf73e3d1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 全ての層を書き換え可能にしてfine tuningする\n",
        "\n",
        "# Let all model parameters rewritable\n",
        "for param in model_ft_ensemble.parameters():\n",
        "    param.requres_grad = True\n",
        "\n",
        "# Train model\n",
        "model, train_loss, valid_loss = train_model(model_ft_ensemble, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n"
      ],
      "metadata": {
        "id": "sNvccMAV0gT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3634sCn0gpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation using validation dataset\n",
        "\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = Decimal(str(x)).quantize(Decimal(str(1/10**d)), rounding=ROUND_HALF_UP)\n",
        "    p = float(p)\n",
        "    return p\n",
        "\n",
        "\n",
        "model_ft_ensemble.eval() # prep model for evaluation\n",
        "\n",
        "outputs,targets,errors =[], [], []\n",
        "for image_tensor, target in test_loader:  \n",
        "      target = target.view(len(target), 1)         \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft_ensemble(image_tensor[:,0], image_tensor[:,1], image_tensor[:,2]) #dim0はbach_size、dim1がarea_num\n",
        "\n",
        "      outputs.append(output[0].item())      \n",
        "      targets.append(target[0].item())\n",
        "      print(f\"estimate: {my_round(output[0].item())} mm, target: {target[0].item()} mm\")\n",
        "\n",
        "      errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYzQCiD02VXJ",
        "outputId": "008c8112-bfae-4e1e-cf1a-8e197b79bc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estimate: 13.93 mm, target: 16.0 mm\n",
            "estimate: 10.97 mm, target: 12.0 mm\n",
            "estimate: 14.95 mm, target: 16.0 mm\n",
            "estimate: 14.16 mm, target: 16.0 mm\n",
            "estimate: 13.76 mm, target: 15.0 mm\n",
            "estimate: 17.25 mm, target: 19.0 mm\n",
            "estimate: 16.1 mm, target: 19.0 mm\n",
            "estimate: 11.14 mm, target: 12.0 mm\n",
            "estimate: 19.53 mm, target: 20.0 mm\n",
            "estimate: 15.94 mm, target: 19.0 mm\n",
            "estimate: 12.18 mm, target: 14.0 mm\n",
            "estimate: 19.04 mm, target: 20.0 mm\n",
            "estimate: 24.27 mm, target: 25.0 mm\n",
            "estimate: 12.08 mm, target: 13.0 mm\n",
            "estimate: 8.55 mm, target: 9.0 mm\n",
            "estimate: 18.41 mm, target: 21.0 mm\n",
            "estimate: 19.33 mm, target: 23.0 mm\n",
            "estimate: 16.5 mm, target: 20.0 mm\n",
            "estimate: 17.09 mm, target: 19.0 mm\n",
            "estimate: 16.17 mm, target: 20.0 mm\n",
            "estimate: 14.19 mm, target: 16.0 mm\n",
            "estimate: 17.25 mm, target: 21.0 mm\n",
            "estimate: 15.87 mm, target: 18.0 mm\n",
            "estimate: 19.1 mm, target: 21.0 mm\n",
            "estimate: 21.71 mm, target: 24.0 mm\n",
            "estimate: 13.35 mm, target: 14.0 mm\n",
            "estimate: 17.33 mm, target: 20.0 mm\n",
            "estimate: 15.82 mm, target: 17.0 mm\n",
            "estimate: 15.41 mm, target: 16.0 mm\n",
            "estimate: 13.79 mm, target: 16.0 mm\n",
            "estimate: 17.01 mm, target: 19.0 mm\n",
            "estimate: 13.32 mm, target: 15.0 mm\n",
            "estimate: 12.81 mm, target: 15.0 mm\n",
            "estimate: 12.59 mm, target: 13.0 mm\n",
            "estimate: 13.91 mm, target: 18.0 mm\n",
            "estimate: 13.53 mm, target: 14.0 mm\n",
            "estimate: 17.63 mm, target: 19.0 mm\n",
            "estimate: 13.22 mm, target: 16.0 mm\n",
            "estimate: 14.56 mm, target: 16.0 mm\n",
            "estimate: 20.9 mm, target: 23.0 mm\n",
            "estimate: 14.36 mm, target: 16.0 mm\n",
            "estimate: 15.98 mm, target: 19.0 mm\n",
            "estimate: 16.82 mm, target: 18.0 mm\n",
            "estimate: 12.6 mm, target: 14.0 mm\n",
            "estimate: 15.5 mm, target: 17.0 mm\n",
            "estimate: 16.34 mm, target: 15.0 mm\n",
            "estimate: 15.04 mm, target: 17.0 mm\n",
            "estimate: 14.96 mm, target: 17.0 mm\n",
            "estimate: 17.02 mm, target: 19.0 mm\n",
            "estimate: 16.26 mm, target: 18.0 mm\n",
            "estimate: 16.23 mm, target: 18.0 mm\n",
            "estimate: 11.85 mm, target: 13.0 mm\n",
            "estimate: 14.37 mm, target: 15.0 mm\n",
            "estimate: 11.32 mm, target: 12.0 mm\n",
            "estimate: 16.57 mm, target: 18.0 mm\n",
            "estimate: 13.66 mm, target: 16.0 mm\n",
            "estimate: 12.67 mm, target: 15.0 mm\n",
            "estimate: 14.34 mm, target: 15.0 mm\n",
            "estimate: 17.38 mm, target: 21.0 mm\n",
            "estimate: 14.18 mm, target: 15.0 mm\n",
            "estimate: 14.21 mm, target: 16.0 mm\n",
            "estimate: 17.12 mm, target: 18.0 mm\n",
            "estimate: 13.31 mm, target: 15.0 mm\n",
            "estimate: 18.99 mm, target: 21.0 mm\n",
            "estimate: 12.31 mm, target: 14.0 mm\n",
            "estimate: 16.05 mm, target: 18.0 mm\n",
            "estimate: 13.6 mm, target: 15.0 mm\n",
            "estimate: 18.26 mm, target: 19.0 mm\n",
            "estimate: 12.91 mm, target: 13.0 mm\n",
            "estimate: 16.71 mm, target: 17.0 mm\n",
            "estimate: 17.58 mm, target: 20.0 mm\n",
            "estimate: 11.87 mm, target: 13.0 mm\n",
            "estimate: 15.77 mm, target: 17.0 mm\n",
            "estimate: 14.45 mm, target: 16.0 mm\n",
            "estimate: 18.1 mm, target: 19.0 mm\n",
            "estimate: 19.01 mm, target: 21.0 mm\n",
            "estimate: 20.7 mm, target: 22.0 mm\n",
            "estimate: 16.74 mm, target: 18.0 mm\n",
            "estimate: 20.66 mm, target: 24.0 mm\n",
            "estimate: 14.64 mm, target: 16.0 mm\n",
            "estimate: 13.45 mm, target: 15.0 mm\n",
            "estimate: 11.8 mm, target: 12.0 mm\n",
            "estimate: 14.13 mm, target: 16.0 mm\n",
            "estimate: 18.62 mm, target: 20.5 mm\n",
            "estimate: 18.3 mm, target: 20.0 mm\n",
            "estimate: 13.69 mm, target: 15.0 mm\n",
            "estimate: 18.5 mm, target: 20.0 mm\n",
            "estimate: 14.13 mm, target: 14.0 mm\n",
            "estimate: 18.52 mm, target: 20.0 mm\n",
            "estimate: 13.96 mm, target: 17.0 mm\n",
            "estimate: 13.72 mm, target: 15.0 mm\n",
            "estimate: 15.07 mm, target: 18.0 mm\n",
            "estimate: 15.78 mm, target: 17.0 mm\n",
            "estimate: 13.48 mm, target: 16.0 mm\n",
            "estimate: 16.22 mm, target: 13.0 mm\n",
            "estimate: 14.15 mm, target: 16.0 mm\n",
            "estimate: 14.23 mm, target: 15.0 mm\n",
            "estimate: 15.26 mm, target: 18.0 mm\n",
            "estimate: 19.61 mm, target: 20.0 mm\n",
            "estimate: 15.78 mm, target: 18.0 mm\n",
            "estimate: 14.82 mm, target: 16.0 mm\n",
            "estimate: 17.4 mm, target: 19.0 mm\n",
            "estimate: 14.03 mm, target: 15.0 mm\n",
            "estimate: 19.36 mm, target: 21.0 mm\n",
            "estimate: 17.0 mm, target: 20.0 mm\n",
            "estimate: 17.45 mm, target: 20.0 mm\n",
            "estimate: 17.02 mm, target: 19.0 mm\n",
            "estimate: 17.1 mm, target: 20.0 mm\n",
            "estimate: 17.37 mm, target: 19.0 mm\n",
            "estimate: 18.98 mm, target: 21.0 mm\n",
            "estimate: 13.77 mm, target: 15.0 mm\n",
            "estimate: 15.83 mm, target: 17.0 mm\n",
            "estimate: 15.53 mm, target: 20.0 mm\n",
            "estimate: 13.25 mm, target: 15.0 mm\n",
            "estimate: 18.04 mm, target: 20.0 mm\n",
            "estimate: 15.78 mm, target: 17.0 mm\n",
            "estimate: 16.41 mm, target: 19.0 mm\n",
            "estimate: 14.94 mm, target: 20.0 mm\n",
            "estimate: 15.46 mm, target: 17.0 mm\n",
            "estimate: 17.32 mm, target: 19.0 mm\n",
            "estimate: 15.5 mm, target: 17.0 mm\n",
            "estimate: 15.62 mm, target: 21.0 mm\n",
            "estimate: 15.35 mm, target: 17.0 mm\n",
            "estimate: 18.28 mm, target: 21.0 mm\n",
            "estimate: 14.1 mm, target: 15.0 mm\n",
            "estimate: 11.5 mm, target: 12.0 mm\n",
            "estimate: 16.04 mm, target: 17.0 mm\n",
            "estimate: 14.74 mm, target: 17.0 mm\n",
            "estimate: 17.48 mm, target: 19.0 mm\n",
            "estimate: 15.33 mm, target: 17.0 mm\n",
            "estimate: 12.54 mm, target: 14.0 mm\n",
            "estimate: 15.8 mm, target: 17.0 mm\n",
            "estimate: 15.38 mm, target: 17.0 mm\n",
            "estimate: 14.67 mm, target: 15.0 mm\n",
            "estimate: 13.62 mm, target: 14.0 mm\n",
            "estimate: 16.57 mm, target: 18.0 mm\n",
            "estimate: 9.27 mm, target: 9.0 mm\n",
            "estimate: 16.91 mm, target: 18.0 mm\n",
            "estimate: 11.57 mm, target: 12.0 mm\n",
            "estimate: 12.77 mm, target: 14.0 mm\n",
            "estimate: 15.93 mm, target: 17.0 mm\n",
            "estimate: 13.79 mm, target: 15.0 mm\n",
            "estimate: 10.37 mm, target: 11.0 mm\n",
            "estimate: 17.84 mm, target: 20.0 mm\n",
            "estimate: 18.99 mm, target: 18.0 mm\n",
            "estimate: 12.52 mm, target: 14.0 mm\n",
            "estimate: 17.55 mm, target: 19.0 mm\n",
            "estimate: 15.55 mm, target: 17.0 mm\n",
            "estimate: 17.94 mm, target: 19.0 mm\n",
            "estimate: 14.94 mm, target: 17.0 mm\n",
            "estimate: 15.33 mm, target: 15.0 mm\n",
            "estimate: 15.66 mm, target: 17.0 mm\n",
            "estimate: 15.96 mm, target: 16.0 mm\n",
            "estimate: 11.73 mm, target: 12.0 mm\n",
            "estimate: 17.28 mm, target: 18.0 mm\n",
            "estimate: 21.1 mm, target: 23.0 mm\n",
            "estimate: 13.4 mm, target: 15.0 mm\n",
            "estimate: 10.18 mm, target: 12.0 mm\n",
            "estimate: 16.71 mm, target: 19.0 mm\n",
            "estimate: 13.57 mm, target: 14.0 mm\n",
            "estimate: 13.16 mm, target: 15.0 mm\n",
            "estimate: 11.56 mm, target: 15.0 mm\n",
            "estimate: 12.72 mm, target: 15.0 mm\n",
            "estimate: 10.27 mm, target: 11.0 mm\n",
            "estimate: 10.06 mm, target: 10.0 mm\n",
            "estimate: 12.11 mm, target: 13.0 mm\n",
            "estimate: 11.36 mm, target: 12.0 mm\n",
            "estimate: 15.22 mm, target: 17.0 mm\n",
            "estimate: 15.44 mm, target: 17.0 mm\n",
            "estimate: 15.46 mm, target: 17.0 mm\n",
            "estimate: 16.01 mm, target: 20.0 mm\n",
            "estimate: 15.3 mm, target: 17.0 mm\n",
            "estimate: 16.39 mm, target: 17.0 mm\n",
            "estimate: 12.52 mm, target: 14.0 mm\n",
            "estimate: 15.84 mm, target: 18.0 mm\n",
            "estimate: 12.57 mm, target: 14.0 mm\n",
            "estimate: 12.83 mm, target: 16.0 mm\n",
            "estimate: 13.62 mm, target: 15.0 mm\n",
            "estimate: 17.45 mm, target: 20.0 mm\n",
            "estimate: 14.81 mm, target: 16.0 mm\n",
            "estimate: 14.18 mm, target: 16.0 mm\n",
            "estimate: 19.02 mm, target: 20.0 mm\n",
            "estimate: 15.39 mm, target: 17.0 mm\n",
            "estimate: 15.1 mm, target: 18.0 mm\n",
            "estimate: 14.73 mm, target: 18.0 mm\n",
            "estimate: 20.03 mm, target: 18.0 mm\n",
            "estimate: 17.78 mm, target: 19.0 mm\n",
            "estimate: 12.68 mm, target: 12.0 mm\n",
            "estimate: 14.38 mm, target: 14.0 mm\n",
            "estimate: 13.59 mm, target: 13.0 mm\n",
            "estimate: 14.43 mm, target: 17.0 mm\n",
            "estimate: 14.96 mm, target: 17.0 mm\n",
            "estimate: 12.54 mm, target: 15.0 mm\n",
            "estimate: 12.96 mm, target: 13.0 mm\n",
            "estimate: 15.68 mm, target: 10.0 mm\n",
            "estimate: 16.33 mm, target: 18.0 mm\n",
            "AveError: -1.522510372862524\n",
            "StdError: 1.2268599775878566\n",
            "AveAbsError: 1.6827616983530473\n",
            "StdAbsError: 0.9944356232046988\n",
            "\n",
            "Corrected_AveAbsError: 0.8255372273827631\n",
            "Corrected_StdAbsError: 0.9056372115513152\n",
            "Round_Corrected_AveAbsError: 0.8254081632653061\n",
            "Round_Corrected_StdAbsError: 0.9057810550723012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw Graphs（もともとの散布図\n",
        "df_ensemble = pd.DataFrame({'estimate':outputs, 'target':targets})\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "sns.lmplot(x='estimate', y='target', data=df_ensemble)\n",
        "plt.xlim(10,24)\n",
        "plt.ylim(10,24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "Li8BDOr12VZa",
        "outputId": "53b26515-c71a-462f-b7c9-7ca1e79e7694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10.0, 24.0)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw histogram\n",
        "sns.distplot(\n",
        "    df_ensemble['target']-df_ensemble['estimate'], bins=15, color='#123456', label='residual_error',\n",
        "    kde=False,\n",
        "    rug=False\n",
        ")\n",
        "plt.legend() # 凡例を表示\n",
        "plt.show()   # ヒストグラムを表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "DA4wwJtT2Vbj",
        "outputId": "55389ae8-9eea-49d5-e45a-84265ee8e42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(df_ensemble)\n",
        "within1 = sum((i <= 1 and i >= -1 for i in df_ensemble['estimate']-df_ensemble['target']))\n",
        "within2 = sum((i <= 2 and i >= -2 for i in df_ensemble['estimate']-df_ensemble['target']))\n",
        "over2 = sum((i > 2 or i < -2 for i in df['estimate']-df['target']))\n",
        "\n",
        "print(f'-1<Error<1: {within1}, ({my_round(within1/total*100)}%)')\n",
        "print(f'-2<Error<2: {within1}, ({my_round(within2/total*100)}%)')\n",
        "print(f'Error over 2: {within1}, ({my_round(over2/total*100)}%)')\n",
        "\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "for i in range(len(df_ensemble)):\n",
        "    if df_ensemble.iloc[i,0]>=18 and df_ensemble.iloc[i,1]>= 18:\n",
        "        TP += 1\n",
        "    if df_ensemble.iloc[i,0]<18 and df_ensemble.iloc[i,1]>= 18:\n",
        "        FN += 1\n",
        "    if df_ensemble.iloc[i,0]>=18 and df_ensemble.iloc[i,1]< 18:\n",
        "        FP += 1 \n",
        "    if df_ensemble.iloc[i,0]<18 and df_ensemble.iloc[i,1]< 18:\n",
        "        TN += 1     \n",
        "\n",
        "print('')\n",
        "print('Hertel 18mm以上の検出精度')\n",
        "print('TP: '+str(TP))\n",
        "print('FP: '+str(FP))\n",
        "print('FN: '+str(FN))\n",
        "print('TN: '+str(TN))\n",
        "print('Sensitivity: '+str(TP/(TP+FN)))\n",
        "print('Specificity: '+str(TN/(FP+TN)))\n",
        "print('Positive predictive value: '+str(TP/(TP+FP)))\n",
        "print('Negative predictive value: '+str(TN/(TN+FN)))\n",
        "\n",
        "\n",
        "okpositive, minogashi, oknegative, kajyou = 0,0,0,0\n",
        "for i in range(len(df_ensemble)):\n",
        "    if df_ensemble.iloc[i,0]>=16 and df_ensemble.iloc[i,1]> 18:\n",
        "        okpositive += 1\n",
        "    if df_ensemble.iloc[i,0]<16 and df_ensemble.iloc[i,1]>= 18:\n",
        "        minogashi += 1\n",
        "    if df_ensemble.iloc[i,0]>=18 and df_ensemble.iloc[i,1]<= 16:\n",
        "        kajyou += 1 \n",
        "    if df_ensemble.iloc[i,0]<18 and df_ensemble.iloc[i,1]<= 16:\n",
        "        oknegative += 1     \n",
        "\n",
        "print('')\n",
        "print('推測18mm以上だが実は16mm未満(過剰): '+str(kajyou)+'例')\n",
        "print('推測16mm未満だが実は18mm以上（見逃がし）: '+str(minogashi)+'例')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC5JBKBJ2Vdp",
        "outputId": "138f3ffd-c30c-4fe5-af74-cc4d0d786bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1<Error<1: 48, (24.49%)\n",
            "-2<Error<2: 48, (71.43%)\n",
            "Error over 2: 48, (15.31%)\n",
            "\n",
            "Hertel 18mm以上の検出精度\n",
            "TP: 27\n",
            "FP: 0\n",
            "FN: 52\n",
            "TN: 117\n",
            "Sensitivity: 0.34177215189873417\n",
            "Specificity: 1.0\n",
            "Positive predictive value: 1.0\n",
            "Negative predictive value: 0.6923076923076923\n",
            "\n",
            "推測18mm以上だが実は16mm未満(過剰): 0例\n",
            "推測16mm未満だが実は18mm以上（見逃がし）: 13例\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bland-Altman-Plot \n",
        "\n",
        "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
        "    data1     = np.asarray(data1)\n",
        "    data2     = np.asarray(data2)\n",
        "    mean      = np.mean([data1, data2], axis=0)\n",
        "    diff      = data1 - data2                   # Difference between data1 and data2\n",
        "    md        = np.mean(diff)                   # Mean of the difference\n",
        "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
        "    plt.scatter(mean, diff, *args, **kwargs)\n",
        "    plt.axhline(md,           color='gray', linestyle='--')\n",
        "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
        "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
        "\n",
        "bland_altman_plot(outputs, targets)\n",
        "plt.title('Bland-Altman Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "3bdeNzKQ2Vfz",
        "outputId": "60708a03-8ea1-4b18-bf2a-623edf358495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Average models**\n",
        "\n",
        "パラメータをaverageしても正しい推測にはならないので却下。"
      ],
      "metadata": {
        "id": "L1BbFRZWcTlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_parent_path = f\"C:/Users/ykita/OneDrive/デスクトップ/Hertel_dataset/models_Hertel_estimation/5-fold-crossvalidation\"\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft[fold]\n",
        "        self.repVGG = nn.Sequential(*list(model_ft[fold].children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc = nn.Linear(in_features=1408, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x) #dropoutを1層追加\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def avg_model_params(modelA, modelB, modelC, modelD, modelE, multi=1/5):\n",
        "    \"\"\" modelA + modelB \"\"\"\n",
        "    sdA = modelA.state_dict()\n",
        "    sdB = modelB.state_dict()\n",
        "    sdC = modelC.state_dict()\n",
        "    sdD = modelD.state_dict()\n",
        "    sdE = modelE.state_dict()\n",
        "    for key in sdA:\n",
        "        sdA[key] = (sdB[key] + sdA[key] +sdC[key] + sdD[key] + sdE[key])*multi\n",
        "    modelA.load_state_dict(sdA)\n",
        "    return modelA\n",
        "\n",
        "\n",
        "model_ft =  [0]*5\n",
        "for fold in [0,1,2,3,4]:\n",
        "    model_ft[fold] = create_RepVGG_A2(deploy=False) \n",
        "    model_ft[fold] = mod_RepVGG()\n",
        "    model_ft[fold] = model_ft[fold].to(device)\n",
        "    model_ft[fold].load_state_dict(torch.load(f\"{model_parent_path}/eye_fold{fold}_RepVGGA2.pth\"))\n",
        "\n",
        "model_ft[0] = avg_model_params(model_ft[0], model_ft[1], model_ft[2], model_ft[3], model_ft[4], 1/5)\n"
      ],
      "metadata": {
        "id": "dPWCGFW2iBrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference on single model\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "model_ft[0].eval() # prep model for evaluation\n",
        "print(f\"model_path: {model_path}\")\n",
        "\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "area_num = 2 #[half, periocular, eye]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in test_loader:  \n",
        "          target = target.view(len(target), 1)         \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft[0](image_tensor[:,area_num]) #dim0はbach_size、dim1がarea_num\n",
        "\n",
        "          outputs.append(output[0].item())      \n",
        "          targets.append(target[0].item())\n",
        "          print(f\"estimate: {my_round(output[0].item())} mm, target: {target[0].item()} mm\")\n",
        "\n",
        "          errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))"
      ],
      "metadata": {
        "id": "2X_p1oJN4yp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Explainable**"
      ],
      "metadata": {
        "id": "63gcPOmbDPNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = r\"C:\\Users\\ykita\\OneDrive\\デスクトップ\\Hertel_dataset\\models_Hertel_estimation\\5-fold-crossvalidation\\eye_fold0_RepVGGA2.pth\"\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.dropout = nn.Dropout(0.25) #Define proportion or neurons to dropout\n",
        "        self.fc = nn.Linear(in_features=1408, out_features=1) #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x) #dropoutを1層追加\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False) \n",
        "model_ft = mod_RepVGG()\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)"
      ],
      "metadata": {
        "id": "ZZ6K3ah3_G7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5j1lepugOBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference on single model\n",
        "\n",
        "def my_round(x, d=2):\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False) \n",
        "model_ft = mod_RepVGG()\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "model_ft.eval() # prep model for evaluation\n",
        "print(f\"model_path: {model_path}\")\n",
        "\n",
        "test_dataset = Create_Datasets(path_list_list, test_idx, CSV_PATH, val_data_transforms) \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)\n",
        "\n",
        "area_num = 2 #[half, periocular, eye]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in test_loader:  \n",
        "          target = target.view(len(target), 1)         \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft(image_tensor[:,area_num]) #dim0はbach_size、dim1がarea_num\n",
        "\n",
        "          outputs.append(output[0].item())      \n",
        "          targets.append(target[0].item())\n",
        "          print(f\"estimate: {my_round(output[0].item())} mm, target: {target[0].item()} mm\")\n",
        "\n",
        "          errors.append(output[0].item()-target[0].item())\n",
        "\n",
        "AbsError = [abs(i) for i in errors]\n",
        "\n",
        "print('AveError: '+str(statistics.mean(errors)))\n",
        "print('StdError: '+str(statistics.stdev(errors)))\n",
        "print('AveAbsError: '+str(statistics.mean(AbsError)))\n",
        "print('StdAbsError: '+str(statistics.stdev(AbsError)))\n",
        "print('')\n",
        "\n",
        "\n",
        "#平均からの差分を補正\n",
        "corrected_output = (np.array(outputs)-np.array(statistics.mean(errors))).tolist()\n",
        "corrected_error = (np.array(corrected_output)-np.array(targets)).tolist()\n",
        "corrected_AbsError = [abs(i) for i in corrected_error]\n",
        "\n",
        "round_output = [my_round(i) for i in outputs]\n",
        "round_corrected_AbsError = [my_round(i) for i in corrected_AbsError]\n",
        "\n",
        "print('Corrected_AveAbsError: '+str(statistics.mean(corrected_AbsError)))\n",
        "print('Corrected_StdAbsError: '+str(statistics.stdev(corrected_AbsError)))\n",
        "print('Round_Corrected_AveAbsError: '+str(statistics.mean(round_corrected_AbsError)))\n",
        "print('Round_Corrected_StdAbsError: '+str(statistics.stdev(round_corrected_AbsError)))"
      ],
      "metadata": {
        "id": "mnaSQqFiXFAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['number']==str(\"1001_R\")].iloc[0,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqj3x0n1aij6",
        "outputId": "72b60805-e35b-4562-b11a-bbbf9e66939d"
      },
      "execution_count": 637,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.0"
            ]
          },
          "metadata": {},
          "execution_count": 637
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = f\"./dataset_250px_uni_eye/1001_R.JPG\"\n",
        "base_name = os.path.splitext(os.path.basename(path))[0] #フォルダより画像番号を抜き出す\n",
        "hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す"
      ],
      "metadata": {
        "id": "r8YjpYnMa9AV"
      },
      "execution_count": 640,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference several images\n",
        "%matplotlib inline\n",
        "\n",
        "#Datasetの読み込み\n",
        "image_path_list = glob.glob(f\"{DATASET_PATH_2}/*\")[0:2]\n",
        "csv_path = CSV_PATH\n",
        "\n",
        "\n",
        "#ラベルの読み込み\n",
        "with codecs.open(csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "targets, outputs = [], []\n",
        "for path in image_path_list:\n",
        "    print(path)\n",
        "    base_name = os.path.splitext(os.path.basename(path))[0] #フォルダより画像番号を抜き出す\n",
        "    hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "    targets.append(hertel) \n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "    return(image_tensor)\n",
        "\n",
        "#Interference\n",
        "model_ft = create_RepVGG_A2(deploy=False) \n",
        "model_ft = mod_RepVGG()\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "model_ft.eval() # prep model for evaluation\n",
        "print(f\"model_path: {model_path}\")\n",
        " \n",
        "for img in image_path_list:\n",
        "      Image.register_open\n",
        "      with torch.inference_mode():\n",
        "          image_tensor = image_transform(img)  \n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          outputs.append(model_ft(image_tensor).item()) \n",
        "\n",
        "for path, target, output in zip(image_path_list, targets, outputs):\n",
        "    imgPIL = Image.open(path)  # 画像読み込み\n",
        "    plt.imshow(imgPIL)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print(f\"target: {target}, output: {output}\")\n"
      ],
      "metadata": {
        "id": "A2O0ls2_U-Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference modified images\n",
        "dataset_path = f\"C:/Users/ykita/OneDrive/デスクトップ/Hertel_dataset/dataset_250px_uni_eye_modified\"\n",
        "\n",
        "#Datasetの読み込み\n",
        "image_path_list = glob.glob(f\"{dataset_path}/*\")\n",
        "csv_path = CSV_PATH\n",
        "\n",
        "\n",
        "#ラベルの読み込み\n",
        "with codecs.open(csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "path = image_path_list[0]\n",
        "os.path.splitext(os.path.basename(path))[0].split(\".\")[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub8oRno2TLVV",
        "outputId": "3facc0ee-2485-4b81-c202-8ff5ef26df77"
      },
      "execution_count": 669,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1_L'"
            ]
          },
          "metadata": {},
          "execution_count": 669
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference modified images\n",
        "dataset_path = f\"C:/Users/ykita/OneDrive/デスクトップ/Hertel_dataset/dataset_250px_uni_eye_modified\"\n",
        "\n",
        "#Datasetの読み込み\n",
        "image_path_list = glob.glob(f\"{dataset_path}/*\")\n",
        "csv_path = CSV_PATH\n",
        "\n",
        "\n",
        "#ラベルの読み込み\n",
        "with codecs.open(csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "targets, outputs = [], []\n",
        "for path in image_path_list:\n",
        "    print(path)\n",
        "    base_name = os.path.splitext(os.path.basename(path))[0].split(\".\")[0] #フォルダより画像番号を抜き出す\n",
        "    hertel = df[df['number']==str(base_name)].iloc[0,1] #CSV上で一致した番号の画像についてHertel値を抜き出す\n",
        "    targets.append(hertel) \n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "    return(image_tensor)\n",
        "\n",
        "#Interference\n",
        "model_ft = create_RepVGG_A2(deploy=False) \n",
        "model_ft = mod_RepVGG()\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.load_state_dict(torch.load(model_path))\n",
        "model_ft.eval() # prep model for evaluation\n",
        "print(f\"model_path: {model_path}\")\n",
        " \n",
        "for img in image_path_list:\n",
        "      Image.register_open\n",
        "      with torch.inference_mode():\n",
        "          image_tensor = image_transform(img)  \n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          outputs.append(model_ft(image_tensor).item()) \n",
        "\n",
        "for path, target, output in zip(image_path_list, targets, outputs):\n",
        "    imgPIL = Image.open(path)  # 画像読み込み\n",
        "    plt.imshow(imgPIL)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print(f\"target: {target}, output: {output}\")\n",
        "    print(f\"{os.path.basename(path)}\")"
      ],
      "metadata": {
        "id": "F2iBxGh80N6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nhtafWpTFtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}