{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/5-fold_test_validation_efficientNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Wsoz46h1E-"
      },
      "source": [
        "#**GravCont_250 5-fold cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSVzemJXhpnA",
        "outputId": "14f15f81-cda4-43b0-ba93-5d5e86572278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 390 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.11.0+cu113)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (4.1.1)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "random_seed = 2 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "#GDriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ITI3BuQXiLVq"
      },
      "outputs": [],
      "source": [
        "glav_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/gravcont_250px/grav\"\n",
        "cont_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/gravcont_250px/cont\"\n",
        "pretrained_model_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/RepVGG-A2.pth\"\n",
        "gradcam_folder_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/GradCam_EfficientNetB4_test_20220628\"\n",
        "result_csv_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/result_EfficientNetB4_test_20220628.csv\"\n",
        "confusion_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/confusion_EfficientNetB4_test_20220628.png\"\n",
        "ROC_path = \"/content/drive/MyDrive/Deep_learning/666mai_dataset/ROC_EfficientNetB4_test_20220628.png\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(gradcam_folder_path):\n",
        "    shutil.rmtree(gradcam_folder_path) \n",
        "os.makedirs(gradcam_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "8LhOc6OjJQEt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ODB-njjTzV",
        "outputId": "4a961aa8-762f-4406-ba3a-907d30736c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 333, cont: 333\n"
          ]
        }
      ],
      "source": [
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    id_list = [re.split('[-_]',os.path.basename(name))[0] for name in path_list]\n",
        "    #id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "\n",
        "grav_path_list = make_path_list(glav_path)\n",
        "cont_path_list = make_path_list(cont_path)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "grav_id = extract_ids(grav_path_list)\n",
        "cont_id = extract_ids(cont_path_list)\n",
        "\n",
        "print(\"grav: {}, cont: {}\".format(len(grav_id), len(cont_id)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ddvc4-rfsnY"
      },
      "source": [
        "#**5-Foldに分割**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmvLpuwnkEzE",
        "outputId": "c0357815-a330-46ae-dd30-d924d0c8490c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239\n",
            "60\n",
            "239\n",
            "60\n",
            "34\n",
            "34\n"
          ]
        }
      ],
      "source": [
        "num_folds = 5 #number of folds\n",
        "\n",
        "train_dataset_grav, val_dataset_grav, train_dataset_cont, val_dataset_cont =  [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)], [[] for i in range(0, num_folds)]\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "#まず全体の1割をテストセットとしてよけておく\n",
        "remain_dataset_cont, test_dataset_cont = train_test_split(cont_path_list, test_size=0.1, shuffle=True, random_state=random_seed) \n",
        "remain_dataset_grav, test_dataset_grav = train_test_split(grav_path_list, test_size=0.1, shuffle=True, random_state=random_seed) \n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_dataset_cont):\n",
        "    for idx in train_idxs:\n",
        "        train_dataset_cont[i].append(remain_dataset_cont[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_dataset_cont[i].append(remain_dataset_cont[idx])\n",
        "    i+=1\n",
        "\n",
        "i=0\n",
        "for train_idxs, val_idxs in kf.split(remain_dataset_grav):\n",
        "    for idx in train_idxs:\n",
        "        train_dataset_grav[i].append(remain_dataset_grav[idx])\n",
        "    for idx in val_idxs:\n",
        "        val_dataset_grav[i].append(remain_dataset_grav[idx])\n",
        "    i+=1\n",
        "\n",
        "print(len(train_dataset_grav[0]))    \n",
        "print(len(val_dataset_grav[0]))\n",
        "print(len(train_dataset_cont[0]))    \n",
        "print(len(val_dataset_cont[0]))\n",
        "print(len(test_dataset_cont))\n",
        "print(len(test_dataset_grav))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQhcDlAVhQ0t"
      },
      "source": [
        "#**Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5q3bnqlpHlF",
        "outputId": "10f07abe-926f-41ef-d3f7-87c7b510e0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pretrained repVGG model already exists\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ranger_adabelief\n",
            "  Downloading ranger_adabelief-0.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from ranger_adabelief) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.1.1)\n",
            "Installing collected packages: ranger-adabelief\n",
            "Successfully installed ranger-adabelief-0.1.0\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "478\n",
            "120\n",
            "68\n"
          ]
        }
      ],
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])      \n",
        "        return tensor_image, target\n",
        "\n",
        "#画像読み込み時間削減のため、Expand2squareの処理は行っている\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "        \n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            \n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "            \n",
        "            \n",
        "          \n",
        "\n",
        "            \"\"\"\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \"\"\"\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "        #print()   \n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        #####################\n",
        "        # test the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, test_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        p=0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        test_acc = running_corrects.item()/len(test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(num_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}' +'\\n'\n",
        "                     f'test_acc: {test_acc:.5f}' + f'({running_corrects:.0f}/{len(test_dataset):.0f})') \n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Define RepVGG\n",
        "##############################################\n",
        "\n",
        "import requests\n",
        "\n",
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "#RepVGGのpretrained modelをダウンロード\n",
        "# def download_file_from_google_drive(id, destination):\n",
        "#     URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "#     session = requests.Session()\n",
        "\n",
        "#     response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "#     token = get_confirm_token(response)\n",
        "\n",
        "#     if token:\n",
        "#         params = { 'id' : id, 'confirm' : token }\n",
        "#         response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "#     save_response_content(response, destination)    \n",
        "\n",
        "# def get_confirm_token(response):\n",
        "#     for key, value in response.cookies.items():\n",
        "#         if key.startswith('download_warning'):\n",
        "#             return value\n",
        "\n",
        "#     return None\n",
        "\n",
        "# def save_response_content(response, destination):\n",
        "#     CHUNK_SIZE = 32768\n",
        "\n",
        "#     with open(destination, \"wb\") as f:\n",
        "#         for chunk in response.iter_content(CHUNK_SIZE):\n",
        "#             if chunk: # filter out keep-alive new chunks\n",
        "#                 f.write(chunk)\n",
        "# file_id = '1PvtYTOX4gd-1VHX8LoT7s6KIyfTKOf8G'\n",
        "# destination = pretrained_model_path\n",
        "\n",
        "# if os.path.exists(destination) is not True:\n",
        "#     download_file_from_google_drive(file_id, destination)\n",
        "# else:\n",
        "#     print(\"pretrained repVGG model already exists\")\n",
        "\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1PvtYTOX4gd-1VHX8LoT7s6KIyfTKOf8G\"\n",
        "destination = pretrained_model_path\n",
        "\n",
        "if os.path.exists(destination) is not True:\n",
        "    gdown.download(url, destination, quiet=False)\n",
        "else:\n",
        "    print(\"pretrained repVGG model already exists\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Deplpy RepVGG-A2\n",
        "##############################################\n",
        "\n",
        "#deploy RepVGG-A2\n",
        "\"\"\"\n",
        "train_model = create_RepVGG_A2(deploy=False)\n",
        "train_model.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-A2-train.pth'))   \n",
        "model_ft = repvgg_model_convert(train_model, create_RepVGG_A2, save_path='/content/drive/MyDrive/Deep_learning/repvgg-A2-deploy.pth')\n",
        "\"\"\"\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "\n",
        "#use pretrained model\n",
        "#model_ft.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-A2-train.pth'))   \n",
        "model_ft.load_state_dict(torch.load (pretrained_model_path))   \n",
        "num_ftrs = model_ft.linear.in_features\n",
        "model_ft.linear = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 1\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "PX = 224\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "fold=0\n",
        "train_list = train_dataset_grav[fold] + train_dataset_cont[fold]\n",
        "train_list_label = list(itertools.repeat(1, len(train_dataset_grav[fold])))+list(itertools.repeat(0, len(train_dataset_cont[fold])))\n",
        "val_list = val_dataset_grav[fold] + val_dataset_cont[fold]\n",
        "val_list_label = list(itertools.repeat(1, len(val_dataset_grav[fold])))+list(itertools.repeat(0, len(val_dataset_cont[fold])))\n",
        "test_list = test_dataset_grav + test_dataset_cont\n",
        "test_list_label = list(itertools.repeat(1, len(test_dataset_grav)))+list(itertools.repeat(0, len(test_dataset_cont)))\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "test_dataset = SimpleImageDataset(test_list, test_list_label, test_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))\n",
        "print(len(test_list))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load (pretrained_model_path))   \n",
        "num_ftrs = model_ft.linear.in_features\n",
        "model_ft.linear = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "#optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min') \n",
        "\n",
        "\n",
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)"
      ],
      "metadata": {
        "id": "YXzP_K7Ataa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "plt.rcParams[\"font.size\"] = 14\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 1.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.xticks(np.arange(0, 20, 4) ) #start, end, 間隔\n",
        "plt.yticks(np.arange(0, 1.4, 0.2) )\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J2gmKzk9yPOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCam**"
      ],
      "metadata": {
        "id": "ihK4e0-90FlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "def GradCAM(img, c, features_fn, classifier_fn):\n",
        "    feats = features_fn(img.cuda())\n",
        "    _, N, H, W = feats.size() #[1,2048,7,7]\n",
        "    out = classifier_fn(feats) #out: [1,1000]\n",
        "    c_score = out[0, c]   #c_scoreとは？？\n",
        "\n",
        "    grads = torch.autograd.grad(c_score, feats)\n",
        "    w = grads[0][0].mean(-1).mean(-1)           #ここでGlobalAveragePoolingをしている\n",
        "    sal = torch.matmul(w, feats.view(N, H*W))\n",
        "    sal = sal.view(H, W).cpu().detach().numpy()\n",
        "    sal = np.maximum(sal, 0) #ReLUと同じ\n",
        "    return sal\n",
        "\n",
        "read_tensor = transforms.Compose([\n",
        "    lambda x: Image.open(x),\n",
        "    lambda x: x.convert('RGB'),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    lambda x: torch.unsqueeze(x, 0) #次元を1に引き延ばす\n",
        "])\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      #print('Image: '+ image_name)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      #print('Label: '+ label)\n",
        "      return(image_name, label)\n",
        "\n",
        "def gradcam(model_ft, test_dataset,  row=0, save=False):\n",
        "    # Split model in two parts\n",
        "    features_fn = nn.Sequential(*list(model_ft.children())[:-2]) #最後の2層（AdaptiveAvgPool2dとLinear)を取り除いたもの\n",
        "    classifier_fn = nn.Sequential(*(list(model_ft.children())[-2:-1] + [Flatten()] + list(model_ft.children())[-1:])) #最終層の前にFlatten()を挿入\n",
        "    #最後の2層\n",
        "\n",
        "    #評価モードにする    \n",
        "    model_ft = model_ft.eval()\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    classes = [\"cont\", \"grav\"]\n",
        "\n",
        "    #画像のパスを指定\n",
        "    #for j in range(3):\n",
        "    for j in range(len(test_dataset)):\n",
        "\n",
        "        #元画像\n",
        "\n",
        "        image = test_dataset[j][0]\n",
        "        image = image.permute(1, 2, 0)\n",
        "\n",
        "        img_tensor = test_dataset[j][0].unsqueeze(0)\n",
        "        #Softmaxにかけたときの確率上位1つのpp(確率)とcc(class番号)を取得(tench→正常,goldfish→斜視)\n",
        "        pp, cc = torch.topk(nn.Softmax(dim=1)(model_ft(img_tensor.to(device))), 1)\n",
        "\n",
        "        #pとcを対にして入力\n",
        "        for i, (p, c) in enumerate(zip(pp[0], cc[0])):  \n",
        "            sal = GradCAM(img_tensor, int(c), features_fn, classifier_fn)\n",
        "            tmp = image.to('cpu').detach().numpy().copy()\n",
        "            img = Image.fromarray((tmp*255).astype(np.uint8))\n",
        "            #TensorをImageに変換\n",
        "            sal = Image.fromarray(sal)\n",
        "            sal = sal.resize(img.size, resample=Image.LINEAR)\n",
        "\n",
        "            print()\n",
        "            print('image: {}'.format(j))\n",
        "            #print(img_path) #あとで参照しやすいように画像のパスを表示\n",
        "\n",
        "            #plt.title('')\n",
        "            print('label: '+classes[test_dataset[j][1]])\n",
        "            print('pred:  '+'{}  {:.1f}%'.format(classes[c], 100*float(p)))\n",
        "            #plt.title('pred:'+'{}: { .1f}%'.format(labels[c], 100*float(p)))        \n",
        "            \n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            #グラフを1行2列に並べたうちの1番目\n",
        "            plt.subplots_adjust(wspace=0,hspace=0)\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(img)\n",
        "            plt.imshow(np.array(sal), alpha=0.5, cmap='jet')\n",
        "\n",
        "            #元の画像を並べて表示\n",
        "            image = test_dataset[j][0]\n",
        "            image = image.permute(1, 2, 0)\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(image)\n",
        "\n",
        "            if save == True:\n",
        "                plt.savefig(gradcam_folder_path+\"/row{}-label{}-pred{}.png\".format(row,classes[test_dataset[j][1]], classes[c]))\n",
        "            row += 1\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#gradcam(model_ft, test_dataset, row=0, save=False)"
      ],
      "metadata": {
        "id": "WcePIt3PyWMb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Automated_analysis**"
      ],
      "metadata": {
        "id": "sf8EN-q10MDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#保存用の空CSVを作成\n",
        "id, number, path, label = [], [], [], []\n",
        "\n",
        "k=0\n",
        "i=0\n",
        "for j in test_dataset_grav:\n",
        "    id.append(k)\n",
        "    number.append(os.path.basename(j))\n",
        "    path.append(j)\n",
        "    label.append(1)\n",
        "    k+=1\n",
        "for j in test_dataset_cont:\n",
        "    id.append(k)\n",
        "    number.append(os.path.basename(j))\n",
        "    path.append(j)\n",
        "    label.append(0)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "# k=0\n",
        "# for i in val_dataset_grav:\n",
        "#     for j in i:\n",
        "#         fold.append(k)\n",
        "#         number.append(os.path.basename(j))\n",
        "#         path.append(j)\n",
        "#         label.append(1)\n",
        "#     k+=1\n",
        "# k=0\n",
        "# for i in val_dataset_cont:\n",
        "#     for j in i:\n",
        "#         fold.append(k)\n",
        "#         number.append(os.path.basename(j))\n",
        "#         path.append(j)\n",
        "#         label.append(0)\n",
        "#     k+=1\n",
        "df_result = pd.DataFrame(index=[],columns=[])\n",
        "df_result = pd.DataFrame(index=[],columns=[\"img_id\", \"img_number\", \"path\",\"label\", \"pred\", \"prob\"])\n",
        "df_result[\"img_id\"] = id\n",
        "df_result[\"img_number\"] = number\n",
        "df_result[\"path\"] = path\n",
        "df_result[\"label\"] = label\n",
        "\n",
        "df_result"
      ],
      "metadata": {
        "id": "8UfoHXFk0J-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#大事なデータを上書きしないよう注意！！#\n",
        "########################################\n",
        "\n",
        "df_result.to_csv(result_csv_path,encoding=\"shift_jis\", index=False) "
      ],
      "metadata": {
        "id": "s_XNgvXXImWC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open reslut_csv\n",
        "with codecs.open(result_csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df_result = pd.read_csv(file, index_col=None, header=0)\n",
        "df_result"
      ],
      "metadata": {
        "id": "q67s2cGlztpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start automated analysis\n",
        "fold = 0\n",
        "\n",
        "#Open reslut_csv\n",
        "with codecs.open(result_csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df_result = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "time_start = time.perf_counter()\n",
        "\n",
        "\n",
        "#Define Data Augumentation\n",
        "TRAIN_RANDOM_ROTATION = 1\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "PX = 224\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "for fold in range(fold, num_folds): #指定したfold数から開始\n",
        "    print(\"fold: {}\".format(fold))\n",
        "    train_list = train_dataset_grav[fold] + train_dataset_cont[fold]\n",
        "    train_list_label = list(itertools.repeat(1, len(train_dataset_grav[fold])))+list(itertools.repeat(0, len(train_dataset_cont[fold])))\n",
        "    val_list = val_dataset_grav[fold] + val_dataset_cont[fold]\n",
        "    val_list_label = list(itertools.repeat(1, len(val_dataset_grav[fold])))+list(itertools.repeat(0, len(val_dataset_cont[fold])))\n",
        "    test_list = test_dataset_grav + test_dataset_cont\n",
        "    test_list_label = list(itertools.repeat(1, len(test_dataset_grav)))+list(itertools.repeat(0, len(test_dataset_cont)))\n",
        "\n",
        "    #define dataset and dataloader\n",
        "    train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "    val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "    test_dataset = SimpleImageDataset(test_list, test_list_label, test_data_transforms)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0)\n",
        "\n",
        "\n",
        "    # show sample image\n",
        "    inputs, classes = next(iter(val_loader))\n",
        "    print(classes)\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    class_names = [\"cont\", \"grav\"]\n",
        "    imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "    # model_ft = torchvision.models.resnet50(pretrained=True)  \n",
        "    # num_ftrs = model_ft.fc.in_features\n",
        "    # model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    model_ft = torchvision.models.efficientnet_b4(pretrained = True)\n",
        "    #model_ft = create_RepVGG_A2(deploy=False)\n",
        "    #model_ft.load_state_dict(torch.load (pretrained_model_path))   \n",
        "    num_ftrs = model_ft.classifier[1].in_features\n",
        "    model_ft.classifier[1] = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    from ranger_adabelief import RangerAdaBelief\n",
        "    optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "    #optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
        "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min') \n",
        "\n",
        "\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=300)\n",
        "\n",
        "\n",
        "    # visualize the loss as the network trained\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "    plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "\n",
        "    # find position of lowest validation loss\n",
        "    minposs = valid_loss.index(min(valid_loss))+1 \n",
        "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.ylim(0, 1.0) # consistent scale\n",
        "    plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig('loss_plot.png', bbox_inches='tight')\n",
        "\n",
        "    #Prediction for validation set\n",
        "    \n",
        "    model_ft.eval() # prep model for evaluation\n",
        "    targets, probs, preds =[], [], []\n",
        "    for image_tensor, target in test_loader:  \n",
        "          #target = target.squeeze(1)     \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft(image_tensor)\n",
        "          _, pred = torch.max(output, 1) \n",
        "        \n",
        "          prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "          prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "          print(prob)\n",
        "          print(pred) \n",
        "          \n",
        "          probs.append(prob) #予測確率\n",
        "          preds.append(int(pred))  #予測結果\n",
        "          targets.append(int(target)) #ラベル\n",
        "    y_label = np.array(targets)\n",
        "    y_pred = np.array(preds)\n",
        "    y_prob = np.array(probs)\n",
        "    print(\"label\")\n",
        "    print(y_label)\n",
        "    print(\"pred\")\n",
        "    print(y_pred)\n",
        "    print(\"prob\")\n",
        "    print(y_prob)\n",
        "\n",
        "    #write result to df\n",
        "    row = 0\n",
        "    if fold == 0:\n",
        "        fold = 0\n",
        "    else:\n",
        "        for m in range(0, fold):\n",
        "            row += len(test_dataset_grav[m]+test_dataset_cont[m])\n",
        "    df_result.loc[row:row+len(y_pred)-1, \"pred\"] = y_pred\n",
        "    column = fold + 9\n",
        "    df_result.loc[row:row+len(y_pred)-1, \"prob\"] = y_prob\n",
        "    df_result.to_csv(result_csv_path,encoding=\"shift_jis\", index=False) #save as csv\n",
        "    \n",
        "    #GradCam\n",
        "    gradcam(model_ft, val_dataset, row, save=True) \n",
        "\n",
        "    #経過時間を表示\n",
        "    time_end = time.perf_counter()\n",
        "    time_elapsed = (time_end - time_start)\n",
        "    print(\"Elapsed time: \"+str(time_elapsed))"
      ],
      "metadata": {
        "id": "WCTAQQQ12tSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ROC curve**"
      ],
      "metadata": {
        "id": "Uh_aAup_hd5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with codecs.open(result_csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df_result = pd.read_csv(file, index_col=None, header=0)\n",
        "df_result"
      ],
      "metadata": {
        "id": "LxNos2lRIQao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#################################################\n",
        "threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "#################################################\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "X = df_result[\"prob\"]\n",
        "Y = df_result[\"label\"]\n",
        "\n",
        "Y_pred_proba = X\n",
        "Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(Y, Y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "#5-fold分のconfusion matrixを加算\n",
        "confusion_arr += confusion_matrix(Y, Y_pred)\n",
        "\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(Y, Y_pred))\n",
        "print(f'Accuracy : {accuracy_score(Y, Y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(Y, Y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(Y, Y_pred)}')\n",
        "print(f'Specificity : {specificity_score(Y, Y_pred)}')\n",
        "print(f'F1 score : {f1_score(Y, Y_pred)}')\n",
        "\n",
        "\n",
        "#ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(Y, Y_pred_proba)     \n",
        "plt.plot(fpr, tpr, marker='o')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.grid()\n",
        "print(f'Area_under_ROC : {roc_auc_score(Y, Y_pred_proba)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "# accuracy.append(accuracy_score(Y, Y_pred))\n",
        "# precision.append(precision_score(Y, Y_pred))\n",
        "# recall.append(recall_score(Y, Y_pred))\n",
        "# specificity.append(specificity_score(Y, Y_pred))\n",
        "# f1score.append(f1_score(Y, Y_pred))\n",
        "# area_u_ROC.append(roc_auc_score(Y, Y_pred_proba))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "#ヒートマップを作成\n",
        "arr_2d = np.round(confusion_arr).astype(int) \n",
        "df_matrix = pd.DataFrame(data=arr_2d, index=[\"Normal\", \"TED\"], columns=[\"Normal\", \"TED\"])\n",
        "print(df_matrix)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(df_matrix, annot=True,fmt=\"d\", cmap='Blues')\n",
        "plt.savefig(confusion_path, dpi=700)\n",
        "plt.show()\n",
        "plt.close('all')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NmOJDYqaQEwW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "118ae818-9d92-4f70-e3b3-ac53a74c4630"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7205882352941176\n",
            "26 8 11 23\n",
            "confusion matrix = \n",
            " [[23 11]\n",
            " [ 8 26]]\n",
            "Accuracy : 0.7205882352941176\n",
            "Precision (true positive rate) : 0.7027027027027027\n",
            "Recall (sensitivity): 0.7647058823529411\n",
            "Specificity : 0.6764705882352942\n",
            "F1 score : 0.7323943661971832\n",
            "Rand_state : 2\n",
            "Area_under_ROC : 0.8330449826989619\n",
            "\n",
            "        Normal  TED\n",
            "Normal      23   11\n",
            "TED          8   26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC curve描き直し\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.rcParams[\"font.family\"] = \"Helvetica\"   # 使用するフォント\n",
        "    plt.rcParams[\"font.size\"] = 10 \n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 1:\n",
        "                  y_true.append(1)\n",
        "            elif i == 0:\n",
        "                  y_true.append(0)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr,tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    plt.savefig(ROC_path, format=\"png\", dpi=700)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "#Draw ROC curve\n",
        "roc_label_list = [1]\n",
        "fig = Draw_roc_curve([Y], [Y_pred_proba], roc_label_list, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cb48jClLk24l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#統計結果を追記\n",
        "df_result['statistics'] = np.nan\n",
        "df_result['result'] = np.nan\n",
        "df_result.loc[0:5, 'statistics'] = [\"Accuracy\", \"Positive predictive value\", \"Sensitivity\", \"Specificity\", \"F-score\", \"Area_under_ROC\", \"Rand_seed\"]\n",
        "df_result.loc[0:5, 'result'] = [accuracy_score(Y, Y_pred), precision_score(Y, Y_pred), recall_score(Y, Y_pred), specificity_score(Y, Y_pred), f1_score(Y, Y_pred), roc_auc_score(Y, Y_pred_proba), random_seed]"
      ],
      "metadata": {
        "id": "oBo9g2XsnJA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv(result_csv_path,encoding=\"shift_jis\", index=False) \n",
        "df_result"
      ],
      "metadata": {
        "id": "gSRr_lYTnF8Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a151cd5-c5b8-4989-c4e1-b38379b05567"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    img_id img_number                                               path  \\\n",
              "0        0   4997.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "1        1   1773.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "2        2   6599.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "3        3     19.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "4        4   1682.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "5        5   3077.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "6        6   7104.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "7        7   4331.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "8        8   5906.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "9        9   5397.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "10      10   5125.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "11      11   5335.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "12      12   7290.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "13      13   1429.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "14      14   6147.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "15      15   5021.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "16      16   6806.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "17      17   4553.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "18      18   4292.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "19      19   3276.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "20      20   2407.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "21      21   1528.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "22      22   7443.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "23      23   3752.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "24      24   5069.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "25      25   2644.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "26      26    507.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "27      27   7644.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "28      28   7295.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "29      29    489.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "30      30   2664.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "31      31   6764.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "32      32   7798.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "33      33   2960.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "34      34   5981.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "35      35   7068.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "36      36   6171.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "37      37   3436.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "38      38   5507.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "39      39   7738.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "40      40   7016.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "41      41   6371.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "42      42    168.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "43      43   2866.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "44      44   7726.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "45      45   2206.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "46      46   5826.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "47      47   3368.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "48      48   1263.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "49      49    513.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "50      50   6888.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "51      51   2351.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "52      52   5041.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "53      53   5881.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "54      54   2034.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "55      55   4904.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "56      56   5183.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "57      57   5258.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "58      58   2556.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "59      59   2968.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "60      60   1043.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "61      61   3813.JPG  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "62      62   1451.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "63      63   2054.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "64      64    191.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "65      65   3102.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "66      66   8040.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "67      67    646.jpg  /content/drive/MyDrive/Deep_learning/666mai_da...   \n",
              "\n",
              "    label  pred      prob  \n",
              "0       1     0  0.146510  \n",
              "1       1     1  0.990127  \n",
              "2       1     1  0.998215  \n",
              "3       1     1  0.987832  \n",
              "4       1     1  0.556878  \n",
              "5       1     0  0.268124  \n",
              "6       1     1  0.999969  \n",
              "7       1     1  0.998955  \n",
              "8       1     1  0.999987  \n",
              "9       1     1  0.999051  \n",
              "10      1     1  0.982963  \n",
              "11      1     1  0.677487  \n",
              "12      1     0  0.453377  \n",
              "13      1     0  0.017827  \n",
              "14      1     1  0.999238  \n",
              "15      1     1  0.999999  \n",
              "16      1     1  0.999995  \n",
              "17      1     1  0.999942  \n",
              "18      1     1  0.998918  \n",
              "19      1     1  0.997290  \n",
              "20      1     1  0.993968  \n",
              "21      1     1  0.947482  \n",
              "22      1     1  0.988360  \n",
              "23      1     1  0.999995  \n",
              "24      1     0  0.107458  \n",
              "25      1     1  1.000000  \n",
              "26      1     1  0.999999  \n",
              "27      1     1  0.977865  \n",
              "28      1     0  0.021693  \n",
              "29      1     1  0.988117  \n",
              "30      1     0  0.219304  \n",
              "31      1     1  0.999999  \n",
              "32      1     0  0.000884  \n",
              "33      1     1  0.814579  \n",
              "34      0     0  0.261963  \n",
              "35      0     1  0.975314  \n",
              "36      0     0  0.002496  \n",
              "37      0     0  0.295006  \n",
              "38      0     1  0.760622  \n",
              "39      0     1  0.538504  \n",
              "40      0     0  0.008346  \n",
              "41      0     0  0.138993  \n",
              "42      0     0  0.000022  \n",
              "43      0     0  0.041510  \n",
              "44      0     0  0.010164  \n",
              "45      0     0  0.121421  \n",
              "46      0     0  0.001918  \n",
              "47      0     1  0.694511  \n",
              "48      0     1  0.862240  \n",
              "49      0     0  0.058988  \n",
              "50      0     0  0.003489  \n",
              "51      0     0  0.388372  \n",
              "52      0     0  0.000524  \n",
              "53      0     0  0.297413  \n",
              "54      0     1  0.985814  \n",
              "55      0     0  0.006643  \n",
              "56      0     0  0.092796  \n",
              "57      0     0  0.020632  \n",
              "58      0     1  0.995399  \n",
              "59      0     1  0.766779  \n",
              "60      0     1  0.573565  \n",
              "61      0     1  0.969891  \n",
              "62      0     0  0.018935  \n",
              "63      0     0  0.081570  \n",
              "64      0     1  0.807118  \n",
              "65      0     0  0.036757  \n",
              "66      0     0  0.035783  \n",
              "67      0     0  0.180032  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73b9e279-181f-40a9-9f24-1411fa70f653\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_id</th>\n",
              "      <th>img_number</th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "      <th>pred</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4997.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.146510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1773.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.990127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6599.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.998215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>19.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1682.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.556878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>3077.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.268124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>7104.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>4331.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.998955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>5906.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>5397.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>5125.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.982963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>5335.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.677487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>7290.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.453377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>1429.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.017827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>6147.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>5021.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>6806.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>4553.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>4292.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.998918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>3276.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.997290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>2407.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.993968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>1528.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.947482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>7443.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.988360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>3752.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>5069.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.107458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>2644.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>507.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>7644.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.977865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>7295.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.021693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>489.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.988117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>2664.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.219304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>6764.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>7798.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>2960.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.814579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>5981.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.261963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>7068.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.975314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>6171.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.002496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>3436.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.295006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>5507.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.760622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>7738.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.538504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>7016.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>6371.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.138993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>168.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>2866.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.041510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>7726.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.010164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>45</td>\n",
              "      <td>2206.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.121421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>5826.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.001918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>3368.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.694511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>48</td>\n",
              "      <td>1263.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.862240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>49</td>\n",
              "      <td>513.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>50</td>\n",
              "      <td>6888.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.003489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>51</td>\n",
              "      <td>2351.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.388372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>52</td>\n",
              "      <td>5041.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>53</td>\n",
              "      <td>5881.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.297413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>54</td>\n",
              "      <td>2034.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.985814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>55</td>\n",
              "      <td>4904.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.006643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>56</td>\n",
              "      <td>5183.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.092796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>57</td>\n",
              "      <td>5258.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.020632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>58</td>\n",
              "      <td>2556.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.995399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>2968.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.766779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>60</td>\n",
              "      <td>1043.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.573565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>61</td>\n",
              "      <td>3813.JPG</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.969891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>62</td>\n",
              "      <td>1451.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.018935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>63</td>\n",
              "      <td>2054.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.081570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>64</td>\n",
              "      <td>191.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.807118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>65</td>\n",
              "      <td>3102.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.036757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66</td>\n",
              "      <td>8040.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.035783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>67</td>\n",
              "      <td>646.jpg</td>\n",
              "      <td>/content/drive/MyDrive/Deep_learning/666mai_da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.180032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73b9e279-181f-40a9-9f24-1411fa70f653')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73b9e279-181f-40a9-9f24-1411fa70f653 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73b9e279-181f-40a9-9f24-1411fa70f653');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HkMWSdP6oGxs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNqVsLGcHswmty6aUE1oQ6R",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}