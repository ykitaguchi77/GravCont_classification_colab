{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Extend_dataset_MobileNet%EF%BC%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetMobileNet_for_ios**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "95c835d9-d006-4745-f558-6bf9a69e0e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "!pip install albumentations==0.4.6\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.9/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from torch_optimizer) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.9/dist-packages (0.4.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (6.0)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.5.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.9.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.19.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (8.4.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2023.2.28)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (23.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.39.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Tue Mar 14 05:46:27 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    53W / 400W |    997MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b7f742-0f07-455e-8cca-243b581d3be7"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d5da6b-7112-4f5a-e2b3-3ae29ec1b6c4"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.96"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_datasetを colab上のフォルダに展開\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #保存先フォルダ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLOフォルダにすでに展開されているデータセットを用いる\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "    \n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "2703846b-b613-4cc4-ada8-5f35d74b174c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 1657\n",
            "cont: 1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5向けにGroupKfoldで仕分けられたデータセットがあるのでこれを用いる　　#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels \n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet用に224px四方に成形しておく\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "if os.path.exists(dst_folder): \n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/train\")\n",
        "os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #処理時間の計測\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:      \n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "          \n",
        "          #切り取った画像を表示\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #処理するDirectoryの設定\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #ここにフォルダ番号を記載する (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])      \n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "        \n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            \n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL1ノルムの絶対値を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##パラメータのL2ノルムの二乗を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()   \n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(num_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)  \n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "# Insert dropout\n",
        "# model_ft.classifier = nn.Sequential(\n",
        "#      nn.Dropout(0.5),\n",
        "#      nn.Linear(num_ftrs, 2)\n",
        "# )\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min') \n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_pathは画像を格納しているフォルダ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI2_SlJUcqDX",
        "outputId": "53ac7ff6-9113-4824-be15-06d0f6b9b01e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ranger_adabelief in /usr/local/lib/python3.9/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from ranger_adabelief) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.5.0)\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "2649\n",
            "664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyRGmXWCcqFA",
        "outputId": "71061098-2dc9-43cf-e8c8-4a074c4f892d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Epoch: [ 0/40\n",
            "train_loss: 4.34290 train_acc: 0.65119\n",
            "valid_loss: 0.45496 valid_acc: 0.85392\n",
            "Validation loss decreased (inf --> 0.454957).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 1/40\n",
            "train_loss: 4.09705 train_acc: 0.83352\n",
            "valid_loss: 0.23766 valid_acc: 0.91867\n",
            "Validation loss decreased (0.454957 --> 0.237659).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 2/40\n",
            "train_loss: 3.98278 train_acc: 0.87958\n",
            "valid_loss: 0.23331 valid_acc: 0.90964\n",
            "Validation loss decreased (0.237659 --> 0.233311).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 3/40\n",
            "train_loss: 3.94375 train_acc: 0.90072\n",
            "valid_loss: 0.25982 valid_acc: 0.91114\n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 4/40\n",
            "train_loss: 3.89569 train_acc: 0.92337\n",
            "valid_loss: 0.19814 valid_acc: 0.91114\n",
            "Validation loss decreased (0.233311 --> 0.198135).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 5/40\n",
            "train_loss: 3.88297 train_acc: 0.92412\n",
            "valid_loss: 0.22855 valid_acc: 0.91416\n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 6/40\n",
            "train_loss: 3.84233 train_acc: 0.94300\n",
            "valid_loss: 0.26938 valid_acc: 0.88705\n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 7/40\n",
            "train_loss: 3.81541 train_acc: 0.94828\n",
            "valid_loss: 0.23335 valid_acc: 0.92620\n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 8/40\n",
            "train_loss: 3.82084 train_acc: 0.94602\n",
            "valid_loss: 0.28102 valid_acc: 0.87952\n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 9/40\n",
            "train_loss: 3.79348 train_acc: 0.95923\n",
            "valid_loss: 0.22376 valid_acc: 0.91566\n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [10/40\n",
            "train_loss: 3.77168 train_acc: 0.96565\n",
            "valid_loss: 0.26331 valid_acc: 0.91717\n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [11/40\n",
            "train_loss: 3.76953 train_acc: 0.96602\n",
            "valid_loss: 0.24474 valid_acc: 0.90211\n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [12/40\n",
            "train_loss: 3.74328 train_acc: 0.97131\n",
            "valid_loss: 0.25102 valid_acc: 0.91114\n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [13/40\n",
            "train_loss: 3.75520 train_acc: 0.96678\n",
            "valid_loss: 0.30032 valid_acc: 0.91114\n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [14/40\n",
            "train_loss: 3.74312 train_acc: 0.97018\n",
            "valid_loss: 0.24593 valid_acc: 0.92319\n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [15/40\n",
            "train_loss: 3.73523 train_acc: 0.96942\n",
            "valid_loss: 0.37006 valid_acc: 0.89458\n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [16/40\n",
            "train_loss: 3.71400 train_acc: 0.97471\n",
            "valid_loss: 0.41099 valid_acc: 0.87952\n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [17/40\n",
            "train_loss: 3.70080 train_acc: 0.97810\n",
            "valid_loss: 0.28756 valid_acc: 0.89458\n",
            "EarlyStopping counter: 13 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [18/40\n",
            "train_loss: 3.68393 train_acc: 0.98339\n",
            "valid_loss: 0.36661 valid_acc: 0.88705\n",
            "EarlyStopping counter: 14 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [19/40\n",
            "train_loss: 3.68984 train_acc: 0.97659\n",
            "valid_loss: 0.38856 valid_acc: 0.89307\n",
            "EarlyStopping counter: 15 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [20/40\n",
            "train_loss: 3.68248 train_acc: 0.98075\n",
            "valid_loss: 0.53213 valid_acc: 0.87801\n",
            "EarlyStopping counter: 16 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [21/40\n",
            "train_loss: 3.68409 train_acc: 0.97659\n",
            "valid_loss: 0.25184 valid_acc: 0.91717\n",
            "EarlyStopping counter: 17 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [22/40\n",
            "train_loss: 3.68637 train_acc: 0.97659\n",
            "valid_loss: 0.27405 valid_acc: 0.91114\n",
            "EarlyStopping counter: 18 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [23/40\n",
            "train_loss: 3.65471 train_acc: 0.98301\n",
            "valid_loss: 0.30212 valid_acc: 0.90060\n",
            "EarlyStopping counter: 19 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [24/40\n",
            "train_loss: 3.65282 train_acc: 0.98377\n",
            "valid_loss: 0.31167 valid_acc: 0.90512\n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "plt.rcParams[\"font.size\"] = 14\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 1.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.xticks(np.arange(0, 20, 4) ) #start, end, 間隔\n",
        "plt.yticks(np.arange(0, 1.4, 0.2) )\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LIT0epLUD1EG",
        "outputId": "df1ce8e6-344c-4592-9193-d3da9c1a7fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)  \n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b712887-e257-42be-e313-891130eb4982"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to CoreML**"
      ],
      "metadata": {
        "id": "f3or7dfeDVg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML 飛ばして下さい\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV2\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"grav\", \"cont\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "    classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        ")\n",
        "\n",
        "# Save model\n",
        "mlmodel.save(model_folder_path+\"/sample_\"+model_name+\".mlmodel\")"
      ],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaFibhPMElc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNA3LupkElea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBZYC-woElgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNyWYywiElif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")            \n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\") \n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "5d8631eb-b096-471d-96c5-0f3913a7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e0bdb6-f54e-47f9-e9f4-413245975212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "476d809b-269b-4cd6-a0fe-82f4da46dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951a6753-9a5b-4e71-e026-7416d32bcc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "430d02b5-7a51-42f0-f012-cd33f4d3178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2649 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "66105c9d-2766-4fff-ef56-107bb88cda87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG: 448x640 1 grav, 18.4ms\n",
            "Speed: 0.7ms pre-process, 18.4ms inference, 38.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f69277f-119c-41dc-d348-1bccda7d470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device) \n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img) \n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "    \n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]  \n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}